{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **TFM Máster Big Data y Data Science**\n\nAlumno: Jonás Jiménez Gil\n\n*Universidad Complutense de Madrid*\n\n*2022/2023*\n","metadata":{}},{"cell_type":"markdown","source":"# **House Prices - Advanced Regression Techniques**\n\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/5407/media/housesbanner.png)\n\nEste trabajo se realiza como parte del **Trabajo Final de Máster** para el **Máster de Big Data y Data Science** de la **Universidad Complutense de Madrid**.\n\nSe ha escogido este tema ya que el tema de la vivienda en España en la actualidad es un tema polémico y de una gran problemática.\n\nNuestro objetivo es predecir el **precio de venta** de unas viviendas utilizando las diferentes características de las mismas.\nPara poder hacerlo contaremos con un dataset con 1460 viviendas localizadas en Ames Lowa (USA),  distribuidas en 80 variables.\n\nPara poder indagar más acerca esta competencia de Kaggle pulsar en el siguiente [enlace](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview)\n","metadata":{}},{"cell_type":"markdown","source":"**Objetivos TFM**\n\n- Desarrollar un modelo de regresión avanzado que pueda predecir con precisión los **precios de las viviendas**. Esto puede involucrar la exploración de diferentes técnicas de regresión, como la regresión lineal, la regresión polinómica, la regresión por bosques aleatorios, la regresión por redes neuronales, etc.\n\n- Realizar un análisis exploratorio de los datos para identificar patrones, relaciones y características importantes en el conjunto de datos de precios de viviendas. Esto puede incluir la identificación de valores atípicos, la imputación de valores faltantes, la exploración de la distribución de las variables, la identificación de correlaciones entre variables, etc.\n\n- Optimizar los hiperparámetros del modelo para mejorar la precisión de las predicciones. Esto puede requerir la implementación de técnicas de validación cruzada para evaluar diferentes configuraciones de hiperparámetros y seleccionar la que mejor se ajuste al conjunto de datos.\n\n- Realizar un análisis para evaluar cómo afectan las diferentes variables del modelo a las predicciones de los precios de las viviendas. Esto puede ayudar a identificar las variables más importantes y a proporcionar información sobre cómo los cambios en las variables afectan los precios de las viviendas.\n\n- Desarrollar visualizaciones que ayuden a comunicar los resultados del modelo y las principales conclusiones del análisis exploratorio de los datos. Esto puede incluir la creación de gráficos de dispersión, diagramas de caja, mapas de calor, diagramas de barras, etc.\n\n- Poner en práctica todos los conocimientos adquiridos durante la realización del Máster\n","metadata":{}},{"cell_type":"markdown","source":"Una vez definidos los objetivos a conseguir con nuestro TFM cargamos todas las librerías que son necesarias para nuestro trabajo","metadata":{}},{"cell_type":"code","source":"# Avoid warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Análisis exploratorio de datos:\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import style\nstyle.use(\"ggplot\")\nimport missingno\nfrom scipy import stats\nfrom scipy.stats import shapiro, norm, skew, boxcox\nfrom scipy.special import boxcox1p\nimport re\nfrom collections import OrderedDict\n\n# Preprocesamiento de datos:\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import make_pipeline\n\n\n# Modelización:\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\n\nfrom sklearn import tree\n\nfrom sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\n\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.inspection import permutation_importance\n\nfrom sklearn.svm import SVR\nimport xgboost as xgb\n\n# Visualización:\nfrom plotly.subplots import make_subplots\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython.core.display import HTML\nimport plotly.graph_objs as go\n\n# Recuperar datos\nimport requests","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:42:58.030275Z","iopub.execute_input":"2023-05-10T07:42:58.030837Z","iopub.status.idle":"2023-05-10T07:42:58.046640Z","shell.execute_reply.started":"2023-05-10T07:42:58.030783Z","shell.execute_reply":"2023-05-10T07:42:58.045221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ÍNDICE**\n\n* [1. Lectura de Datos](#section-one)\n* [2. Análisis exploratorio (EDA)](#section-two)\n    - [2.1 Primer vistazo a los datos](#section-two-subsection-one)\n    - [2.2 Variable target](#section-two-subsection-two)\n    - [2.3 Variables numéricas](#section-two-subsection-three)\n        - [2.3.1 Variables numéricas discretas](#section-two-subsection-three-one)\n        - [2.3.2 Variables numéricas continuas](#section-two-subsection-three-two)\n    - [2.4 Variables categóricas](#section-two-subsection-four)\n    - [2.5 Valores nulos](#section-two-subsection-five)\n* [3. Preprocesado y análisis de datos](#section-three)\n    - [3.1 Nulos variables numéricas](#section-three-subsection-one)\n    - [3.2 Nulos variables categóricas](#section-three-subsection-two)\n    - [3.3 Convertir variables categóricas a numéricas](#section-three-subsection-three) \n* [4. Feature Engineering](#section-four)    \n     - [4.1 Primeros pasos](#section-four-subsection-one)\n     - [4.2 Creación de nuevas categorías](#section-four-subsection-two)\n     - [4.3 Transformación logarítmica de la variable target ('SalePrice')](#section-four-subsection-three)\n     - [4.4 Transformación Box-Cox de las variables excesivamente asimétricas](#section-four-subsection-four)\n     - [4.5 Escalado de variables numéricas](#section-four-subsection-five)  \n     - [4.6 Variables Dummies](#section-four-subsection-six) \n* [5. Modelos](#section-five)      \n     - [5.1 Estrategias](#section-five-subsection-one) \n     - [5.2 Regresión Lineal](#section-five-subsection-two) \n     - [5.3 Regresión Ridge](#section-five-subsection-three) \n     - [5.4 Regresión Lasso](#section-five-subsection-four) \n     - [5.5 Modelo de los K vecinos más cercanos](#section-five-subsection-five) \n     - [5.6 Árboles de decisión](#section-five-subsection-six) \n     - [5.7 Modelo Random Forest](#section-five-subsection-seven) \n     - [5.8 Modelo Regresor de vectores de apoyo (SVR)](#section-five-subsection-eight) \n     - [5.9 Modelo Gradient Boosting Regressor](#section-five-subsection-nine) \n     - [5.10 Modelo Stacked Regressor](#section-five-subsection-ten)      \n* [6. Predicción](#section-six)   \n     - [6.1 Valores](#section-six-subsection-one)    \n     - [6.2 Características más importantes del modelo](#section-six-subsection-two) \n     - [6.3 Exportación modelo final (Stacked Regressor)](#section-six-subsection-three)     \n* [CONCLUSIONES](#section-seven)\n* [BIBLIOGRAFÍA](#section-eight)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n## **1. Lectura de Datos**","metadata":{}},{"cell_type":"markdown","source":"En primer lugar importamos los dos datasets (train y test) y los almacenamos en dos dataframes de Pandas.","metadata":{}},{"cell_type":"code","source":"# Leer los datasets\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:42:58.049146Z","iopub.execute_input":"2023-05-10T07:42:58.050094Z","iopub.status.idle":"2023-05-10T07:42:58.121364Z","shell.execute_reply.started":"2023-05-10T07:42:58.050048Z","shell.execute_reply":"2023-05-10T07:42:58.120485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comprobar el tamaño del df Train\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:42:58.122950Z","iopub.execute_input":"2023-05-10T07:42:58.123564Z","iopub.status.idle":"2023-05-10T07:42:58.130370Z","shell.execute_reply.started":"2023-05-10T07:42:58.123532Z","shell.execute_reply":"2023-05-10T07:42:58.129185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comprobar el tamaño del df Test\ntest.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:42:58.131841Z","iopub.execute_input":"2023-05-10T07:42:58.132581Z","iopub.status.idle":"2023-05-10T07:42:58.142259Z","shell.execute_reply.started":"2023-05-10T07:42:58.132550Z","shell.execute_reply":"2023-05-10T07:42:58.140955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mostramos el tamaño\nprint('Tamaño de tabla de valores entrenamiento:',train.shape[0])\nprint('Tamaño de tabla para predicciones:',test.shape[0])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:42:58.145025Z","iopub.execute_input":"2023-05-10T07:42:58.145479Z","iopub.status.idle":"2023-05-10T07:42:58.154487Z","shell.execute_reply.started":"2023-05-10T07:42:58.145437Z","shell.execute_reply":"2023-05-10T07:42:58.153502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El dataset de train está compuesto por 81 columnas y 1460 entradas. Vamos a visualizar el nombre de sus colmunas.","metadata":{}},{"cell_type":"code","source":"print(train.columns.values)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:42:58.156008Z","iopub.execute_input":"2023-05-10T07:42:58.156441Z","iopub.status.idle":"2023-05-10T07:42:58.166317Z","shell.execute_reply.started":"2023-05-10T07:42:58.156412Z","shell.execute_reply":"2023-05-10T07:42:58.165149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Y los primeros 5 valores de cada una de ellas","metadata":{}},{"cell_type":"code","source":"train.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:42:58.168095Z","iopub.execute_input":"2023-05-10T07:42:58.168434Z","iopub.status.idle":"2023-05-10T07:42:58.227236Z","shell.execute_reply.started":"2023-05-10T07:42:58.168405Z","shell.execute_reply":"2023-05-10T07:42:58.226177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Este es un resumen de todas nuestras variables:\n\n   - **SalePrice**: Es el precio de la propiedad en dólares. Esta es nuestra variable target.\n   - **MSSubClass**: La clase de edificio.\n   - **MSZoning**: La clasificación general por zonas\n   - **LotFrontage**: la longitd en pies lineales  de calle conectada con la propiedad\n   - **LotArea**: Tamaño del lote en pies cuadrados\n   - **Street**: Tipo de acceso por carretera\n   - **Alley**: Tipo de acceso al callejón\n   - **LotShape**: Forma general de la propiedad\n   - **LandContour**: Planitud de la propiedad\n   - **Utilities**: Tipos de servicios disponibles\n   - **LotConfig**: Configuración de la parcela\n   - **LandSlope**: Pendiente de la propiedad\n   - **Neighborhood**: Ubicación física dentro de los límites de la ciudad de Ames\n   - **Condition1**: Proximidad a carretera principal o ferrocarril\n   - **Condition2**: Proximidad a carretera principal o ferrocarril (si existe una segunda)\n   - **BldgType**: Tipo de vivienda\n   - **HouseStyle**: Estilo de la vivienda\n   - **OverallQual**: Calidad general de materiales y acabados\n   - **OverallCond**: Estado de conservación general\n   - **YearBuilt**: Fecha de construcción original\n   - **YearRemodAdd**: Fecha de remodelación\n   - **RoofStyle**: Tipo de tejado\n   - **RoofMatl**: Material del tejado\n   - **Exterior1st**: Revestimiento exterior de la casa\n   - **Exterior2nd**: Revestimiento exterior de la casa (si hay más de un material)\n   - **MasVnrType**: Tipo de revestimiento de mampostería\n   - **MasVnrArea**: Área del revestimiento de mampostería en pies cuadrados\n   - **ExterQual**: Calidad del material exterior\n   - **ExterCond**: Estado actual del material en el exterior\n   - **Foundation**: Tipo de cimentación\n   - **BsmtQual**: Altura del sótano\n   - **BsmtCond**: Estado general del sótano\n   - **BsmtExposure**: Muros del sótano a nivel de jardín o con salida al exterior\n   - **BsmtFinType1**: Calidad de la superficie acabada del sótano\n   - **BsmtFinSF1**: Tipo 1 pies cuadrados acabados\n   - **BsmtFinType2**: Calidad de la segunda superficie acabada (si existe)\n   - **BsmtFinSF2**: Pies cuadrados acabados de tipo 2\n   - **BsmtUnfSF**: Pies cuadrados sin terminar de la superficie del sótano\n   - **TotalBsmtSF**: Superficie total del sótano en pies cuadrados\n   - **Heating**: Tipo de calefacción\n   - **HeatingQC**: Estado y calidad de la calefacción\n   - **CentralAir**: Aire acondicionado central\n   - **Electrical**: Sistema eléctrico\n   - **1stFlrSF**: Pies cuadrados del primer piso\n   - **2ndFlrSF**: Pies cuadrados del segundo piso\n   - **LowQualFinSF**: Superficie de acabados de baja calidad en pies cuadrados (en todas las plantas)\n   - **GrLivArea**: Superficie habitable por encima del nivel del suelo en pies cuadrados\n   - **BsmtFullBath**: Baños completos del sótano\n   - **BsmtHalfBath**: Medios baños del sótano\n   - **FullBath**: Baños completos sobre rasante\n   - **HalfBath**: Medios baños sobre rasante\n   - **Bedroom**: Número de habitaciones sobre rastante\n   - **Kitchen**: Número de cocinas\n   - **KitchenQual**: Calidad de las cocinas\n   - **TotRmsAbvGrd**: Número total de habitaciones sobre raante (no incluye baños)\n   - **Functional**: Rating de funcionalidad de la vivienda\n   - **Fireplaces**: Número de chimeneas\n   - **FireplaceQu**: Calidad de las chimeneas\n   - **GarageType**: Ubicación del garaje\n   - **GarageYrBlt**: Fecha de construcción del garaje\n   - **GarageFinish**: Acabado interior del garaje\n   - **GarageCars**: Tamaño del garaje en capacidad de coches\n   - **GarageArea**: Tamaño del garaje en pies cuadrados\n   - **GarageQual**: Calidad del garaje\n   - **GarageCond**: Estado del garaje\n   - **PavedDrive**: Calzada pavimentada\n   - **WoodDeckSF**: Superficie de la cubierta de madera en pies cuadrados\n   - **OpenPorchSF**: Área de porche abierto en pies cuadrados\n   - **EnclosedPorch**: Superficie del porche cerrado en pies cuadrados\n   - **3SsnPorch**: Superficie del porche 3Estaciones en pies cuadrados\n   - **ScreenPorch**: Área de porche vidriado en pies cuadrados\n   - **PoolArea**: Área de la piscina en pies cuadrados\n   - **PoolQC**: Calidad de la piscina\n   - **Fence**: Calidad de la valla\n   - **MiscFeature**: Característica miscelánea no incluida en otras categorías\n   - **MiscVal**: Valor de la característica miscelánea\n   - **MoSold**: Mes de venta\n   - **YrSold**: Año de venta\n   - **SaleType**: Tipo de venta\n   - **SaleCondition**: Condición de venta ","metadata":{}},{"cell_type":"markdown","source":"Podemos observar que se podrían empezar a agrupar en los siguientes grupos:\n\n- **'Id'**\n- **Variable Target**: 'SalePrice'\n- **Información del lote**: 'LotFrontage', 'LotArea', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope'\n- **Información de construcción**: 'MSSubClass', 'BldgType', 'HouseStyle', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical'\n- **Información de la vivienda**: '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu'\n- **Información del garaje**: 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond'\n- **Información de la terraza y porche**: 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch'\n- **Información de la piscina y la cerca**: 'PoolArea', 'PoolQC', 'Fence'\n- **Información diversa**: 'MiscFeature', 'MiscVal'\n- **Información de la venta**: 'MoSold', 'YrSold', 'SaleType', 'SaleCondition'","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n## **2. Análisis exploratorio (EDA)**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-two-subsection-one\"></a>\n### **2.1 Primer vistazo a los datos**","metadata":{}},{"cell_type":"markdown","source":"Creamos una función para simplificar el análisis de las características generales de los datos. Esta función devuelve los *tipos, recuentos, distintos, recuentos nulos, proporción de faltantes y valores únicos* de cada campo/característica.\n\nUna vez que se han calculado estas estadísticas para cada columna, se concatena todo en un único dataframe utilizando el método *\"concat\"*. Luego, la función crea una lista de nombres de columna para el dataframe resumen y asigna esta lista como el atributo \"columns\" del dataframe resumen. Finalmente, se devuelve el dataframe resumen.\n\nLa función también establece algunas opciones de visualización de pandas para asegurarse de que todas las columnas se muestren en el resumen y de que los números se muestren con dos decimales.","metadata":{}},{"cell_type":"code","source":"def dataframe_summary(train):\n    \"\"\"Generamos una función que nos genera un sumario del dataframe, incluyendo tipos de datos, valores únicos...\"\"\"\n    \n    obs = train.shape[0] #Número de filas\n    types = train.dtypes #Tipos de datos\n    counts = train.apply(lambda x: x.count()) #Número total\n    distincts = train.apply(lambda x: x.nunique()) #Valores únicos\n    nulls = train.isnull().sum() #Número total de nulos\n    missing_ratio = (nulls / obs) * 100 #Porcentaje de missings respecto al total\n    skewness = train.select_dtypes(include='number').skew() #Asimetría\n    kurtosis = train.select_dtypes(include='number').kurt() #Curtosis\n    top_vals = train.apply(lambda x: x.value_counts(dropna=False).index[0]) #Valores más repetidos\n    top_counts = train.apply(lambda x: x.value_counts(dropna=False).iloc[0]) #La moda de cada variable\n    top_percent = (top_counts / obs) * 100 #Porcentaje del que más se repite\n    \n    #Concatenamos todos\n    summary_df = pd.concat([types, counts, distincts, nulls, missing_ratio, top_vals, top_counts, top_percent, skewness, kurtosis], axis=1)\n    \n    #Creamos las columnas\n    summary_df.columns = [\"Type\", \"Count\", \"Distinct\", \"Missing\", \"Missing_Ratio\", \"Most_Common\", \"Most_Common_Count\", \"Most_Common_Percent\", \"Skewness\", \"Kurtosis\"]\n    \n    return summary_df\n\npd.set_option('display.max_columns', None) #Mostramos todas las columnas\npd.options.display.float_format = '{:.2f}'.format #Mostramos solamente dos decimales\nHTML(dataframe_summary(train).to_html(classes=[\"table-bordered\", \"table-striped\", \"table-hover\"])) #Aplicamos formato HTML\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:42:58.229327Z","iopub.execute_input":"2023-05-10T07:42:58.229728Z","iopub.status.idle":"2023-05-10T07:42:58.391265Z","shell.execute_reply.started":"2023-05-10T07:42:58.229700Z","shell.execute_reply":"2023-05-10T07:42:58.390178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Nuestro conjunto consta de tres tipos de datos:\n        - Float\n        - Integer\n        - Object    \n- En conjunto, nos encontramos con un total de 81 variables:\n        - 43 características de tipo categórico\n        - 38 características numéricas de las cuales 1 es nuestra variable target \"SalePrice\" y otra es el \"Id\".        \n- Hay algunas características que tienen asignado el tipo de datos de manera incorrecta como por ejemplo: MSSubClass y OverallCond deberían ser tipos de datos objeto.\n- Nulos: Los datos tienen 19 características con nulos, cinco de ellas categóricas y con más de un 47% de ratios perdidos. Tendremos que pensar en como tratar esos nulos, o directamente eliminarlas:\n        - PoolQC\n        - MiscFeature\n        - Alley\n        - Fence\n        - FireplaceQu\n- Tenemos categorías con una alta correlación con 'SalePrice', con una correlación menor, sesgadas para la derecha etc, todo esto lo analizaremos más en detalle más adelante.","metadata":{}},{"cell_type":"code","source":"# Convertimos estas 2 a tipo objeto\ntrain[['MSSubClass', 'OverallCond']] = train[['MSSubClass', 'OverallCond']].astype('object')\ntest[['MSSubClass', 'OverallCond']] = test[['MSSubClass', 'OverallCond']].astype('object')","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:42:58.392739Z","iopub.execute_input":"2023-05-10T07:42:58.393038Z","iopub.status.idle":"2023-05-10T07:42:58.403427Z","shell.execute_reply.started":"2023-05-10T07:42:58.393011Z","shell.execute_reply":"2023-05-10T07:42:58.402311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ya que no es necesaria para el proceso de predicción almacenaremos nuestra variable \"Id\" en unos datasets a parte y la eliminamos de nuestros datasets principales.","metadata":{}},{"cell_type":"code","source":"#Almacenamos ID\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Eliminamos Id\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:42:58.406484Z","iopub.execute_input":"2023-05-10T07:42:58.406812Z","iopub.status.idle":"2023-05-10T07:42:58.418345Z","shell.execute_reply.started":"2023-05-10T07:42:58.406783Z","shell.execute_reply":"2023-05-10T07:42:58.417032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:42:58.419839Z","iopub.execute_input":"2023-05-10T07:42:58.420279Z","iopub.status.idle":"2023-05-10T07:42:58.477001Z","shell.execute_reply.started":"2023-05-10T07:42:58.420238Z","shell.execute_reply":"2023-05-10T07:42:58.475872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-two-subsection-two\"></a>\n### **2.2 Variable target**\n\nHacemos un primer análisis rápido de nuestra variable target","metadata":{}},{"cell_type":"code","source":"# Mostrar algunas características de la variable target\nprint(train['SalePrice'].describe())\n# Imprimir la distribución\nplt.figure(figsize=(9, 8))\nsns.displot(train['SalePrice'], color='g', bins=100, alpha=0.4, kde=True);","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:42:58.478503Z","iopub.execute_input":"2023-05-10T07:42:58.478839Z","iopub.status.idle":"2023-05-10T07:42:59.246358Z","shell.execute_reply.started":"2023-05-10T07:42:58.478811Z","shell.execute_reply":"2023-05-10T07:42:59.245206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos que el 50% de los valores se sitúan por debajo de 163000, que casi coincide con la media de todos los valores (180921). También observamos que un 75% de los valores se encuentran situados en cantidades inferiores a 214000.\n\nTambién obervamos que nuestra variable target tiene una distribución muy similar a otras variables categóricas como son:\n\n    - LotFrontage, TotalBsmtSF, BsmtHalfBath, FullBath, KitchenAbvGr y PoolArea.","metadata":{}},{"cell_type":"code","source":"# Crear una figura con cinco subtramas y establecer su tamaño\nfig, axs = plt.subplots(1, 6, figsize=(20, 5))\n\n# Generar un histograma para cada variable y mostrarlo en su subtrama correspondiente\nsns.histplot(train['LotFrontage'], ax=axs[0])\nsns.histplot(train['TotalBsmtSF'], ax=axs[1])\nsns.histplot(train['BsmtHalfBath'], ax=axs[2])\nsns.histplot(train['FullBath'], ax=axs[3])\nsns.histplot(train['KitchenAbvGr'], ax=axs[4])\nsns.histplot(train['PoolArea'], ax=axs[5])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:42:59.247702Z","iopub.execute_input":"2023-05-10T07:42:59.248034Z","iopub.status.idle":"2023-05-10T07:43:01.173574Z","shell.execute_reply.started":"2023-05-10T07:42:59.248004Z","shell.execute_reply":"2023-05-10T07:43:01.172478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ahora vamos a separar a las variables entre numéricas y categóricas para un mejor análisis","metadata":{}},{"cell_type":"code","source":"# Separación de variables numéricas y categóricas.\ncat_var = train.select_dtypes(include=['object'])\nnum_var = train.select_dtypes(exclude=['object'])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:01.175051Z","iopub.execute_input":"2023-05-10T07:43:01.176259Z","iopub.status.idle":"2023-05-10T07:43:01.185332Z","shell.execute_reply.started":"2023-05-10T07:43:01.176216Z","shell.execute_reply":"2023-05-10T07:43:01.184043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-two-subsection-three\"></a>\n### **2.3 Variables numéricas**","metadata":{}},{"cell_type":"code","source":"# Generamos un resumen estadístico y las numéricas y las trasponemos\ndisplay(num_var.describe().transpose())","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:01.186895Z","iopub.execute_input":"2023-05-10T07:43:01.187303Z","iopub.status.idle":"2023-05-10T07:43:01.291752Z","shell.execute_reply.started":"2023-05-10T07:43:01.187272Z","shell.execute_reply":"2023-05-10T07:43:01.290750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Existen dos tipos de características numéricas en nuestro dataset:\n\n            - Discretas: aquellas que solo puede tomar un conjunto limitado de valores aislados y separados, generalmente enteros\n            - Continuas: pueden tomar cualquier valor dentro de un intervalo o rango específico.\n            \nAl contar con características numéricas discretas es mejor separarlas y analizarlas de manera independiente, ya que a veces tienden a analizarse de una manera más similar a las categóricas","metadata":{}},{"cell_type":"code","source":"# Generamos histograma para ver la distribución de todas las variables numéricas\nnum_var.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8);","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:01.296240Z","iopub.execute_input":"2023-05-10T07:43:01.296566Z","iopub.status.idle":"2023-05-10T07:43:13.256622Z","shell.execute_reply.started":"2023-05-10T07:43:01.296539Z","shell.execute_reply":"2023-05-10T07:43:13.255337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-two-subsection-three-one\"></a>\n#### **2.3.1 Variables numéricas discretas**","metadata":{}},{"cell_type":"markdown","source":"Consideramos que las variables discretas son las que tienen un número de valores diferentes igual o menor a 15","metadata":{}},{"cell_type":"code","source":"# Creamos una lista con las variables discretas\ndiscrete_cols = []\nfor col in num_var.columns:\n    if num_var[col].dtype == 'int64' and len(num_var[col].unique()) <= 15:\n        discrete_cols.append(col)\n        \nprint(discrete_cols)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:13.258613Z","iopub.execute_input":"2023-05-10T07:43:13.259048Z","iopub.status.idle":"2023-05-10T07:43:13.274878Z","shell.execute_reply.started":"2023-05-10T07:43:13.259007Z","shell.execute_reply":"2023-05-10T07:43:13.273281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convertir las columnas numéricas a float64\nfor col in discrete_cols:\n    train[col] = train[col].astype('float64')","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:13.277190Z","iopub.execute_input":"2023-05-10T07:43:13.277680Z","iopub.status.idle":"2023-05-10T07:43:13.291268Z","shell.execute_reply.started":"2023-05-10T07:43:13.277637Z","shell.execute_reply":"2023-05-10T07:43:13.290336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generamos una matriz de correlación entre las variables discretas para ver la correlación que pueden tener entre ellas.","metadata":{}},{"cell_type":"code","source":"corr_matrix_disc = train[discrete_cols + ['SalePrice']].corr()\n\n# Visualiza la matriz de correlación con un mapa de calor\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix_disc, annot=True, cmap='coolwarm')\nplt.title('Matriz de correlación entre variables discretas', fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:13.292841Z","iopub.execute_input":"2023-05-10T07:43:13.293541Z","iopub.status.idle":"2023-05-10T07:43:14.344438Z","shell.execute_reply.started":"2023-05-10T07:43:13.293502Z","shell.execute_reply":"2023-05-10T07:43:14.343277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos una gran correlación positiva entre 3 variables principalmente respecto a SalePrice, que son:\n    \n        - TotRmsAbvGrd: 0.53\n        - Fireplaces: 0.47\n        - GarageCars: 0.64\n        - FullBath: 0.56\n        \n        \nSon tres variables que en un primer momento podríamos haber pensado que podrían tener una gran correlación con el precio de venta de la vivienda, ya que normalmente cuantas más habitaciones tiene una vivienda más cara es, y lo mismo con el garaje, baños y con las chimeneas.\n\nA su vez observamos una gran correlación entre estas variables:\n\n    - TotRmsAbvGr y BeedroomAbvGr: 0.68\n    - OverallQual y GarageCars: 0.6\n    - Fullbath y OveralQuall y TotRmsAbvGr: 0.55\n    \nPodríamos presuponer de una manera directa que el número total de habitaciones y el número total de dormitorios tendrían una relación bastante directo. De la misma manera, el número de GarageCars podría implicar que cuantos más coches mejor será la casa por lo tanto mejor Calidad General tendrá. Mismo caso con el número total de baños.\n\nVamos a generar ahora un boxplot para analizar una a una todas las variables","metadata":{}},{"cell_type":"code","source":"# Crear una matriz de subtramas para mostrar los gráficos\nfig, axs = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n\n# Recorrer las variables discretas y crear un gráfico para cada una\nfor i, var in enumerate(discrete_cols):\n    row = i // 4\n    col = i % 4\n    sns.boxplot(data=train, x=var, y='SalePrice', ax=axs[row, col], palette='Purples_r')\n    axs[row, col].set_xlabel(var, fontsize=12)\n    axs[row, col].set_ylabel('SalePrice', fontsize=12)\n    axs[row, col].tick_params(axis='x', labelsize=10)\n    axs[row, col].tick_params(axis='y', labelsize=10)\n\n# Ajustar la disposición de los gráficos\nplt.tight_layout()\n\n# Mostrar el gráfico\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:14.345962Z","iopub.execute_input":"2023-05-10T07:43:14.346326Z","iopub.status.idle":"2023-05-10T07:43:18.687747Z","shell.execute_reply.started":"2023-05-10T07:43:14.346294Z","shell.execute_reply":"2023-05-10T07:43:18.686613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **OverallQual**: se observa una relación lineal muy potente entre las dos variables. Cuanto mayor es la calidad general mayor es el precio\n\n- **BsmtFullBath, BsmtHalfBath y HalfBath**: Se observa una relación lineal ligera, ya que aumenta ligeramente el precio conforme más valor tienen estas categorías. Por ejemplo si el número de baños es 0, el valor será menor.\n\n\n- **FullBath**: Hay una relación muy directa entre el número total de baños y el precio, a más baños, más precio.\n\n\n- **BedroomAbvGr**: No se observa una relación clara entre el número de dormitorios sobre rasante y el precio de venta de la vivienda. Los precios más altos se encuentran en viviendas con 0, 4 u 8 dormitorios, mientras que los precios más bajos corresponden a viviendas con 2 o 6 dormitorios.\n\n\n- **KitchenAbvGr**: Existe una relación inversa entre el número de cocinas sobre rasante y el precio de venta de la vivienda. El precio medio disminuye a medida que aumenta el número de cocinas, siendo el más alto para viviendas con una sola cocina.\n\n\n- **TotRmsAbvGrd**: Existe una relación no lineal entre el número de habitaciones totales sobre rasante y el precio de venta de la vivienda. El precio medio aumenta hasta las 11 habitaciones y luego disminuye ligeramente.\n\n\n- **Fireplaces**: Existe una relación directa entre el número de chimeneas en la vivienda y el precio de venta. A medida que aumenta el número de chimeneas, también aumenta el precio de venta.\n\n\n- **GarageCars**: Observamos una relación no lineal, ya que aumenta el precio hasta los 3 GarageCars, pero disminuye ligeramente en el 4.\n\n\n- **PoolArea**: Existe una relación directa entre el tamaño de la piscina y el precio de venta de la vivienda. A medida que aumenta el área de la piscina, también aumenta el precio de venta, siendo la superficie de 555 sqft la que presenta el precio medio más alto.\n\n\n- **MoSold**: No se observa una relación clara entre el mes de venta de la vivienda y el precio de venta.\n\n\n- **YrSold**: No se observa una relación clara entre el año de venta de la vivienda y el precio de venta.","metadata":{}},{"cell_type":"markdown","source":"Ahora pasamos a analizar el resto de variables numéricas, con el mismo esquema de análisis.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-two-subsection-three-two\"></a>\n#### **2.3.2 Variables numéricas continuas**","metadata":{}},{"cell_type":"markdown","source":"De la misma manera que hemos creado las discretas, ahora creamos las continuas","metadata":{}},{"cell_type":"code","source":"#Creamos una variable con solamente las numéricas continuas excluyendo la variable target\ncontinuous_cols = []\n\nfor col in train.columns:\n    if train[col].dtype == 'int64' and col not in discrete_cols and col != 'SalePrice':\n        continuous_cols.append(col)\n        \nprint(continuous_cols)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:18.689055Z","iopub.execute_input":"2023-05-10T07:43:18.689428Z","iopub.status.idle":"2023-05-10T07:43:18.701460Z","shell.execute_reply.started":"2023-05-10T07:43:18.689397Z","shell.execute_reply":"2023-05-10T07:43:18.700373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calcular la matriz de correlaciones elimiando los nulos para poder ver todas las categorías\ncorr_matrix_cont = train[continuous_cols + ['SalePrice']].dropna().corr()\n\n# Crear el heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix_cont, annot=True, cmap='coolwarm')\nplt.title('Matriz de correlación para variables continuas', fontsize=10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:18.703565Z","iopub.execute_input":"2023-05-10T07:43:18.704021Z","iopub.status.idle":"2023-05-10T07:43:20.284430Z","shell.execute_reply.started":"2023-05-10T07:43:18.703981Z","shell.execute_reply":"2023-05-10T07:43:20.283195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos una gran cantidad de variables muy correlacionadas, vamos a estudiarlas en más profundidad.","metadata":{}},{"cell_type":"code","source":"# Calculamos la matriz de correlación entre todas las variables numéricas y \"SalePrice\", y se seleccionan todas las filas excepto la última (que es \"SalePrice\").\ntrain_num_corr = num_var.corr()[\"SalePrice\"][:-1]\n\n# Seleccionamos las variables que tienen una correlación absoluta mayor que 0.5 y se ordenan de forma descendente\ngolden_features_list = train_num_corr[abs(train_num_corr)> 0.5].sort_values(ascending=False)\n\n# Mostramos el número de variables seleccionadas y la lista de estas variables, formateando los valores de correlación a tres decimales y mostrando el índice de las variables (en lugar de los nombres).\nprint(\"Existen {} valores fuertemente correlacionados con la variable objetivo SalePrice:\\n{}\".format(len(golden_features_list),\ngolden_features_list.apply(lambda x: '{:.3f}'.format(x)).to_string(index=True)))","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:20.286350Z","iopub.execute_input":"2023-05-10T07:43:20.286734Z","iopub.status.idle":"2023-05-10T07:43:20.303805Z","shell.execute_reply.started":"2023-05-10T07:43:20.286699Z","shell.execute_reply":"2023-05-10T07:43:20.302412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Observamos que tenemos unas cuantas características con una correlación positiva muy alta (mayor a 0,50), es decir, si aumenta su valor, aumenta el precio de la venta:\n\n        - OverallQual     0.791\n        - GrLivArea       0.709\n        - GarageCars      0.640\n        - GarageArea      0.623\n        - TotalBsmtSF     0.614\n        - 1stFlrSF        0.606\n        - FullBath        0.561\n        - TotRmsAbvGrd    0.534\n        - YearBuilt       0.523\n        - YearRemodAdd    0.507\n\n- Ausencia de correlación en algunas variables que presumiblemente iban a tener más relación como ´LotArea\n- Algunas que tienen una correlación negativa como 'EnclosedPorch' o 'ScreenPorch'.\n- Muchas variables con una correlación casi de 0, es decir, no se puede observar una relación lineal entre las dos variables.\n\nVamos a estudiar más en profundidad las 4 que observamos con más correlación respecto a nuestra variable target","metadata":{}},{"cell_type":"markdown","source":"**GrLivArea y TotRomsAbvGrdE**\n\nPodemos analizar de manera conjunta estas dos variables ya que creemos que están intimamente relacionadas. Generaremos un scatterplot para cada una de ellas, mostrando su correlación y los outliers con los que cuentan.","metadata":{}},{"cell_type":"code","source":"# Calculamos los outliers de GrLivArea\noutliers_grlivarea = train[np.abs(train['GrLivArea'] - train['GrLivArea'].mean()) > 5*train['GrLivArea'].std()]\n\n# Graficamos GrLivArea\nfig, ax = plt.subplots(1, 2, figsize=(12,5))\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=train, color='b', ax=ax[0])\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=outliers_grlivarea, color='r', ax=ax[0])\nsns.regplot(x='GrLivArea', y='SalePrice', data=train.drop(outliers_grlivarea.index), scatter=False, color='orange', ax=ax[0])\nax[0].text(x=3000, y=750000, s='Correlation with SalePrice: {:6.4f}'.format(train.GrLivArea.corr(train['SalePrice'])))\nax[0].text(x=3000, y=700000, s='Correlation without Outliers: {:6.4f}'.format(train.drop(outliers_grlivarea.index).GrLivArea.corr(train.drop(outliers_grlivarea.index)['SalePrice'])))\nax[0].legend(['All data', 'Outliers', 'Without outliers'])\n\n# Calculamos los outliers de TotRomsAbvGrd\noutliers_totrooms = train[np.abs(train['TotRmsAbvGrd'] - train['TotRmsAbvGrd'].mean()) > 5*train['TotRmsAbvGrd'].std()]\n\n# Graficamos TotRomsAbvGrd\nsns.scatterplot(x='TotRmsAbvGrd', y='SalePrice', data=train, color='b', ax=ax[1])\nsns.scatterplot(x='TotRmsAbvGrd', y='SalePrice', data=outliers_totrooms, color='r', ax=ax[1])\nsns.regplot(x='TotRmsAbvGrd', y='SalePrice', data=train.drop(outliers_totrooms.index), scatter=False, color='orange', ax=ax[1])\nax[1].text(x=12, y=750000, s='Correlation with SalePrice: {:6.4f}'.format(train.TotRmsAbvGrd.corr(train['SalePrice'])))\nax[1].text(x=12, y=700000, s='Correlation without Outliers: {:6.4f}'.format(train.drop(outliers_totrooms.index).TotRmsAbvGrd.corr(train.drop(outliers_totrooms.index)['SalePrice'])))\nax[1].legend(['All data', 'Outliers', 'Without outliers'])\n\n#Mostramos el número total de outliers\nprint(f\"Número total de outliers de GrLivArea: {len(outliers_grlivarea)}\")\nprint(f\"Número total de outliers de TotRmsAbvGrd: {len(outliers_totrooms)}\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:20.305600Z","iopub.execute_input":"2023-05-10T07:43:20.306232Z","iopub.status.idle":"2023-05-10T07:43:21.506761Z","shell.execute_reply.started":"2023-05-10T07:43:20.306201Z","shell.execute_reply":"2023-05-10T07:43:21.505658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Identificamos 4 outliers en la categoría 'GrLivArea' y observamos claramente como conforme aumenta su valor, aumenta el precio de la vivienda, lo mismo en 'TotRmsAbvGrd' pero sin encontrar outliers en este caso.\nEliminaremos solamente dos de estos. Los que tengan un valor de GrLivArea superior a 4000 y un SalePrice inferior a 3000","metadata":{}},{"cell_type":"code","source":"# Eliminamos algunos outliers\ntrain = train.drop(train[(train.GrLivArea>4000) & (train.SalePrice<300000)].index)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:21.508268Z","iopub.execute_input":"2023-05-10T07:43:21.508603Z","iopub.status.idle":"2023-05-10T07:43:21.522431Z","shell.execute_reply.started":"2023-05-10T07:43:21.508574Z","shell.execute_reply":"2023-05-10T07:43:21.521179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ahora vamos a compararlas con la variable 'OverallQual', la cual nos parece una variable importante a la hora de predecir el precio.","metadata":{}},{"cell_type":"code","source":"#Ploetamos la relación entre estas dos variables y la calidad general de la vivienda\nfig, ax = plt.subplots(figsize=(8, 6))\n\nsns.scatterplot(x='GrLivArea', y='TotRmsAbvGrd', hue='OverallQual', data=train, ax=ax)\nax.set_title('Relación de GrLivArea vs TotRmsAbvGrd con OverallQual')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:21.525305Z","iopub.execute_input":"2023-05-10T07:43:21.526039Z","iopub.status.idle":"2023-05-10T07:43:22.105948Z","shell.execute_reply.started":"2023-05-10T07:43:21.525997Z","shell.execute_reply":"2023-05-10T07:43:22.104834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos la gran relación, a más número de habitaciones y a mayor valor de GrLivArea mayor calidad de la vivienda, probablemente nos relaciona que probablemente las viviendas más grandes, tienen mejor puntuación en calidad, que probablemente esté relacionada también con acabados, baños, ausencia o no de piscina o chimenea etc.","metadata":{}},{"cell_type":"markdown","source":"**GarageCars y GarageArea**\n\nVamos a analizar estas dos variables que aunque una es numérica continua y otra discreta pueden tener mucha correlación entre ambas","metadata":{}},{"cell_type":"code","source":"# Calculamos la correlación entre ambas y SalePrice\ntrain[['GarageCars', 'GarageArea', 'SalePrice']].corr()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:22.107673Z","iopub.execute_input":"2023-05-10T07:43:22.108383Z","iopub.status.idle":"2023-05-10T07:43:22.122845Z","shell.execute_reply.started":"2023-05-10T07:43:22.108338Z","shell.execute_reply":"2023-05-10T07:43:22.121653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos un problema de colinealidad entre estas dos variables, con una correlación positiva enorme entre ambas. Vamos a tratarlo generando una variable a partir de ellas dos más adelante y eliminando estas dos variables.\n\nVamos a ver ahora la relación de ambas con 'SalePrice'","metadata":{}},{"cell_type":"code","source":"# Generamos los gráficos de relación con 'SalePrice'\nfig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(10, 5))\nsns.scatterplot(x='GarageCars', y='SalePrice', data=train, ax=ax1, color='blue')\nsns.scatterplot(x='GarageArea', y='SalePrice', data=train, ax=ax2, color='blue')\nax1.set_title('GarageCars vs SalePrice')\nax2.set_title('GarageArea vs SalePrice')\nplt.show()\n\n# Calculamos la correlación entre las variables\ncorr1 = train['GarageCars'].corr(train['SalePrice'])\ncorr2 = train['GarageArea'].corr(train['SalePrice'])\nprint('Correlación entre GarageCars y SalePrice:', corr1)\nprint('Correlación entre GarageArea y SalePrice:', corr2)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:22.126257Z","iopub.execute_input":"2023-05-10T07:43:22.126603Z","iopub.status.idle":"2023-05-10T07:43:22.776758Z","shell.execute_reply.started":"2023-05-10T07:43:22.126572Z","shell.execute_reply":"2023-05-10T07:43:22.775943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generamos un boxplot de cada de las variables\nfig = plt.figure(figsize=(20,5))\n# GarageCars\nfig1 = fig.add_subplot(131)\nsns.boxplot(data=train, x='GarageCars', ax=fig1)\n#GarageArea\nfig2 = fig.add_subplot(132)\nsns.boxplot(data=train, x='GarageArea', ax=fig2)\n#GarageArea/GarageCars\nfig3 = fig.add_subplot(133)\nsns.boxplot(data=train, x='GarageCars', y='GarageArea', ax=fig3)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:22.778088Z","iopub.execute_input":"2023-05-10T07:43:22.778651Z","iopub.status.idle":"2023-05-10T07:43:23.482960Z","shell.execute_reply.started":"2023-05-10T07:43:22.778620Z","shell.execute_reply":"2023-05-10T07:43:23.481819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A partir del boxplot siguiente, podemos observar que más de 3 plazas de aparcamiento y más de 900 de superficie podrían ser outliers, ya que cuentan con poquísimas observaciones.\n\nAunque hay una relación entre ellos, lo más probable es que con un menor número de plazas de aparcamiento, puede haber más superficie de garaje para otros fines, razón por la cual la correlación entre ellos es 0,89 y no 1.\n\nVamos a crear una variable compuesta por ambas para ver si conseguimos una mejor correlación y así eliminamos la colinealidad. Probaremos dividiendo y multiplicando.","metadata":{}},{"cell_type":"code","source":"# Seleccionar variables\ndf = train[['SalePrice', 'GarageArea', 'GarageCars']]\n\n# Crear variables compuestas\ndf['GarageAreaByCar'] = train.GarageArea/train.GarageCars\ndf['GarageArea_x_Car'] = train.GarageArea*train.GarageCars\n\n# Gráfico de dispersión y regresión\nfig = plt.figure(figsize=(18, 6))\nfig1 = fig.add_subplot(121); sns.regplot(x='GarageAreaByCar', y='SalePrice', data=df); plt.legend(['Outliers'])\nplt.text(x=-100, y=750000, s='Correlación con SalePrice: {:6.4f}'.format(df.GarageArea.corr(df['SalePrice'])))\nprint('                                                                 Outliers:',(df.GarageArea>=1200).sum())\ndf = df.loc[df.GarageArea<1200]\nsns.regplot(x='GarageAreaByCar', y='SalePrice', data=df); plt.title('Garage Area')\nplt.text(x=-100, y=700000, s='Correlación sin Outliers: {:6.4f}'.format(df.GarageArea.corr(df['SalePrice'])))\n\nfig2 = fig.add_subplot(122); sns.regplot(x='GarageArea_x_Car', y='SalePrice', data=df); plt.legend(['Outliers'])\nplt.text(x=-100, y=750000, s='Correlación con SalePrice: {:6.4f}'.format(df.GarageArea_x_Car.corr(df['SalePrice'])))\nprint('                                                                 Outliers:',(df.GarageArea_x_Car>=3700).sum())\ndf = df.loc[df.GarageArea_x_Car<3700]\nsns.regplot(x='GarageArea_x_Car', y='SalePrice', data=df); plt.title('Garage Area Multiply By Cars is the best!')\nplt.text(x=-100, y=700000, s='Correlación sin Outliers: {:6.4f}'.format(df.GarageArea_x_Car.corr(df['SalePrice'])))\nplt.show()\n\n# Eliminar variables creadas\ndel df","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:23.484392Z","iopub.execute_input":"2023-05-10T07:43:23.484754Z","iopub.status.idle":"2023-05-10T07:43:25.158004Z","shell.execute_reply.started":"2023-05-10T07:43:23.484722Z","shell.execute_reply":"2023-05-10T07:43:25.156949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como podemos observar la variable 'GarageAreaByCar' no nos sirve para mucho ya que tiene una correlación casi inexistente con 'SalePrice'.\nEn cambio 'GarageArea_x_Car' tiene una correlación más alta que cualquiera de las dos variables por separado.\nHemos generado una variable que nos multiplica el área de garajes por el número de coches.\n\nConsideraremos que todos los valores mayores a 3700 son outliers, luego los eliminaremos, y comprobamos que mejora la correlación de nuestra variable en 0,004.","metadata":{}},{"cell_type":"code","source":"# Creamos la nueva variable\ntrain['GarageArea_x_Car'] = train.GarageArea*train.GarageCars\ntest['GarageArea_x_Car'] = test.GarageArea*test.GarageCars\n# Eliminamos outliers\ntrain = train[train.GarageArea * train.GarageCars < 3700]","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:25.159730Z","iopub.execute_input":"2023-05-10T07:43:25.160349Z","iopub.status.idle":"2023-05-10T07:43:25.171030Z","shell.execute_reply.started":"2023-05-10T07:43:25.160314Z","shell.execute_reply":"2023-05-10T07:43:25.170068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**YearBuilt y YearRemodAdd**\n\nObservaremos conjuntamente estas dos variables ya que son variables relacionadas con el tiempo","metadata":{}},{"cell_type":"code","source":"# Imprimimos su relación con la variable 'SalePrice'\nfig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(10, 5))\nsns.scatterplot(x='YearBuilt', y='SalePrice', data=train, ax=ax1, color='blue')\nsns.scatterplot(x='YearRemodAdd', y='SalePrice', data=train, ax=ax2, color='blue')\nax1.set_title('YearBuilt vs SalePrice')\nax2.set_title('YearRemodAdd vs SalePrice')\n\n# Calculamos la correlación entre las variables\ncorr1 = train['YearBuilt'].corr(train['SalePrice'])\ncorr2 = train['YearRemodAdd'].corr(train['SalePrice'])\nprint('Correlación entre YearBuilt y SalePrice:', corr1)\nprint('Correlación entre YearRemodAdd y SalePrice:', corr2)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:25.172569Z","iopub.execute_input":"2023-05-10T07:43:25.172923Z","iopub.status.idle":"2023-05-10T07:43:25.787959Z","shell.execute_reply.started":"2023-05-10T07:43:25.172893Z","shell.execute_reply":"2023-05-10T07:43:25.787184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se observa ligeramente un aumento del precio conforme la vivienda o la reforma se ha hecho más reciente, tiene una correlación moderada ambas variables de alrededor del 0.50.\n\nPara poder observarlo de una manera más detallada vamos a crear otro gráfico que nos relacione a las 3.","metadata":{}},{"cell_type":"code","source":"# Creamos un df con estas 3 columnas solamente\ndf = train[['SalePrice', 'YearBuilt', 'YearRemodAdd']]\n\nfig = plt.figure(figsize=(20,10))\nfig1 = fig.add_subplot(121); sns.scatterplot(y = df['SalePrice'], x = df['YearBuilt'], hue=df['YearRemodAdd'], palette= 'YlOrRd')\nplt.text(x=3700, y=600000, s='YearBuilt Correlation with SalePrice: {:1.4f}'.format(df.YearBuilt.corr(df['SalePrice'])))\n\nfig1.set_xlim(left=df['YearBuilt'].min(), right=df['YearBuilt'].max())\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:25.789041Z","iopub.execute_input":"2023-05-10T07:43:25.789822Z","iopub.status.idle":"2023-05-10T07:43:27.125903Z","shell.execute_reply.started":"2023-05-10T07:43:25.789791Z","shell.execute_reply":"2023-05-10T07:43:27.124790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este gráfico si que podemos observar que conforme la reforma o la construcción de la vivienda es más reciente el precio es mayor, aumenta significativamente. Se observa una relación bastante clara. También se observa que las casas con el precio más caro suelen ser las que se han reformado recientemente, lo cual tiene bastante sentido","metadata":{}},{"cell_type":"markdown","source":"**TotalBsmtSF y 1stFlrSF**\n\nVamos a analizar estas dos categorías que tienen una gran correlación con SalePrice","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\n# Definición de un outlier\noutlier = (train['TotalBsmtSF'] > 3000) & (train['SalePrice'] < 300000)\n\n# Gráfico de dispersión con las columnas TotalBsmtSF y SalePrice antes de eliminar los valores atípicos\nsns.scatterplot(x='TotalBsmtSF', y='SalePrice', data=train, color='b', ax=ax[0])\n\n# Gráfico de regresión con las columnas TotalBsmtSF y SalePrice después de eliminar los valores atípicos\nsns.regplot(x='TotalBsmtSF', y='SalePrice', data=train, scatter=False, color='orange', ax=ax[0])\n\n# Agregar texto con los coeficientes de correlación\nax[0].text(x=2000, y=750000, s='Correlación con SalePrice: {:6.4f}'.format(train.TotalBsmtSF.corr(train['SalePrice'])))\n\n# Gráfico de dispersión con las columnas 1stFlrSF y SalePrice antes de eliminar los valores atípicos\nsns.scatterplot(x='1stFlrSF', y='SalePrice', data=train, color='b', ax=ax[1])\n\n# Gráfico de regresión con las columnas 1stFlrSF y SalePrice después de eliminar los valores atípicos\nsns.regplot(x='1stFlrSF', y='SalePrice', data=train, scatter=False, color='orange', ax=ax[1])\n\n# Agregar texto con los coeficientes de correlación\nax[1].text(x=1000, y=750000, s='Correlación con SalePrice: {:6.4f}'.format(train['1stFlrSF'].corr(train['SalePrice'])))\n\n# Agregar puntos rojos para indicar los outliers en el gráfico de dispersión de TotalBsmtSF y SalePrice\nsns.scatterplot(x='TotalBsmtSF', y='SalePrice', data=train[outlier], color='red', ax=ax[0])\nax[0].text(x=4000, y=270000, s='Outlier')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:27.137821Z","iopub.execute_input":"2023-05-10T07:43:27.138518Z","iopub.status.idle":"2023-05-10T07:43:29.184207Z","shell.execute_reply.started":"2023-05-10T07:43:27.138478Z","shell.execute_reply":"2023-05-10T07:43:29.183076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos un outlier en TotalBsmtSF, ya que tenemos un valor con un TotalBsmtSF superior a 3000 y con un SalePrice inferior a 300000. Visualizamos también que tiene una gran correlación con SalePrice (0.65) y que se ve claramente como a mayor valor en TotalBsmtSF mayor será el SalePrice. A su vez se observa una gran cantidad de ceros, que seguramente coincidan con viviendas que no tienen garaje\n\nPor la parte de 1stFlrSF, de la misma manera se observa la gran correlación que tienen con SalePrice (0.63) y como conforme aumenta su valor aumenta proporcionalmente el valor de la vivienda.\n\nPasaremos ahora a analizar las variables categóricas","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-two-subsection-four\"></a>\n### **2.4 Variables categóricas**","metadata":{}},{"cell_type":"markdown","source":"**2.4.1 Análisis general**\n\nEn primer lugar haremos un análisis general de todas las variables categóricas del dataset.","metadata":{}},{"cell_type":"code","source":"# Creamos una lista con el nombre de todas las columnas categóricas y lo mostramos\ncat_vars = train.select_dtypes(include=['object']).columns.tolist()\nprint(cat_vars)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:29.185837Z","iopub.execute_input":"2023-05-10T07:43:29.186524Z","iopub.status.idle":"2023-05-10T07:43:29.193480Z","shell.execute_reply.started":"2023-05-10T07:43:29.186485Z","shell.execute_reply":"2023-05-10T07:43:29.192300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mostramos un análisis rápido estadístico de estas\ntrain[cat_vars].describe().T","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:29.195169Z","iopub.execute_input":"2023-05-10T07:43:29.195511Z","iopub.status.idle":"2023-05-10T07:43:29.300085Z","shell.execute_reply.started":"2023-05-10T07:43:29.195472Z","shell.execute_reply":"2023-05-10T07:43:29.298936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Contamos con muchas variables con nulos y muchas con una frecuencia bajísima del elemento más repetido, es decir estará más repartido. Vamos a generar un gráfico para cada una de ellas mirando su distribución para sacar conclusiones.","metadata":{}},{"cell_type":"code","source":"# Convertimos las variables categóricas al tipo de datos category\nfor var in cat_vars:\n    train[var] = train[var].astype('category')\n\n# Creamos un subplot para cada variable\nfig, axs = plt.subplots(11, 4, figsize=(25, 60))\nfor i, var in enumerate(cat_vars):\n    try:\n        sns.countplot(x=var, data=train, ax=axs[i//4, i%4])\n    except IndexError:\n        break\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:29.301494Z","iopub.execute_input":"2023-05-10T07:43:29.301813Z","iopub.status.idle":"2023-05-10T07:43:39.326928Z","shell.execute_reply.started":"2023-05-10T07:43:29.301786Z","shell.execute_reply":"2023-05-10T07:43:39.325863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tenemos algunas categoras tremendamente desbalanceadas en su distribución de valores\n\n- Las categorías \"Utilities\", \"GarageQual\", \"GarageCond\", \"Street\", \"RoofMtl\", 'SaleType'... tienen casi la totalidad de sus registros en un sólo valor.\n- \"PoolQC\" tiene una distribución de registros similar en cada uno de sus dos diferentes valores\n- El resto de categorías presentan una distribución más equilibrada, con valores más o menos repetidos.\n\nAhora mostraremos su distribució respecto a SalePrice","metadata":{}},{"cell_type":"code","source":"# Ploteamos la distribución de todas respecto a SalePrice\n\nfig, axs = plt.subplots(11, 4, figsize=(25, 60))\nfor i, var in enumerate(cat_vars):\n    try:\n        sns.stripplot(x=var, y='SalePrice', data=train, ax=axs[i//4, i%4], size=3)\n    except IndexError:\n        pass\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:39.328272Z","iopub.execute_input":"2023-05-10T07:43:39.328595Z","iopub.status.idle":"2023-05-10T07:43:53.110998Z","shell.execute_reply.started":"2023-05-10T07:43:39.328566Z","shell.execute_reply":"2023-05-10T07:43:53.109696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A simple vista podemos observar la diferencia de valor en 'SalePrice' dependiendo de la variable de cada categoría:\n        \n        - En algunas categorías como 'LotShape' o 'Fence' parece que no afecta demasiado. \n        - En otras como 'OverallCond','OverallQual', 'GarageCond', 'GarageQual' a primera vista observamos que afecta muchísimo\n\nVamos a comprobar la correlación entre variables en nuestro dataset. La correlación es una medida estadística que nos permite conocer el grado de asociación que existe entre dos o más variables. Esta medida nos va a indicar si existe una relación lineal entre las variables, y si es así, nos indica la dirección y la intensidad de dicha relación.\n\nGracias a la correlación podremos comprobar las categorías que están muy ligadas a nuestra variable target (SalePrices), y por tanto identificar cuales serán las que influyen de una manera más directa en ella.\n\nEn nuestro caso, cuanto más claro sea el cuadrado, mayor será la correlación entre dos variables.","metadata":{}},{"cell_type":"code","source":"# Calculamos la matriz de correlación para las variables categóricas\ncorrmat_cat = train.corr(numeric_only=True)\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat_cat, vmax=.8, square=True);","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:53.113024Z","iopub.execute_input":"2023-05-10T07:43:53.114563Z","iopub.status.idle":"2023-05-10T07:43:54.349003Z","shell.execute_reply.started":"2023-05-10T07:43:53.114515Z","shell.execute_reply":"2023-05-10T07:43:54.347724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Gran correlación de SalePrice con GrLivArea, 1stFlrSF, 2ndFlrSF, TotalBsmtSF... Nos esperábamos esta correlación ya que tiene bastante sentido que cuanto mayor sea el valor de estas categorías mayor sea el precio de venta de la vivienda\n* Altísima correlación entre GarageCars y GarageArea. Entendible, en el futuro crearemos una variable compuesta por ellas dos.\n* Muy alta correlación entre variables de tiempo (YearBuilt y GarageYrtBlt). Tiene mucho sentido ya que entiendo que cuando se construye una vivienda normalmente se construye también su garaje.\n* Altísima correlación entre GrLivArea y TotRmsAbvGrd. Tiene mucho sentido, a mayor tamanõ de una vivienda, mayor numero de habitaciones.","metadata":{}},{"cell_type":"markdown","source":"Ahora vamos a mostrar las variables que tienen una correlación mayor a 0.26 con SalePrice. Consideramos que 0.26 es una medida que nos muestra ya una correlatividad importante entre los dos valores. Para ello seleccionaremos las columnas que tengan una correlación respecto a SalePrice mayor a 0.26, generaremos un nuevo dataframe y las ordenaremos por orden de correlación. A continuación, mostraremos una representación visual de la matriz de correlación con las variables seleccionadas utilizando un mapa de calor, donde los valores más oscuros indican una mayor correlación positiva y los valores más claros indican una menor correlación o una correlación negativa. Además, eliminaremos las celdas superiores para evitar la duplicación de información y mostraremos el valor de correlación en cada celda. Finalmente, eliminaremos las variables intermedias creadas (corr, dropSelf y top_corr) para liberar memoria y evitar confusiones en futuros cálculos.","metadata":{}},{"cell_type":"code","source":"# Generamos la matriz de correlación\ncorr = train[train.SalePrice>1].corr()\n# Creamos el nuevo df\ntop_corr_cols = corr[abs((corr.SalePrice)>=.26)].SalePrice.sort_values(ascending=False).keys()\ntop_corr = corr.loc[top_corr_cols, top_corr_cols]\n# Creamos la matriz triangular\ndropSelf = np.zeros_like(top_corr)\ndropSelf[np.triu_indices_from(dropSelf)] = True\n# Imprimimos\nplt.figure(figsize=(20, 20))\n# Ajustamos parámetros\nsns.heatmap(top_corr, cmap=sns.diverging_palette(220, 10, as_cmap=True), annot=True, fmt=\".2f\", mask=dropSelf)\nsns.set(font_scale=1.4)\n\nplt.show()\n# Eliminamos variables\ndel corr, dropSelf, top_corr","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:54.350670Z","iopub.execute_input":"2023-05-10T07:43:54.351801Z","iopub.status.idle":"2023-05-10T07:43:56.671874Z","shell.execute_reply.started":"2023-05-10T07:43:54.351761Z","shell.execute_reply":"2023-05-10T07:43:56.670734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos ver que la variable target 'SalePrice' tiene la mayor correlación con la variable 'OverallQual' (0,80), seguida de. GrLivArea (0,73) y de la nueva categoría que creamos anteriormente ('GarageArea_x_Car'). Esto parece lógico, ya que de hecho esperamos que la calidad general y el tamaño de la superficie habitable influyan más en nuestros juicios de valor sobre una propiedad.\n\nDel gráfico anterior también se desprende claramente que la multicolinealidad es un problema. Tendremos que tratarla de alguna de estas tres maneras:\n\n        - Eliminando una o más variables de las que estén altamente correlacionadas.\n        - Combinando dos o más variables  altamente correlacionadas en una nueva variable (Feature Engineering)\n        - Utilizar técnicas de regularización, como la regresión ridge o la regresión Lasso, que reducen los coeficientes de las variables con alta correlación en el modelo.\n\n- La correlación entre GarageCars y GarageArea es muy alta (0,89) como hemos visto antes, y además tienen una correlación muy alta también con SalePrice. La trataremos quedándonos solamente con la variable compuesta creada anteriormente y eliminando estas dos categorías.\n- Entre TotalBsmtSF y 1stFlrSF encontramos una correlación de 0,80 y una correlación con el precio de venta similar (0,65 y 0,63). Seguramente creemos variables compuestas y las eliminemos también.\n- YearBuilt tiene un poco más de correlación con SalePrice (0,52) que GarageYrBlt (0,49), y una alta correlación entre ellos (0,83)\n- Entre TotRmsAbvGrd y GrLivArea hay una correlación de 0.83, pero TotRmsAbvGrd sólo tiene 0,51 de correlación con SalePrice.\n\nVamos a analizar más profundamente estas variables","metadata":{}},{"cell_type":"markdown","source":"**OverallQual y OverallCond**\n\nSin llevarnos ningún tipo de sorpresa observamos que hay una relación muy directa entre la calidad general y el estado general y el precio de venta, a mayor calidad general o mejor estado, mayor es el precio.\n\nEl crecimiento del precio de venta respecto a 'OverallQual' es exponencial y la correlación con 'SalePrice' es altísima (0.80), en cambio 'verallCond' casi no tiene correlación con 'SalePrice'\n\nConvertiremos el tipo de categoría a numérica, ya que observamos que ambas categorías podría ser tratadas como si fuesen numéricas.","metadata":{}},{"cell_type":"code","source":"# Convertimos el tipo de variable a numérica\ntrain[\"OverallQual\"] = train[\"OverallQual\"].astype(int)\ntest[\"OverallQual\"] = test[\"OverallQual\"].astype(int)\n\ntrain[\"OverallCond\"] = train[\"OverallCond\"].astype(int)\ntest[\"OverallCond\"] = test[\"OverallCond\"].astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:56.673260Z","iopub.execute_input":"2023-05-10T07:43:56.673568Z","iopub.status.idle":"2023-05-10T07:43:56.683109Z","shell.execute_reply.started":"2023-05-10T07:43:56.673541Z","shell.execute_reply":"2023-05-10T07:43:56.681893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creamos el gráfico\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\nsns.boxplot(x='OverallQual', y='SalePrice', data=train, ax=ax1)\nsns.boxplot(x='OverallCond', y='SalePrice', data=train, ax=ax2)\nax1.set_xlabel('Overall Quality', fontsize=12)\nax1.set_ylabel('SalePrice', fontsize=12)\nax1.tick_params(axis='both', which='major', labelsize=10)\nax2.set_xlabel('Overall Condition', fontsize=12)\nax2.set_ylabel('SalePrice', fontsize=12)\nax2.tick_params(axis='both', which='major', labelsize=10)\n\n# Calculamos la correlación entre ambas y SalePrice\ntrain[['OverallQual', 'OverallCond', 'SalePrice']].corr()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:56.684984Z","iopub.execute_input":"2023-05-10T07:43:56.685740Z","iopub.status.idle":"2023-05-10T07:43:57.539327Z","shell.execute_reply.started":"2023-05-10T07:43:56.685702Z","shell.execute_reply":"2023-05-10T07:43:57.537982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **MSZoning y Neighborhood**\n\nVamos a analizar estas dos variables de manera independiente y luego vamos a compararlas entre ellas porque creo que van a tener una gran relación en sus valores.","metadata":{}},{"cell_type":"code","source":"# Calcular el valor medio de SalePrice para cada barrio\nneighborhood_means = train.groupby('Neighborhood')['SalePrice'].mean().sort_values()\n# Crear un objeto de tipo \"OrderedDict\" que asocia cada barrio con su valor medio de SalePrice\nneighborhood_order = OrderedDict(zip(neighborhood_means.index, neighborhood_means.values))\n\n# Calcular el valor medio de SalePrice para MSZoning\nmszoning_means = train.groupby('MSZoning')['SalePrice'].mean().sort_values()\n# Crear un objeto de tipo \"OrderedDict\" que asocia MSZoning con su valor medio de SalePrice\nmszoning_order = OrderedDict(zip(mszoning_means.index, mszoning_means.values))\n\n# Creamos el gráfico\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\nsns.boxplot(x='MSZoning', y='SalePrice', data=train, ax=ax1, order=mszoning_order.keys())\nsns.boxplot(x='Neighborhood', y='SalePrice', data=train, ax=ax2, order=neighborhood_order.keys())\nax1.set_xlabel('MSZoning', fontsize=12)\nax1.set_ylabel('SalePrice', fontsize=12)\nax1.tick_params(axis='both', which='major', labelsize=10)\nax2.set_xlabel('Neighborhood', fontsize=12)\nax2.set_ylabel('SalePrice', fontsize=12)\nax2.tick_params(axis='both', which='major', labelsize=10)\n# Rotamos los valores del eje X de Neighborhood\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:57.541337Z","iopub.execute_input":"2023-05-10T07:43:57.541688Z","iopub.status.idle":"2023-05-10T07:43:58.517837Z","shell.execute_reply.started":"2023-05-10T07:43:57.541655Z","shell.execute_reply":"2023-05-10T07:43:58.516730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Viendo estos gráficos observamos algunas cosas:\n\n- Las viviendas de tipo FV (Floating Village Residential) y RL (Residential Low Density) tienen un precio mucho más elevado que el resto de tipos.\n- Tanto las viviendas de tipo RL como RM tienen muchos valores fuera del límite máximo.\n- Probablemente sea interesante a futuro simplicar esta característica y por ejemplo reducirla a Residencial o No Residencial.\n- La diferencia de precio por barrio es realmente importante:\n    - NoRidge, NridgHt y StoneBr tienen una media de SalePrice superior al 70% respecto al valor medio\n    - MeadowV, IDOTRR y BrDale tienen una media de SalePrice inferior al 40% respecto al valor medio","metadata":{}},{"cell_type":"code","source":"# Generamos un gráfico relacionando las dos variables y SalePrice\nfig, ax = plt.subplots(figsize=(16,8))\nsns.scatterplot(data=train, x='Neighborhood', y='SalePrice', hue='MSZoning', ax=ax)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=10)\nax.set_xlabel('Neighborhood', fontsize=12)\nax.set_ylabel('SalePrice', fontsize=12)\nax.legend(fontsize=10)\nplt.title('Relación entre Neighborhood, MSZoning y SalePrice', fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:58.519087Z","iopub.execute_input":"2023-05-10T07:43:58.519405Z","iopub.status.idle":"2023-05-10T07:43:59.356816Z","shell.execute_reply.started":"2023-05-10T07:43:58.519378Z","shell.execute_reply":"2023-05-10T07:43:59.355347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Observamos que los barrios con el precio medio más alto por vivienda son los que tienen la casi totalidad de sus viviendas de tipo RL o FV\n- Algunos barrios ('MeadowV', 'BrDale', 'Blueste', 'IDOTRR'), tienen la mayoría de sus valores asociados a  RM(Residential Medium Density), y con un SalePrice muy bajo, lo cual nos puede indicar que se puede puede tratar de un barrio residencial del extrarradio con un valor de vivienda bajo.\n\nHaremos una última comprobación para ver el precio medio de venta por barrio comparado con la media y observamos la grandísima diferencia por barrios, luego el barrio en el que se encuentre una vivienda influirá de una manera muy directa en el precio de venta.","metadata":{}},{"cell_type":"code","source":"# Calculamos la media de SalePrice\nglobal_mean_price = train['SalePrice'].mean()\n\n# Agrupamos por barrio y calculamos la media de SalePrice\nneighborhood_mean_price = train.groupby('Neighborhood')['SalePrice'].mean()\n\n# Calculamos el porcentaje de aumento respecto al promedio general\npercent_increase = ((neighborhood_mean_price / global_mean_price) - 1) * 100\n\nprint(percent_increase)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:59.358467Z","iopub.execute_input":"2023-05-10T07:43:59.358997Z","iopub.status.idle":"2023-05-10T07:43:59.369632Z","shell.execute_reply.started":"2023-05-10T07:43:59.358952Z","shell.execute_reply":"2023-05-10T07:43:59.368095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Heating y HeatingQC**\n\nVamos a analizar tanto el tipo de calefación como la calidad de la misma. Estas variables luego las convertiremos a numéricas las dos.","metadata":{}},{"cell_type":"code","source":"# Creamos el gráfico\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\nsns.boxplot(x='Heating', y='SalePrice', data=train, ax=ax1)\nsns.boxplot(x='HeatingQC', y='SalePrice', data=train, ax=ax2)\nax1.set_xlabel('Heating', fontsize=12)\nax1.set_ylabel('SalePrice', fontsize=12)\nax1.tick_params(axis='both', which='major', labelsize=10)\nax2.set_xlabel('HeatingQC', fontsize=12)\nax2.set_ylabel('SalePrice', fontsize=12)\nax2.tick_params(axis='both', which='major', labelsize=10)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:43:59.371162Z","iopub.execute_input":"2023-05-10T07:43:59.371613Z","iopub.status.idle":"2023-05-10T07:44:00.016424Z","shell.execute_reply.started":"2023-05-10T07:43:59.371569Z","shell.execute_reply":"2023-05-10T07:44:00.015330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n    \nY también calculando su precio medio por valor","metadata":{}},{"cell_type":"code","source":"heatingqc_means = train.groupby('HeatingQC')['SalePrice'].mean()\nprint(heatingqc_means)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:00.018094Z","iopub.execute_input":"2023-05-10T07:44:00.018546Z","iopub.status.idle":"2023-05-10T07:44:00.027320Z","shell.execute_reply.started":"2023-05-10T07:44:00.018503Z","shell.execute_reply":"2023-05-10T07:44:00.026090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heating_means = train.groupby('Heating')['SalePrice'].mean()\nprint(heating_means)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:00.028696Z","iopub.execute_input":"2023-05-10T07:44:00.029059Z","iopub.status.idle":"2023-05-10T07:44:00.039595Z","shell.execute_reply.started":"2023-05-10T07:44:00.029029Z","shell.execute_reply":"2023-05-10T07:44:00.038208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Observamos que hay una relación importante entre **HeatingQC** y **SalePrice**, habiendo unas diferencias mayores de 50.000 dólares dependiendo del grado. Nos parece una relación lógica, a mejor calefación mayor precio.\n- Estas diferencias duplican casi el valor de una calidad a otra\n- Observamos también muchos valores fuera de rango en sus puntos máximos\n- También nos afecta de una manera importante el tipo de calefación con el que contamos respecto al precio. Si tenemos una vivienda con calefación de gas el precio va a ser sensiblemente mayor a cualquier otro tipo de calefación.\n- **OthW** y **Wall** tienen todos sus valores en un rango muy acotado, seguramente sea por el poco número de registros con los que cuentan.\n- **FloorFurnace** tiene el valor más bajo de precio de venta, probablemente se asocie con viviendas sin sotano","metadata":{}},{"cell_type":"markdown","source":"Vamos a convertir ahora las dos variables a numéricas para una mejor interpretación en el futuro.\n\nEmpezaremos con **HeatingQC** con estos valores:\n\n    - 5: Ex (215041.45)\n    - 4: Gd (156858.87)\n    - 3: TA (142506.58)    \n    - 2: Fa (123919.49)\n    - 1: Po (87000.00)","metadata":{}},{"cell_type":"code","source":"# Mapeamos primero HeatingQC\n# Creamos un diccionario para reemplazar los nombres\nheatingqc_dict = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1}\n\n# Remplazamos los nombres con los valores numéricos\ntrain['HeatingQC'] = train['HeatingQC'].replace(heatingqc_dict)\ntest['HeatingQC'] = test['HeatingQC'].replace(heatingqc_dict)\n\n# Convertimos la variable a tipo numérico\ntrain['HeatingQC'] = pd.to_numeric(train['HeatingQC'])\ntest['HeatingQC'] = test['HeatingQC'].replace(heatingqc_dict)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:00.041473Z","iopub.execute_input":"2023-05-10T07:44:00.041887Z","iopub.status.idle":"2023-05-10T07:44:00.058321Z","shell.execute_reply.started":"2023-05-10T07:44:00.041854Z","shell.execute_reply":"2023-05-10T07:44:00.057096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Realizamos una relación entre las dos variables y SalePrice y observamos datos interesantes:\n\n- El tipo de calefacción de gas con mayor precio medio de venta de vivienda (**GasA**) es justo el que aglutina la mayor parte de los valores de HeatingQC altos, luego quizás, depende del tipo de calefacción el mantenimiento habrá sido de una manera u otra, y esto ha afectado a su precio.\n- **OthW**, **Wall**, **Grav** y **Floor**, apenas cuentas con valores, y los que cuentan son de un valor muy bajo.\n- Las dos calefaciones con gas son las que aglutinan la mayoria de los valores del dataset.","metadata":{}},{"cell_type":"code","source":"#Creamos el gráfico\nfig, ax = plt.subplots(figsize=(12,8))\nsns.scatterplot(data=train, x='Heating', y='SalePrice', hue='HeatingQC', ax=ax)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=10)\nax.set_xlabel('Heating', fontsize=12)\nax.set_ylabel('SalePrice', fontsize=12)\nax.legend(fontsize=10)\nplt.title('Relación entre Heating, HeatingQC y SalePrice', fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:00.059917Z","iopub.execute_input":"2023-05-10T07:44:00.060275Z","iopub.status.idle":"2023-05-10T07:44:00.676690Z","shell.execute_reply.started":"2023-05-10T07:44:00.060245Z","shell.execute_reply":"2023-05-10T07:44:00.675505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Repetimos el proceso también convirtiendo **'Heating'** en numérica con estos valores:\n\n    - 6: GasA (182065.51)\n    - 5: GasW (166632.17)\n    - 4: OthW (125750.00)    \n    - 3: Wall (92100.00)\n    - 2: Grav (75271.43)\n    - 1: Floor (72500.00)","metadata":{}},{"cell_type":"code","source":"# Creamos un diccionario para reemplazar los nombres\nheating_dict = {'GasA': 6, 'GasW': 5, 'OthW': 4, 'Wall': 3, 'Grav': 2, 'Floor': 1}\n\n# Remplazamos los nombres con los valores numéricos\ntrain['Heating'] = train['Heating'].replace(heating_dict)\ntest['Heating'] = test['Heating'].replace(heating_dict)\n\n# Convertimos la variable a tipo numérico\ntrain['Heating'] = pd.to_numeric(train['Heating'])\ntest['Heating'] = test['Heating'].replace(heating_dict)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:00.678098Z","iopub.execute_input":"2023-05-10T07:44:00.678530Z","iopub.status.idle":"2023-05-10T07:44:00.694063Z","shell.execute_reply.started":"2023-05-10T07:44:00.678497Z","shell.execute_reply":"2023-05-10T07:44:00.693088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **CentralAir**\n\nComprobamos que la presencia de **CentralAir** afecta enormemente al precio de la venta, siendo de casi el doble si tiene o no tiene.\nConvertimos también a numérica esta variable:\n    \n            1: Y (186238.28)\n            2: N (105264.07)","metadata":{}},{"cell_type":"code","source":"centralair_means = train.groupby('CentralAir')['SalePrice'].mean()\nprint(centralair_means)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:00.695453Z","iopub.execute_input":"2023-05-10T07:44:00.695881Z","iopub.status.idle":"2023-05-10T07:44:00.703831Z","shell.execute_reply.started":"2023-05-10T07:44:00.695838Z","shell.execute_reply":"2023-05-10T07:44:00.702558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mostramos la distribución\nsns.set(font_scale=0.8)\nsns.boxplot(x='CentralAir', y='SalePrice', data=train)\nplt.title('Distribución de precios de venta por CentralAir')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:00.705419Z","iopub.execute_input":"2023-05-10T07:44:00.706553Z","iopub.status.idle":"2023-05-10T07:44:00.985734Z","shell.execute_reply.started":"2023-05-10T07:44:00.706513Z","shell.execute_reply":"2023-05-10T07:44:00.984661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creamos un diccionario para reemplazar los nombres\nCentralAir_dict = {'Y': 1, 'N': 0}\n\n# Remplazamos los nombres con los valores numéricos\ntrain['CentralAir'] = train['CentralAir'].replace(CentralAir_dict)\ntest['CentralAir'] = test['CentralAir'].replace(CentralAir_dict)\n\n# Convertimos la variable a tipo numérico\ntrain['CentralAir'] = pd.to_numeric(train['CentralAir'])\ntest['CentralAir'] = pd.to_numeric(test['CentralAir'])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:00.987038Z","iopub.execute_input":"2023-05-10T07:44:00.987347Z","iopub.status.idle":"2023-05-10T07:44:01.000253Z","shell.execute_reply.started":"2023-05-10T07:44:00.987321Z","shell.execute_reply":"2023-05-10T07:44:00.999084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **FireplacesQu**\n\nAnalizamos esta categoría ya que su prima hermana Fireplaces tiene una alta correlación (0.47) con nuestra variable target SalePrice, luego esta quizás también la tenga.\n\nObservamos de la misma manera que con HeatingQC, que el estado de la chimenea afecta de una manera muy importante a su valor, cuanto en mejor estado esté, mayor será el precio de la vivienda.\nUna calificación de Ex es casi el doble que TA.\n\nLa convertimos a variable numérica con esta conversión:\n\n- 5: Ex (337712.50)\n- 4: Gd (226566.30)\n- 3: TA (205844.40)\n- 2: Fa (167298.48)\n- 1: Po (129764.15)","metadata":{}},{"cell_type":"code","source":"fireplacesqu_means = train.groupby('FireplaceQu')['SalePrice'].mean()\nprint(fireplacesqu_means)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:01.001836Z","iopub.execute_input":"2023-05-10T07:44:01.002415Z","iopub.status.idle":"2023-05-10T07:44:01.010369Z","shell.execute_reply.started":"2023-05-10T07:44:01.002383Z","shell.execute_reply":"2023-05-10T07:44:01.009164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale=0.8)\nsns.boxplot(x='FireplaceQu', y='SalePrice', data=train)\nplt.title('Distribución de SalePrices por FireplacesQu')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:01.011777Z","iopub.execute_input":"2023-05-10T07:44:01.012178Z","iopub.status.idle":"2023-05-10T07:44:01.343659Z","shell.execute_reply.started":"2023-05-10T07:44:01.012139Z","shell.execute_reply":"2023-05-10T07:44:01.342473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creamos un diccionario para reemplazar los nombres\nFireplaceQu_dict = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1}\n\n# Remplazamos los nombres con los valores numéricos\ntrain['FireplaceQu'] = train['FireplaceQu'].replace(FireplaceQu_dict)\ntest['FireplaceQu'] = test['FireplaceQu'].replace(FireplaceQu_dict)\n\n# Convertimos la variable a tipo numérico\ntrain['FireplaceQu'] = pd.to_numeric(train['FireplaceQu'])\ntest['FireplaceQu'] = pd.to_numeric(test['FireplaceQu'])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:01.345075Z","iopub.execute_input":"2023-05-10T07:44:01.345468Z","iopub.status.idle":"2023-05-10T07:44:01.360991Z","shell.execute_reply.started":"2023-05-10T07:44:01.345421Z","shell.execute_reply":"2023-05-10T07:44:01.359490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **HouseStyle**\n\nConsidero bastante importante analizar la variable HouseStyle, además de analizarla luego la compararemos con MsZoning ya que podemos intuir que puede haber algún tipo de relación entre ellas.\n\nLas viviendas con un HouseStyle de 2.5Fin tienen un valor medio de más de 220000 dólares, con una diferencia de más de 50000 dólares respecto al tercer tipo más caro.\nSe puede observar que los tipos que tienen un precio de venta más caro son los que implican una casa de mayor tamaño.\n\nAdemás la convertimos a variable numérica con esta conversión:\n\n    - 8: 2.5Fin (220000.00)\n    - 7: 2Story (210221.86)\n    - 6: 1Story (176020.97)\n    - 5: SLvl (166703.38)\n    - 4: 2.5Unf (157354.55)\n    - 3: SFoyer (135074.49)\n    - 2: 1.5Fin (143116.74)\n    - 1: 1.5Unf (110150.00)","metadata":{}},{"cell_type":"code","source":"# Calculamos la media para cada valor\nHouseStyle_means = train.groupby('HouseStyle')['SalePrice'].mean()\nprint(HouseStyle_means)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:01.362896Z","iopub.execute_input":"2023-05-10T07:44:01.363289Z","iopub.status.idle":"2023-05-10T07:44:01.373910Z","shell.execute_reply.started":"2023-05-10T07:44:01.363255Z","shell.execute_reply":"2023-05-10T07:44:01.373032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Mostramos el gráfico\nsns.set(font_scale=0.8)\nsns.boxplot(x='HouseStyle', y='SalePrice', data=train)\nplt.title('Distribución de precios de venta por HouseStyle')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:01.375635Z","iopub.execute_input":"2023-05-10T07:44:01.376353Z","iopub.status.idle":"2023-05-10T07:44:01.781774Z","shell.execute_reply.started":"2023-05-10T07:44:01.376308Z","shell.execute_reply":"2023-05-10T07:44:01.780484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Realizamos una comparación relacionando el **HouseStyle** con **MSZoning** y con **SalePrice** y observamos que las viviendas del tipo RL o FV son las que están asociadas a los HouseStyles más caros.\nPero tampoco observamos una distribución muy cribada para poder determinar nada con esta gráfico","metadata":{}},{"cell_type":"code","source":"#Graficamos la relación\nsns.scatterplot(data=train, x='HouseStyle', y='SalePrice', hue='MSZoning')\nplt.xticks(rotation=90)\nplt.title('Relación entre HouseStyle, MSZoning y SalePrice')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:01.783791Z","iopub.execute_input":"2023-05-10T07:44:01.784246Z","iopub.status.idle":"2023-05-10T07:44:02.325432Z","shell.execute_reply.started":"2023-05-10T07:44:01.784203Z","shell.execute_reply":"2023-05-10T07:44:02.324279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No seguiremos analizando variables categóricas ya que son demasiadas, pasaremos a limpiar de nulos todas las categorías del modelo","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-two-subsection-five\"></a>\n### **2.5 Valores nulos**\n\nUna vez analizadas todas las características pasaremos al preprocesado y limpieza de los datos.\n\nComprobamos el número de nulos con los que contamos en el dataset","metadata":{}},{"cell_type":"code","source":"# Comprobar el número total de nulos en nuestro dataset train\ntrain.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:02.327190Z","iopub.execute_input":"2023-05-10T07:44:02.327629Z","iopub.status.idle":"2023-05-10T07:44:02.343969Z","shell.execute_reply.started":"2023-05-10T07:44:02.327591Z","shell.execute_reply":"2023-05-10T07:44:02.342303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos rapidamente algunas características de las variables","metadata":{}},{"cell_type":"code","source":"# Misma comprobación para test\ntest.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:02.345338Z","iopub.execute_input":"2023-05-10T07:44:02.345709Z","iopub.status.idle":"2023-05-10T07:44:02.373442Z","shell.execute_reply.started":"2023-05-10T07:44:02.345667Z","shell.execute_reply":"2023-05-10T07:44:02.372394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Observamos algunas estadísticas de las categóricas\ncat_var.describe()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:02.374973Z","iopub.execute_input":"2023-05-10T07:44:02.375335Z","iopub.status.idle":"2023-05-10T07:44:02.491109Z","shell.execute_reply.started":"2023-05-10T07:44:02.375306Z","shell.execute_reply":"2023-05-10T07:44:02.489984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observaciones**\n\n    - Tenemos outliers en bastantes categorías, tanto por su parte superior como por su parte inferior.\n    - Tenemos algunas variables con un número de missings muy alto, tenemos que pensar como actuar con ellas.\n    - Hay algunas variables con solamente dos tipos de valores, y que la mayoría tienen una cantidad de valores únicos inferiores a 6\n    - Tenemos más nulos en el conjunto de datos de test que en el de train. \n    \nAhora vamos a verlo de una manera más detallada. Para ello imprimimos las categorías con nulos y que % del total de registro suponen. Es una manera útil de visualizarlo. Además graficaremos mediante una gráfica 'missingno' el resultado. Esta gráfica es una herramienta de visualización que se utiliza para visualizar datos faltantes en un conjunto de datos.","metadata":{}},{"cell_type":"code","source":"# Obtener el número total de filas en el DataFrame\ntotal_rows = len(train)\n\n# Iterar a través de cada columna en el DataFrame\nfor col in train.columns:\n    # Verificar si la columna contiene valores nulos\n    if train[col].isnull().sum() > 0:\n        # Calcular el porcentaje de valores nulos en esa columna\n        null_percentage = (train[col].isnull().sum() / total_rows) * 100\n        # Imprimir el nombre de la columna, el número de valores nulos y el porcentaje de valores nulos en esa columna\n        print(\"\\n{}: {} ({}%)\".format(col, train[col].isnull().sum(), round(null_percentage, 2)))","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:02.492631Z","iopub.execute_input":"2023-05-10T07:44:02.493023Z","iopub.status.idle":"2023-05-10T07:44:02.533612Z","shell.execute_reply.started":"2023-05-10T07:44:02.492993Z","shell.execute_reply":"2023-05-10T07:44:02.532352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extraigo los nombres de la columnas que presentan valores nulos.\nnull_cols = (train.isna()).sum()[train.isna().sum() > 0].index\n\n# Matriz gráfica donde los espacios en blanco representan valores nulos\nmissingno.matrix(train[null_cols])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:02.535394Z","iopub.execute_input":"2023-05-10T07:44:02.535806Z","iopub.status.idle":"2023-05-10T07:44:03.482041Z","shell.execute_reply.started":"2023-05-10T07:44:02.535772Z","shell.execute_reply":"2023-05-10T07:44:03.481016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El resultado de este análisis indica la cantidad de valores faltantes o \"missing\" para cada variable en el conjunto de datos y nos muestra las siguientes observaciones:\n\n- 19 categorías tienen valores nulos\n- De estas 19, solamente tres son numéricas: **LotFrontage**, **MasVnrArea** y **GarageYrBlt**, el resto categóricas\n- Hay algunas categorías que tienen un valor de missings demasiado importante. Consideramos eliminar todas aquellas categorías que tengan un valor de missings superior al 30%, y que en nuestro caso son:\n    - Alley, FireplaceQu, PoolQC, Fence y MiscFeature\n- Hay algunas categorías con un número de missings muy bajo, tendremos que tratarlos de una manera diferente","metadata":{}},{"cell_type":"markdown","source":"Repetimos el mismo análisis para los missings de nuestro dataset de test","metadata":{}},{"cell_type":"code","source":"# Obtener el número total de filas en el DataFrame test\ntotal_rows_test = len(test)\n\n# Iterar a través de cada columna en el DataFrame\nfor col in test.columns:\n    # Verificar si la columna contiene valores nulos\n    if test[col].isnull().sum() > 0:\n        # Calcular el porcentaje de valores nulos en esa columna\n        null_percentage_test = (train[col].isnull().sum() / total_rows) * 100\n        # Imprimir el nombre de la columna, el número de valores nulos y el porcentaje de valores nulos en esa columna\n        print(\"\\n{}: {} ({}%)\".format(col, test[col].isnull().sum(), round(null_percentage, 2)))","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:03.483396Z","iopub.execute_input":"2023-05-10T07:44:03.483730Z","iopub.status.idle":"2023-05-10T07:44:03.533324Z","shell.execute_reply.started":"2023-05-10T07:44:03.483702Z","shell.execute_reply":"2023-05-10T07:44:03.532185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extraigo los nombres de la columnas que presentan valores nulos.\nnull_cols_test = (test.isna()).sum()[test.isna().sum() > 0].index\n\n# Matriz gráfica donde los espacios en blanco representan valores nulos\nmissingno.matrix(test[null_cols_test])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:03.535217Z","iopub.execute_input":"2023-05-10T07:44:03.535560Z","iopub.status.idle":"2023-05-10T07:44:04.792260Z","shell.execute_reply.started":"2023-05-10T07:44:03.535529Z","shell.execute_reply":"2023-05-10T07:44:04.790828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Tenemos más categorías con missings en nuestro conjunto de test que en nuestro conjunto de train\n- Observando los dos gráficos anteriores se ve que faltan muchas variables.\n\nAntes de proceder a un análisis más profundo, tenemos que echar un vistazo a estas variables que faltan durante el proceso de EDA. Después de eso, voy a aplicar la ingeniería de características para hacer frente a los valores que faltan y hacer más variables correlacionadas para hacer un modelo de predicción precisa.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n## **3. Preprocesado y análisis de datos**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-three-subsection-one\"></a>\n### **3.1 Nulos variables numéricas**\n\nDe cara al tratamiento de los missings vamos a seguir con la estrategia de juntar los datasets de train y test, limpiarlos y luego volver a separarlos.","metadata":{}},{"cell_type":"code","source":"# Hacemos una intersección entre Train y Test seleccionando solamente las columnas compartidas (asi nos quitamnos SalePrice)\ncommon_cols = train.columns.intersection(test.columns)\n# Almacenamos en un nuevo dataframe la columna SalePrice \ndf = train[['SalePrice']]\n# Concatenamos los dos datasets\ntotal_data = pd.concat([train[common_cols], test[common_cols]])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:04.794967Z","iopub.execute_input":"2023-05-10T07:44:04.795334Z","iopub.status.idle":"2023-05-10T07:44:04.842247Z","shell.execute_reply.started":"2023-05-10T07:44:04.795303Z","shell.execute_reply":"2023-05-10T07:44:04.841362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comprobamos tamaños\nprint(\"Tamaño de total_data:\", total_data.shape)\nprint(\"Tamaño de train:\", train.shape)\nprint(\"Tamaño de test:\", test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:04.843418Z","iopub.execute_input":"2023-05-10T07:44:04.843727Z","iopub.status.idle":"2023-05-10T07:44:04.849860Z","shell.execute_reply.started":"2023-05-10T07:44:04.843700Z","shell.execute_reply":"2023-05-10T07:44:04.848530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Separación de columnas numéricas y categóricas.\ncat_cols= total_data.select_dtypes(include=['object','category']).columns\nnum_cols = total_data.select_dtypes(exclude=['object','category']).columns","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:04.851369Z","iopub.execute_input":"2023-05-10T07:44:04.851691Z","iopub.status.idle":"2023-05-10T07:44:04.864698Z","shell.execute_reply.started":"2023-05-10T07:44:04.851663Z","shell.execute_reply":"2023-05-10T07:44:04.863448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En primer lugar comprobaremos que las categorías con un valor muy alto de nulos, estas entradas de NaN se tratan de datos perdidos, no de por ejemplo la ausencia de este elemento (por ejemplo en PoolQC, Fence...)\n\nComenzaremos por los nulos de las **variables numéricas**.","metadata":{}},{"cell_type":"code","source":"# Almacenamos todas las columnas con nulos en una variable\ncols_connulos_num = total_data[num_cols].isna().any()\ncols_connulos_num = cols_connulos_num[cols_connulos_num == True].index","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:04.866216Z","iopub.execute_input":"2023-05-10T07:44:04.866654Z","iopub.status.idle":"2023-05-10T07:44:04.878830Z","shell.execute_reply.started":"2023-05-10T07:44:04.866624Z","shell.execute_reply":"2023-05-10T07:44:04.877594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mostramos el total de nulos\ntotal_data[cols_connulos_num].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:04.880345Z","iopub.execute_input":"2023-05-10T07:44:04.880645Z","iopub.status.idle":"2023-05-10T07:44:04.889314Z","shell.execute_reply.started":"2023-05-10T07:44:04.880618Z","shell.execute_reply":"2023-05-10T07:44:04.888174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mostramos su información estadística\ntrain[cols_connulos_num].describe()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:04.890951Z","iopub.execute_input":"2023-05-10T07:44:04.891282Z","iopub.status.idle":"2023-05-10T07:44:04.945842Z","shell.execute_reply.started":"2023-05-10T07:44:04.891254Z","shell.execute_reply":"2023-05-10T07:44:04.944811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tenemos 3 categorías con una cantidad muy alta de nulos, y el resto con unos nulos casi residuales.\nVamos a mostrar los valores únicos de cada una de estas columnas para ver como procedemos","metadata":{}},{"cell_type":"code","source":"# Mostramos los datos de 'LotFrontage'\ntotal_data['LotFrontage'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:04.947042Z","iopub.execute_input":"2023-05-10T07:44:04.947376Z","iopub.status.idle":"2023-05-10T07:44:04.957095Z","shell.execute_reply.started":"2023-05-10T07:44:04.947348Z","shell.execute_reply":"2023-05-10T07:44:04.956020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mostramos los datos de 'FireplaceQu'\ntotal_data['FireplaceQu'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:04.958266Z","iopub.execute_input":"2023-05-10T07:44:04.958555Z","iopub.status.idle":"2023-05-10T07:44:04.971922Z","shell.execute_reply.started":"2023-05-10T07:44:04.958530Z","shell.execute_reply":"2023-05-10T07:44:04.970823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Probamos a comparar los nulos de esta variable con Fireplaces para ver que valor tienen. Observamos que es un 0, es decir NaN -> ausencia de chimenea","metadata":{}},{"cell_type":"code","source":"# Comparamos los nulos de FireplaceQu con Fireplaces\ntotal_data[total_data['FireplaceQu'].isnull()][['Fireplaces','FireplaceQu']]","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:04.973212Z","iopub.execute_input":"2023-05-10T07:44:04.973521Z","iopub.status.idle":"2023-05-10T07:44:04.991538Z","shell.execute_reply.started":"2023-05-10T07:44:04.973494Z","shell.execute_reply":"2023-05-10T07:44:04.990417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mostramos los datos de 'GarageYrBlt'\ntotal_data['GarageYrBlt'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:04.992807Z","iopub.execute_input":"2023-05-10T07:44:04.993230Z","iopub.status.idle":"2023-05-10T07:44:05.003012Z","shell.execute_reply.started":"2023-05-10T07:44:04.993197Z","shell.execute_reply":"2023-05-10T07:44:05.001726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparamos los nulos de GarageYrBlt con YearBuilt, GarageCars y GarageArea\ntotal_data[total_data['GarageYrBlt'].isnull()][['GarageYrBlt','YearBuilt','GarageCars','GarageArea']]","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.004685Z","iopub.execute_input":"2023-05-10T07:44:05.005921Z","iopub.status.idle":"2023-05-10T07:44:05.024113Z","shell.execute_reply.started":"2023-05-10T07:44:05.005881Z","shell.execute_reply":"2023-05-10T07:44:05.023005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- En el caso de **'MasVnrArea'** y **'LotFrontage'** vamos a rellenar sus nulos por la media de sus valores\n- Para la columna **'GarageYrBlt'** hemos comparado la misma con 3 categorías que podían tener mucha relación, 'YearBuilt','GarageCars','GarageArea', observando que siempre que no tenemos año de construcción del garaje corresponde con que el valor en 'GarageCars' y 'GarageArea' es cero, vamos a rellenar sus nulos por ceros.\n- Para **'FireplaceQu'** hemos comparado con Fireplaces, y siempre que hay un NaN hay un cero en Fireplaces, luego rellenaremos por 0.","metadata":{}},{"cell_type":"code","source":"# Rellenamos con la media\ntotal_data['MasVnrArea'] = total_data['MasVnrArea'].fillna(train['MasVnrArea'].mean())\ntotal_data['LotFrontage'] = total_data['LotFrontage'].fillna(train['LotFrontage'].mean())\n\n# Rellenamos con 0's\ntotal_data['GarageYrBlt'] = total_data['GarageYrBlt'].fillna(\"0\")\ntotal_data['FireplaceQu'] = total_data['FireplaceQu'].fillna(\"0\")","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.025576Z","iopub.execute_input":"2023-05-10T07:44:05.026107Z","iopub.status.idle":"2023-05-10T07:44:05.037240Z","shell.execute_reply.started":"2023-05-10T07:44:05.026078Z","shell.execute_reply":"2023-05-10T07:44:05.035950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para el resto de columnas con nulos, debido a su bajo número (1 o 2) sustituiremos esos valores por su moda","metadata":{}},{"cell_type":"code","source":"# Rellenamos por la media\ntotal_data['BsmtFinSF1'] = total_data['BsmtFinSF1'].fillna(total_data['BsmtFinSF1'].mean())\ntotal_data['BsmtFinSF2'] = total_data['BsmtFinSF2'].fillna(total_data['BsmtFinSF2'].mean())\ntotal_data['BsmtUnfSF'] = total_data['BsmtUnfSF'].fillna(total_data['BsmtUnfSF'].mean())\ntotal_data['TotalBsmtSF'] = total_data['TotalBsmtSF'].fillna(total_data['TotalBsmtSF'].mean())\ntotal_data['BsmtFullBath'] = total_data['BsmtFullBath'].fillna(total_data['BsmtFullBath'].mean())\ntotal_data['BsmtHalfBath'] = total_data['BsmtHalfBath'].fillna(total_data['BsmtHalfBath'].mean())\ntotal_data['GarageCars'] = total_data['GarageCars'].fillna(total_data['GarageCars'].mean())\ntotal_data['GarageArea'] = total_data['GarageArea'].fillna(total_data['GarageArea'].mean())\ntotal_data['GarageArea_x_Car'] = total_data['GarageArea_x_Car'].fillna(total_data['GarageArea_x_Car'].mean())","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.038459Z","iopub.execute_input":"2023-05-10T07:44:05.038778Z","iopub.status.idle":"2023-05-10T07:44:05.053997Z","shell.execute_reply.started":"2023-05-10T07:44:05.038751Z","shell.execute_reply":"2023-05-10T07:44:05.053192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comprobamos que ya no tenemos nulos en las columnas numéricas","metadata":{}},{"cell_type":"code","source":"total_data[cols_connulos_num].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.055215Z","iopub.execute_input":"2023-05-10T07:44:05.056015Z","iopub.status.idle":"2023-05-10T07:44:05.072565Z","shell.execute_reply.started":"2023-05-10T07:44:05.055985Z","shell.execute_reply":"2023-05-10T07:44:05.071516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ya tenemos limpiados los nulos de las columnas **numéricas**. Ahora pasaremos a las **categóricas**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-three-subsection-two\"></a>\n### **3.2 Nulos variables categóricas**","metadata":{}},{"cell_type":"code","source":"# Creamos una lista de columnas solamente con las categóricas\ncols_connulos_cat = total_data[cat_cols].isna().any()\ncols_connulos_cat = cols_connulos_cat[cols_connulos_cat == True].index","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.074064Z","iopub.execute_input":"2023-05-10T07:44:05.074453Z","iopub.status.idle":"2023-05-10T07:44:05.118840Z","shell.execute_reply.started":"2023-05-10T07:44:05.074422Z","shell.execute_reply":"2023-05-10T07:44:05.117761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizamos el total de nulos por columna\ntotal_data[cols_connulos_cat].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.120239Z","iopub.execute_input":"2023-05-10T07:44:05.120576Z","iopub.status.idle":"2023-05-10T07:44:05.154649Z","shell.execute_reply.started":"2023-05-10T07:44:05.120547Z","shell.execute_reply":"2023-05-10T07:44:05.153634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Las visualizamos más en detalle\ntotal_data[cols_connulos_cat].describe()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.156064Z","iopub.execute_input":"2023-05-10T07:44:05.156385Z","iopub.status.idle":"2023-05-10T07:44:05.230901Z","shell.execute_reply.started":"2023-05-10T07:44:05.156358Z","shell.execute_reply":"2023-05-10T07:44:05.229757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos que seguramente tengamos que tratar de una manera muy diferente cada una de las diferentes categorías.\nEn primer lugar teníamos 4 categorías con una cantidad enorme de valores nulos (+1000 nulls):\n\n    - Alley\n    - PoolQC\n    - Fence\n    - MiscFeature \n    \nVamos a empezar analizandolas.","metadata":{}},{"cell_type":"markdown","source":"#### **Categorías con una cantidad de nulos demasiado grande**","metadata":{}},{"cell_type":"markdown","source":"**Alley**: Observamos que Alley solamente tiene 2 valores únicos y que probablemente sus NaN significa que no existe callejón","metadata":{}},{"cell_type":"code","source":"# Mostramos la distribución\ntotal_data['Alley'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.231998Z","iopub.execute_input":"2023-05-10T07:44:05.232319Z","iopub.status.idle":"2023-05-10T07:44:05.241047Z","shell.execute_reply.started":"2023-05-10T07:44:05.232291Z","shell.execute_reply":"2023-05-10T07:44:05.239933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PoolQC**: Misma estrategia que la anterior, hacemos una comparación con la variable \"PoolArea\" con la cual guarda una gran relación y observamos que los NaN corresponden con 0s, es decir, cuando no existe hay un NaN","metadata":{}},{"cell_type":"code","source":"# Mostramos la distribución\ntotal_data['PoolQC'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.242857Z","iopub.execute_input":"2023-05-10T07:44:05.243418Z","iopub.status.idle":"2023-05-10T07:44:05.252975Z","shell.execute_reply.started":"2023-05-10T07:44:05.243386Z","shell.execute_reply":"2023-05-10T07:44:05.251923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comprobamos con PoolArea\ntotal_data[total_data['PoolQC'].isnull()][['PoolArea','PoolQC']]","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.254473Z","iopub.execute_input":"2023-05-10T07:44:05.254808Z","iopub.status.idle":"2023-05-10T07:44:05.275430Z","shell.execute_reply.started":"2023-05-10T07:44:05.254780Z","shell.execute_reply":"2023-05-10T07:44:05.274387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fence**: Observamos que Fence solamente tiene 4 valores únicos y que probablemente sus NaN significa que no existe verja","metadata":{}},{"cell_type":"code","source":"# Mostramos la distribución\ntotal_data['Fence'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.292618Z","iopub.execute_input":"2023-05-10T07:44:05.293145Z","iopub.status.idle":"2023-05-10T07:44:05.302146Z","shell.execute_reply.started":"2023-05-10T07:44:05.293077Z","shell.execute_reply":"2023-05-10T07:44:05.301353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MiscFeature**: Observamos que MiscFeature solamente tiene 4 valores únicos y que probablemente sus NaN significa que no existe esa categoría extra","metadata":{}},{"cell_type":"code","source":"# Mostramos la distribución\ntotal_data['MiscFeature'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.303688Z","iopub.execute_input":"2023-05-10T07:44:05.303989Z","iopub.status.idle":"2023-05-10T07:44:05.315031Z","shell.execute_reply.started":"2023-05-10T07:44:05.303962Z","shell.execute_reply":"2023-05-10T07:44:05.313755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Consideramos que los nulos en estas 4 categorías se debe a la falta de esa estructura, luego vamos a sustituir en estos casos el nulo por el valor \"None\". Otra estrategia a utilizar hubiera sido eliminarlas por completo ya que muchas de ellas tienen más de un 30% de valores missings, pero consideramos que esta estrategia es más adecuada para nuestros modelos.","metadata":{}},{"cell_type":"code","source":"# Sustituimos por None\ntotal_data[\"Alley\"].fillna(\"None\", inplace=True)\ntotal_data[\"PoolQC\"].fillna(\"None\", inplace=True)\ntotal_data[\"Fence\"].fillna(\"None\", inplace=True)\ntotal_data[\"MiscFeature\"].fillna(\"None\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.316635Z","iopub.execute_input":"2023-05-10T07:44:05.317063Z","iopub.status.idle":"2023-05-10T07:44:05.327203Z","shell.execute_reply.started":"2023-05-10T07:44:05.317025Z","shell.execute_reply":"2023-05-10T07:44:05.326055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comprobamos los que nos quedan\ntotal_data[cols_connulos_cat].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.328636Z","iopub.execute_input":"2023-05-10T07:44:05.329471Z","iopub.status.idle":"2023-05-10T07:44:05.362953Z","shell.execute_reply.started":"2023-05-10T07:44:05.329427Z","shell.execute_reply":"2023-05-10T07:44:05.361701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ahora iremos analizando una a una el resto de variables con missings","metadata":{}},{"cell_type":"markdown","source":"#### **MasVnrType**","metadata":{}},{"cell_type":"code","source":"# Mostramos la distribución\ntotal_data['MasVnrType'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.364749Z","iopub.execute_input":"2023-05-10T07:44:05.365203Z","iopub.status.idle":"2023-05-10T07:44:05.374599Z","shell.execute_reply.started":"2023-05-10T07:44:05.365170Z","shell.execute_reply":"2023-05-10T07:44:05.373436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comprobamos si la categoría tiene valores nulos, a que se corresponden con otra muy relacionada\ntotal_data[total_data['MasVnrType'].isnull()][['MasVnrArea','MasVnrType']]","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.376221Z","iopub.execute_input":"2023-05-10T07:44:05.376727Z","iopub.status.idle":"2023-05-10T07:44:05.392346Z","shell.execute_reply.started":"2023-05-10T07:44:05.376688Z","shell.execute_reply":"2023-05-10T07:44:05.391284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No detectamos ningún patrón","metadata":{}},{"cell_type":"code","source":"# Calculamos su moda\ntotal_data['MasVnrType'].mode()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.393902Z","iopub.execute_input":"2023-05-10T07:44:05.394259Z","iopub.status.idle":"2023-05-10T07:44:05.405027Z","shell.execute_reply.started":"2023-05-10T07:44:05.394229Z","shell.execute_reply":"2023-05-10T07:44:05.403941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como no podemos asociar ningún valor a MasVnrType respecto a MasVnrArea, así que lo que haremos será reemplazar sus nulos por su moda.","metadata":{}},{"cell_type":"code","source":"total_data['MasVnrType'] = total_data['MasVnrType'].fillna(total_data['MasVnrType'].mode().iloc[0])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.406714Z","iopub.execute_input":"2023-05-10T07:44:05.407169Z","iopub.status.idle":"2023-05-10T07:44:05.415845Z","shell.execute_reply.started":"2023-05-10T07:44:05.407085Z","shell.execute_reply":"2023-05-10T07:44:05.415017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Basement ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2')**\n\nTenemos unas cuantas categorías que podrían estar relacionadas con Basement con nulos, vamos a ver como las podemos tratar.# Visualizamos el total de nulos de cada una\ntotal_data[['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n 'BsmtFinType2']].isna().sum()","metadata":{}},{"cell_type":"code","source":"# Visualizamos el total de nulos de cada una\ntotal_data[['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n 'BsmtFinType2']].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.416763Z","iopub.execute_input":"2023-05-10T07:44:05.417080Z","iopub.status.idle":"2023-05-10T07:44:05.437658Z","shell.execute_reply.started":"2023-05-10T07:44:05.417051Z","shell.execute_reply":"2023-05-10T07:44:05.436510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparamos las variables relacionadas con el Basement con las que no tienen nulos.","metadata":{}},{"cell_type":"code","source":"total_data[total_data['BsmtQual'].isnull()][['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','BsmtFinSF1',\n                        'BsmtFinSF2','BsmtUnfSF','TotalBsmtSF']]","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.438934Z","iopub.execute_input":"2023-05-10T07:44:05.439292Z","iopub.status.idle":"2023-05-10T07:44:05.462833Z","shell.execute_reply.started":"2023-05-10T07:44:05.439265Z","shell.execute_reply":"2023-05-10T07:44:05.462054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Observamos que probablemente datos que faltan pueden deberse a que no hay sótano, comparando con el resto de categorías, cuando tenemos un nulo se corresponde a un 0.\n- Sustituiremos en este caso todos los nulos por \"No_Bsmt\"","metadata":{}},{"cell_type":"code","source":"# Creamos un bucle para cuando tenga un nulo lo sustituya por No_Bsmt\nfor col in ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n 'BsmtFinType2']:\n    total_data[col].fillna('No_Bsmt',inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.463893Z","iopub.execute_input":"2023-05-10T07:44:05.464695Z","iopub.status.idle":"2023-05-10T07:44:05.473473Z","shell.execute_reply.started":"2023-05-10T07:44:05.464664Z","shell.execute_reply":"2023-05-10T07:44:05.472660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comprobamos si se han rellenado\ntotal_data[['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n 'BsmtFinType2']].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.474486Z","iopub.execute_input":"2023-05-10T07:44:05.475470Z","iopub.status.idle":"2023-05-10T07:44:05.498564Z","shell.execute_reply.started":"2023-05-10T07:44:05.475429Z","shell.execute_reply":"2023-05-10T07:44:05.497471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Garage**\n\nSeguimos con la misma estrategia que con Basement. Intentamos observar y tratar conjuntamente todas las categorías relacionadas con Garage","metadata":{}},{"cell_type":"code","source":"# Visualizamos el total de nulos de cada una\ntotal_data[['GarageType','GarageFinish','GarageQual','GarageCond']].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.499868Z","iopub.execute_input":"2023-05-10T07:44:05.500360Z","iopub.status.idle":"2023-05-10T07:44:05.515761Z","shell.execute_reply.started":"2023-05-10T07:44:05.500329Z","shell.execute_reply":"2023-05-10T07:44:05.514728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Las comparamos con el resto de categorías sin nulos relacionadas con Garage","metadata":{}},{"cell_type":"code","source":"total_data[total_data['GarageType'].isnull()][['GarageType','GarageFinish','GarageQual','GarageCond','GarageYrBlt','GarageCars']]","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.517404Z","iopub.execute_input":"2023-05-10T07:44:05.518273Z","iopub.status.idle":"2023-05-10T07:44:05.538930Z","shell.execute_reply.started":"2023-05-10T07:44:05.518217Z","shell.execute_reply":"2023-05-10T07:44:05.537870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comprobamos que cuando el valor es NaN en las variables **'GarageCars'** y **'GarageYrBlt'** es 0, luego no exite garaje en esa vivienda, por tanto rellenaremos los nulos con **\"No_Garage\"**","metadata":{}},{"cell_type":"code","source":"# Creamos un bucle para cuando tenga un nulo lo sustituya por No_Garage\nfor col in ['GarageType','GarageFinish','GarageQual','GarageCond']:\n    total_data[col].fillna('No_Garage',inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.540236Z","iopub.execute_input":"2023-05-10T07:44:05.540539Z","iopub.status.idle":"2023-05-10T07:44:05.549767Z","shell.execute_reply.started":"2023-05-10T07:44:05.540512Z","shell.execute_reply":"2023-05-10T07:44:05.548706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comprobamos si quedan nulos\ntotal_data[['GarageType','GarageFinish','GarageQual','GarageCond']].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.551554Z","iopub.execute_input":"2023-05-10T07:44:05.552223Z","iopub.status.idle":"2023-05-10T07:44:05.567976Z","shell.execute_reply.started":"2023-05-10T07:44:05.552184Z","shell.execute_reply":"2023-05-10T07:44:05.566808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Resto variables categóricas**\n\nPara el resto de variables categóricas tanto en el dataset de train como en el de test, debido a su bajo número y a la alta frecuencia de su moda se sustituirán por esta misma","metadata":{}},{"cell_type":"code","source":"# Las localizamos\ntotal_data[cols_connulos_cat].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.569215Z","iopub.execute_input":"2023-05-10T07:44:05.569530Z","iopub.status.idle":"2023-05-10T07:44:05.602204Z","shell.execute_reply.started":"2023-05-10T07:44:05.569504Z","shell.execute_reply":"2023-05-10T07:44:05.601200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creamos un bucle para cuando tenga un nulo lo sustituya por su moda\nfor col in ['Electrical','MSZoning','Utilities','Exterior1st','Exterior2nd','KitchenQual','Functional','SaleType']:\n    total_data[col].fillna(train[col].mode().iloc[0], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.603747Z","iopub.execute_input":"2023-05-10T07:44:05.604681Z","iopub.status.idle":"2023-05-10T07:44:05.619540Z","shell.execute_reply.started":"2023-05-10T07:44:05.604650Z","shell.execute_reply":"2023-05-10T07:44:05.618708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Número total de valores nulos en el conjunto de datos\nprint(total_data.isnull().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.620563Z","iopub.execute_input":"2023-05-10T07:44:05.621226Z","iopub.status.idle":"2023-05-10T07:44:05.674329Z","shell.execute_reply.started":"2023-05-10T07:44:05.621197Z","shell.execute_reply":"2023-05-10T07:44:05.672982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ya tenemos nuestro dataset limpio de nulos!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-three-subsection-three\"></a>\n### **3.3 Convertir variables categóricas a numéricas**\n\nCodificamos algunas características categóricas como números ordenados cuando hay información en el orden. Luego cambiamos los tipos de categóricas a numéricas. De esta manera nos servirán de una manera mucho más eficaz para el modelado.","metadata":{}},{"cell_type":"code","source":"total_data = total_data.replace({\"Alley\": {\"Grvl\": 1, \"Pave\": 2, \"None\": 0},\n                                 \"BsmtCond\": {\"No_Bsmt\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n                                 \"BsmtExposure\" : {\"No_Bsmt\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3, \"No\" : 0},\n                                 \"BsmtFinType1\" : {\"No_Bsmt\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6},\n                                 \"BsmtFinType2\" : {\"No_Bsmt\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6},\n                                 \"BsmtQual\" : {\"No_Bsmt\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                                 \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                                 \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                                 \"GarageCond\" : {\"No_Garage\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                                 \"GarageQual\" : {\"No_Garage\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                                 \"PoolQC\" : {\"None\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                                 \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                                 \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                                 \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                                 \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                                 \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                                 \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                                 \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}\n                                })","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.677839Z","iopub.execute_input":"2023-05-10T07:44:05.678233Z","iopub.status.idle":"2023-05-10T07:44:05.777659Z","shell.execute_reply.started":"2023-05-10T07:44:05.678201Z","shell.execute_reply":"2023-05-10T07:44:05.776577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Las convertimos a int32\ntotal_data[[\"Alley\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"BsmtQual\", \"ExterCond\", \"ExterQual\", \"GarageCond\", \"GarageQual\", \"PoolQC\", \"Functional\", \"KitchenQual\", \"LandSlope\", \"LotShape\", \"PavedDrive\", \"Street\", \"Utilities\"]] = total_data[[\"Alley\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"BsmtQual\", \"ExterCond\", \"ExterQual\", \"GarageCond\", \"GarageQual\", \"PoolQC\", \"Functional\", \"KitchenQual\", \"LandSlope\", \"LotShape\", \"PavedDrive\", \"Street\", \"Utilities\"]].astype(\"int32\")","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.779124Z","iopub.execute_input":"2023-05-10T07:44:05.779459Z","iopub.status.idle":"2023-05-10T07:44:05.793387Z","shell.execute_reply.started":"2023-05-10T07:44:05.779431Z","shell.execute_reply":"2023-05-10T07:44:05.792403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Observamos los tipos de cada categoría\ntotal_data.info()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.794811Z","iopub.execute_input":"2023-05-10T07:44:05.795117Z","iopub.status.idle":"2023-05-10T07:44:05.838299Z","shell.execute_reply.started":"2023-05-10T07:44:05.795090Z","shell.execute_reply":"2023-05-10T07:44:05.837306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n## **4. Feature Engineering**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-four-subsection-one\"></a>\n### **4.1 Primeros pasos**","metadata":{}},{"cell_type":"markdown","source":"Para una interpretación más fácil y adecuada, vamos a convertir todas las variables numéricas que están en pies o en pies cuadrados al sistema métrico decimal, a metros o metros cuadrados\n\n1 pie (ft) = 0,3048 metros (m)\n\n1 pie cuadrado (sqft) = 0.092903 metros cuadrados (m2)\n\n1 acre = 4046.86 metros cuadrados (m2)\n\nEn nuestro caso no crearemos nuevas variables para los m o m2, sino que convertiremos las actuales.\n\nPara este proceso generaremos un nuevo dataset con todos estos cambios\n","metadata":{}},{"cell_type":"code","source":"# Duplicamos los datasets pero mantienendo el mismo orden de columnas\ntotal_data_proc = total_data.copy().reindex(columns=total_data.columns)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.839937Z","iopub.execute_input":"2023-05-10T07:44:05.840285Z","iopub.status.idle":"2023-05-10T07:44:05.847665Z","shell.execute_reply.started":"2023-05-10T07:44:05.840253Z","shell.execute_reply":"2023-05-10T07:44:05.846646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convertir LotFrontage a metros\ntotal_data_proc['LotFrontage'] = total_data_proc['LotFrontage'] * 0.3048\n\n# Convertir LotArea a metros cuadrados\ntotal_data_proc['LotArea'] = total_data_proc['LotArea'] * 0.092903\n\n# Convertir MasVnrArea a metros cuadrados\ntotal_data_proc['MasVnrArea'] = total_data_proc['MasVnrArea'] * 0.092903\n\n# Convertir BsmtFinSF1 a metros cuadrados\ntotal_data_proc['BsmtFinSF1'] = total_data_proc['BsmtFinSF1'] * 0.092903\n\n# Convertir BsmtFinSF2 a metros cuadrados\ntotal_data_proc['BsmtFinSF2'] = total_data_proc['BsmtFinSF2'] * 0.092903\n\n# Convertir TotalBsmtSF a metros cuadrados\ntotal_data_proc['TotalBsmtSF'] = total_data_proc['TotalBsmtSF'] * 0.092903\n\n# Convertir 1stFlrSF a metros cuadrados\ntotal_data_proc['1stFlrSF'] = total_data_proc['1stFlrSF'] * 0.092903\n\n# Convertir 2ndFlrSF a metros cuadrados\ntotal_data_proc['2ndFlrSF'] = total_data_proc['2ndFlrSF'] * 0.092903\n\n# Convertir LowQualFinSF a metros cuadrados\ntotal_data_proc['LowQualFinSF'] = total_data_proc['LowQualFinSF'] * 0.092903\n\n# Convertir GrLivArea a metros cuadrados\ntotal_data_proc['GrLivArea'] = total_data_proc['GrLivArea'] * 0.092903\n\n# Convertir GarageArea a metros cuadrados\ntotal_data_proc['GarageArea'] = total_data_proc['GarageArea'] * 0.092903\n\n# Convertir WoodDeckSF a metros cuadrados\ntotal_data_proc['WoodDeckSF'] = total_data_proc['WoodDeckSF'] * 0.092903\n\n# Convertir OpenPorchSF a metros cuadrados\ntotal_data_proc['OpenPorchSF'] = total_data_proc['OpenPorchSF'] * 0.092903\n\n# Convertir EnclosedPorch a metros cuadrados\ntotal_data_proc['EnclosedPorch'] = total_data_proc['EnclosedPorch'] * 0.092903\n\n# Convertir 3SsnPorch a metros cuadrados\ntotal_data_proc['3SsnPorch'] = total_data_proc['3SsnPorch'] * 0.092903\n\n# Convertir ScreenPorch a metros cuadrados\ntotal_data_proc['ScreenPorch'] = total_data_proc['ScreenPorch'] * 0.092903\n\n# Convertir PoolArea a metros cuadrados\ntotal_data_proc['PoolArea'] = total_data_proc['PoolArea'] * 0.092903","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.849527Z","iopub.execute_input":"2023-05-10T07:44:05.849863Z","iopub.status.idle":"2023-05-10T07:44:05.872871Z","shell.execute_reply.started":"2023-05-10T07:44:05.849820Z","shell.execute_reply":"2023-05-10T07:44:05.871928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Volvemos a separar los datasets en train y test para poder comparar los resultados de las nuevas variables con la variable target SalePrice","metadata":{}},{"cell_type":"code","source":"# Separamos total_data en dos DataFrames separados, train y test, según la longitud del DataFrame original train.\ntrain = total_data.iloc[:len(train), :]\ntest = total_data.iloc[len(train):, :]","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.874227Z","iopub.execute_input":"2023-05-10T07:44:05.874546Z","iopub.status.idle":"2023-05-10T07:44:05.880548Z","shell.execute_reply.started":"2023-05-10T07:44:05.874518Z","shell.execute_reply":"2023-05-10T07:44:05.879711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Le añadimos la variable target\ntrain.loc[:, 'SalePrice'] = df['SalePrice']","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.881454Z","iopub.execute_input":"2023-05-10T07:44:05.881788Z","iopub.status.idle":"2023-05-10T07:44:05.896106Z","shell.execute_reply.started":"2023-05-10T07:44:05.881737Z","shell.execute_reply":"2023-05-10T07:44:05.895111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Una vez convertidas vamos a crear nuevas variables con las cuales podamos enriquecer nuestro modelo. Para este paso duplicaremos nuestro dataset y añadiremos las nuevas columnas a él en este.","metadata":{}},{"cell_type":"code","source":"# Duplicamos los datasets\ntrain_proc = train.copy()\ntest_proc = test.copy()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.897342Z","iopub.execute_input":"2023-05-10T07:44:05.898640Z","iopub.status.idle":"2023-05-10T07:44:05.909216Z","shell.execute_reply.started":"2023-05-10T07:44:05.898602Z","shell.execute_reply":"2023-05-10T07:44:05.908410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.910405Z","iopub.execute_input":"2023-05-10T07:44:05.911241Z","iopub.status.idle":"2023-05-10T07:44:05.963906Z","shell.execute_reply.started":"2023-05-10T07:44:05.911210Z","shell.execute_reply":"2023-05-10T07:44:05.962709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_proc.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:05.965822Z","iopub.execute_input":"2023-05-10T07:44:05.966297Z","iopub.status.idle":"2023-05-10T07:44:06.020620Z","shell.execute_reply.started":"2023-05-10T07:44:05.966254Z","shell.execute_reply":"2023-05-10T07:44:06.018351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-four-subsection-two\"></a>\n### **4.2 Creación de nuevas categorías**\n\nVamos a crear nuevas categorías que enriquezcan nuestro modelo a partir de las categorías que tenemos actualmente. Estos cambios los aplicaremos tanto en el dataset de train como en el de test.\nAdemás eliminaremos algunas de las categorías que se usaron para crear otras nuevas.","metadata":{}},{"cell_type":"markdown","source":"1. **Área total de la vivienda**: Suma del área de todas las plantas, incluído el sótano.\n\nObtenemos una correlación altísima (0.83), mayor que todas sus variables por separado. Nos relaciona que a mayor TotalSF mayor SalePrice\n","metadata":{}},{"cell_type":"code","source":"# Creamos la nueva categoría\ntrain_proc['TotalSF'] = train_proc['TotalBsmtSF'] + train_proc['1stFlrSF'] + train_proc['2ndFlrSF']\ntest_proc['TotalSF'] = test_proc['TotalBsmtSF'] + test_proc['1stFlrSF'] + test_proc['2ndFlrSF']\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"TotalSF\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Imprimimos un gráfico de dispersión que nos relacione las dos\nsns.scatterplot(x=train_proc[\"TotalSF\"], y=train_proc[\"SalePrice\"], data=train_proc)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:06.022419Z","iopub.execute_input":"2023-05-10T07:44:06.023082Z","iopub.status.idle":"2023-05-10T07:44:06.429411Z","shell.execute_reply.started":"2023-05-10T07:44:06.023039Z","shell.execute_reply":"2023-05-10T07:44:06.428417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. **Área total de construcción**: Suma del área total de todos los elementos construidos en la vivienda. Incluímos porches, garages etc.\n\nObtenemos una correlación con la variable target enorme (0.86), que nos relaciona que a mayor ConstructArea mayor SalePrice","metadata":{}},{"cell_type":"code","source":"# Creamos la nueva categoría\ntrain_proc['ConstructArea'] = (train_proc.TotalBsmtSF + train_proc.WoodDeckSF + train_proc.GrLivArea +\n                             train_proc.OpenPorchSF + train_proc.ScreenPorch + train_proc.EnclosedPorch +\n                             train_proc.MasVnrArea + train_proc.GarageArea + train_proc.PoolArea )\n\ntest_proc['ConstructArea'] = (test_proc.TotalBsmtSF + test_proc.WoodDeckSF + test_proc.GrLivArea +\n                             test_proc.OpenPorchSF + test_proc.ScreenPorch + test_proc.EnclosedPorch +\n                             test_proc.MasVnrArea + test_proc.GarageArea + test_proc.PoolArea )\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"ConstructArea\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Imprimimos un gráfico de dispersión que nos relacione las dos\nsns.scatterplot(x=train_proc[\"ConstructArea\"], y=train_proc[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:06.430675Z","iopub.execute_input":"2023-05-10T07:44:06.431065Z","iopub.status.idle":"2023-05-10T07:44:06.814586Z","shell.execute_reply.started":"2023-05-10T07:44:06.431037Z","shell.execute_reply":"2023-05-10T07:44:06.813526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. **Área total de la vivienda habitable**: Suma del área de todas las plantas, excluído el sótano. Es interesante saber este valor para saber cuanta superficie habitable tienen. También tiene una correlación altísima (0.74), aunque menor que las anteriores","metadata":{}},{"cell_type":"code","source":"# Creamos la nueva categoría\ntrain_proc['TotalSFH']=train_proc['1stFlrSF'] + train_proc['2ndFlrSF']\n\ntest_proc['TotalSFH']=test_proc['1stFlrSF'] + test_proc['2ndFlrSF']\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"TotalSFH\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Imprimimos un gráfico de dispersión que nos relacione las dos\nsns.scatterplot(x=train_proc[\"TotalSFH\"], y=train_proc[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:06.816031Z","iopub.execute_input":"2023-05-10T07:44:06.816393Z","iopub.status.idle":"2023-05-10T07:44:07.172861Z","shell.execute_reply.started":"2023-05-10T07:44:06.816364Z","shell.execute_reply":"2023-05-10T07:44:07.171779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. **Área total de porches**: Suma del área total de porches de la vivienda, sumando todos los tipos de porches disponibles. Obtenemos una correlación con la variable SalePrice bastante más baja (0.39), además gráficamente no se ve tan claro cuando aumenta uno aumenta el precio ya que tiene una correlación positiva debil.","metadata":{}},{"cell_type":"code","source":"# Creamos la nueva categoría\ntrain_proc['Total_porch_sf'] = (train_proc['OpenPorchSF'] + train_proc['3SsnPorch'] +\n                              train_proc['EnclosedPorch'] + train_proc['ScreenPorch'] +\n                              train_proc['WoodDeckSF'])\n\ntest_proc['Total_porch_sf'] = (test_proc['OpenPorchSF'] + test_proc['3SsnPorch'] +\n                              test_proc['EnclosedPorch'] + test_proc['ScreenPorch'] +\n                              test_proc['WoodDeckSF'])\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"Total_porch_sf\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Imprimimos un gráfico de dispersión que nos relacione las dos\nsns.scatterplot(x=train_proc[\"Total_porch_sf\"], y=train_proc[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:07.174655Z","iopub.execute_input":"2023-05-10T07:44:07.175279Z","iopub.status.idle":"2023-05-10T07:44:07.559201Z","shell.execute_reply.started":"2023-05-10T07:44:07.175246Z","shell.execute_reply":"2023-05-10T07:44:07.558027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. **Superficie total de garajes**: Tamaño total de garaje por vivienda. Se consigue multiplicando el área de cada garaje por el número de coches. En este caso obtenemos una alta correlación positiva (0.70) y en el gráfico observamos como parece que se agrupan en 3 grandes bloques.","metadata":{}},{"cell_type":"code","source":"# Creamos la nueva categoría\ntrain_proc['TotalGarageSize'] = train_proc['GarageArea'] * train_proc['GarageCars']\n\ntest_proc['TotalGarageSize'] = test_proc['GarageArea'] * test_proc['GarageCars']\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"TotalGarageSize\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Imprimimos un gráfico de dispersión que nos relacione las dos\nsns.scatterplot(x=train_proc[\"TotalGarageSize\"], y=train_proc[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:07.561118Z","iopub.execute_input":"2023-05-10T07:44:07.561505Z","iopub.status.idle":"2023-05-10T07:44:07.973075Z","shell.execute_reply.started":"2023-05-10T07:44:07.561474Z","shell.execute_reply":"2023-05-10T07:44:07.971973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6. **Superficie habitable por habitación**: refleja la distribución del espacio habitable en una propiedad. De esta manera podemos comprobar el tamaño medio por habitación habitable. La hemos conseguido sumando las dos superficies habitables de cada vivienda (Primera planta y segunda) y dividiéndolas por el número medio de habitaciones. Conseguimos una correlación positiva moderada (0.55)","metadata":{}},{"cell_type":"code","source":"# Creamos la nueva categoría\ntrain_proc[\"Amplitud\"] = (train_proc['1stFlrSF'] + train_proc['2ndFlrSF']) / train_proc['TotRmsAbvGrd']\n    \ntest_proc[\"Amplitud\"] = (test_proc['1stFlrSF'] + test_proc['2ndFlrSF']) / test_proc['TotRmsAbvGrd']\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"Amplitud\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Imprimimos un gráfico de dispersión que nos relacione las dos\nsns.scatterplot(x=train_proc[\"Amplitud\"], y=train_proc[\"SalePrice\"])# Creamos la nueva categoría","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:07.974333Z","iopub.execute_input":"2023-05-10T07:44:07.974658Z","iopub.status.idle":"2023-05-10T07:44:08.392157Z","shell.execute_reply.started":"2023-05-10T07:44:07.974628Z","shell.execute_reply":"2023-05-10T07:44:08.391159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"7. **Tiene Sótano**: Mostramos si tiene sótano o no\n\n    0: No tiene\n    \n    1: Tiene\n    \nPodemos observar que la ausencia o presencia de sótano en una vivienda influye de una manera muy significativa en el valor de la misma, ya que el precio medio de una vivienda con sótano es un 73.19% más alto que si no tuviera","metadata":{}},{"cell_type":"code","source":"# Creamos la nueva categoría\ntrain_proc['HasBsmt'] = pd.Series(len(train['TotalBsmtSF']), index=train.index)\ntrain_proc['HasBsmt'] = 0 \ntrain_proc.loc[train['TotalBsmtSF'] > 0, 'HasBsmt'] = 1\n\ntest_proc['HasBsmt'] = pd.Series(len(test['TotalBsmtSF']), index=test.index)\ntest_proc['HasBsmt'] = 0 \ntest_proc.loc[test['TotalBsmtSF'] > 0, 'HasBsmt'] = 1\n\n# Calculamos la media\nBsmt_means = train_proc.groupby('HasBsmt')['SalePrice'].mean()\n\n# Imprimir los resultados\nprint(Bsmt_means)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:08.393705Z","iopub.execute_input":"2023-05-10T07:44:08.394043Z","iopub.status.idle":"2023-05-10T07:44:08.407499Z","shell.execute_reply.started":"2023-05-10T07:44:08.394014Z","shell.execute_reply":"2023-05-10T07:44:08.406371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"8. **Número total de baños**: Suma de todos los baños, medio baños, en sótano etc en cada vivienda. Obtenemos una correlación importante (0.64)","metadata":{}},{"cell_type":"code","source":"# Creamos la nueva categoría\ntrain_proc['Total_Bathrooms'] = (train_proc['FullBath'] + (0.5 * train_proc['HalfBath']) +\n                               train_proc['BsmtFullBath'] + (0.5 * train_proc['BsmtHalfBath']))\n\ntest_proc['Total_Bathrooms'] = (test_proc['FullBath'] + (0.5 * test_proc['HalfBath']) +\n                               test_proc['BsmtFullBath'] + (0.5 * test_proc['BsmtHalfBath']))\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"Total_Bathrooms\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Imprimimos un gráfico de dispersión que nos relacione las dos\nsns.scatterplot(x=train_proc[\"Total_Bathrooms\"], y=train_proc[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:08.409000Z","iopub.execute_input":"2023-05-10T07:44:08.409808Z","iopub.status.idle":"2023-05-10T07:44:08.799359Z","shell.execute_reply.started":"2023-05-10T07:44:08.409765Z","shell.execute_reply":"2023-05-10T07:44:08.798286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"9. **Edad de la casa en el momento de la venta**: Para calcular la edad de la casa en el momento de la venta, necesitamos restar el año en que se vendió la casa del año en que se construyó la casa\n\nEn este caso tenemos una correlación negativa moderada (-0.52), cuánto más vieja sea la casa, menor es su valor","metadata":{}},{"cell_type":"code","source":"# Creamos la nueva categoría\ntrain_proc[\"Age\"] = train_proc[\"YrSold\"] - train_proc[\"YearBuilt\"]\ntrain_proc.loc[train_proc[\"Age\"] < 0, \"Age\"] = train_proc[\"YearBuilt\"]\n\ntest_proc[\"Age\"] = test_proc[\"YrSold\"] - test_proc[\"YearBuilt\"]\ntest_proc.loc[test_proc[\"Age\"] < 0, \"Age\"] = test_proc[\"YearBuilt\"]\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"Age\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Imprimimos un gráfico de dispersión que nos relacione las dos\nsns.scatterplot(x=train_proc[\"Age\"], y=train_proc[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:08.800672Z","iopub.execute_input":"2023-05-10T07:44:08.801014Z","iopub.status.idle":"2023-05-10T07:44:09.219785Z","shell.execute_reply.started":"2023-05-10T07:44:08.800984Z","shell.execute_reply":"2023-05-10T07:44:09.218603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"10. **Vivienda Remodelada**: Calcularemos si la vivienda ha sido remodelada o no\n\n    0: No remodelada\n    \n    1: Remodelada\n\nNos sorprende que el valor de las viviendas NO remodeladas es ligeramente superior al de las viviendas remodeladas. Además tiene una correlación casi nula. Esta característica no nos aportará demasiado.","metadata":{}},{"cell_type":"code","source":"# Creamos la nueva categoría\ntrain_proc['Remodeled'] = (train_proc['YearBuilt'] != train_proc['YearRemodAdd']).astype(int)\ntest_proc['Remodeled'] = (test_proc['YearBuilt'] != test_proc['YearRemodAdd']).astype(int)\n\n# Calculamos la media\nremodeled_means = train_proc.groupby('Remodeled')['SalePrice'].mean()\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"Remodeled\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Imprimir los resultados\nprint(remodeled_means)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:09.221862Z","iopub.execute_input":"2023-05-10T07:44:09.222330Z","iopub.status.idle":"2023-05-10T07:44:09.235361Z","shell.execute_reply.started":"2023-05-10T07:44:09.222288Z","shell.execute_reply":"2023-05-10T07:44:09.234279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"11. **Intérvalo entre remodelación y venta**: Tiempo que ha pasado entre la última remodelación y la venta, si no ha habido remodelación en ese caso, desde la construcción a la venta. Observamos una correlación negativa moderada, a menor intérvalo mayor precio de venta. Podemos asociar esto a que cuanto más tiempo haya pasado desde la remodelación, en peor estado estará.","metadata":{}},{"cell_type":"code","source":"# Se crea una nueva columna que nos contabiliza el tiempo que ha pasado desde la remodelación a la venta \ntrain_proc['RemodAge'] = train_proc['YrSold'] - train_proc['YearRemodAdd']\ntrain_proc.loc[train_proc['RemodAge'] < 0, 'RemodAge'] = train_proc['Age'].loc[train_proc['RemodAge'] < 0]\n\ntest_proc['RemodAge'] = test_proc['YrSold'] - test_proc['YearRemodAdd']\ntest_proc.loc[test_proc['RemodAge'] < 0, 'RemodAge'] = test_proc['Age'].loc[test_proc['RemodAge'] < 0]\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"RemodAge\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Imprimimos un gráfico de dispersión que nos relacione las dos\nsns.scatterplot(x=train_proc[\"RemodAge\"], y=train_proc[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:09.237246Z","iopub.execute_input":"2023-05-10T07:44:09.237912Z","iopub.status.idle":"2023-05-10T07:44:09.646821Z","shell.execute_reply.started":"2023-05-10T07:44:09.237871Z","shell.execute_reply":"2023-05-10T07:44:09.645860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"12. **Tiene Piscina**: Mostramos si tiene piscina o no\n\n    0: No tiene\n    \n    1: Tiene\n    \nPodemos observar que la ausencia o presencia de piscina en una vivienda influye de una manera muy significativa en el valor de la misma, ya que el precio medio de una vivienda con piscina es un 64.68% más alto que si no tuviera. Sin embargo su correlación con SalePrice es muy baja","metadata":{}},{"cell_type":"code","source":"# Creamos la nueva categoría\ntrain_proc['haspool'] = train_proc['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntest_proc['haspool'] = test_proc['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\npool_means = train_proc.groupby('haspool')['SalePrice'].mean()\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"haspool\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Imprimir los resultados\nprint(pool_means)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:09.647957Z","iopub.execute_input":"2023-05-10T07:44:09.648271Z","iopub.status.idle":"2023-05-10T07:44:09.665738Z","shell.execute_reply.started":"2023-05-10T07:44:09.648244Z","shell.execute_reply":"2023-05-10T07:44:09.664364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"13. **Tiene Verja**: Mostramos si tiene verja o no\n\n    0: No tiene\n    \n    1: Tiene\n    \nPodemos observar que la ausencia o presencia de verja en una vivienda influye de una manera relativa en el valor de la misma, ya que el precio medio de una vivienda sin verja es 35000 euros mayor que si tiene. Vemos que tiene un ligera correlación negativa. Estos valores habría que tomarlos con cuidado ya que la presencia de verja pasa solamente en un 19,75% de los datos","metadata":{}},{"cell_type":"code","source":"# Comprobamos la distribución\ntrain_proc['Fence'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:09.667404Z","iopub.execute_input":"2023-05-10T07:44:09.667919Z","iopub.status.idle":"2023-05-10T07:44:09.680548Z","shell.execute_reply.started":"2023-05-10T07:44:09.667888Z","shell.execute_reply":"2023-05-10T07:44:09.679539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos que hay diferentes tipos de verjas y luego un valor que es None","metadata":{}},{"cell_type":"code","source":"# Creamos la nueva categoría\ntrain_proc[\"HasFence\"] = np.where(train_proc[\"Fence\"] == \"None\", 0, 1)\ntest_proc[\"HasFence\"] = np.where(test_proc[\"Fence\"] == \"None\", 0, 1)\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"HasFence\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Calculamos la media\nFence_means = train_proc.groupby('HasFence')['SalePrice'].mean()\nprint(Fence_means)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:09.681808Z","iopub.execute_input":"2023-05-10T07:44:09.682203Z","iopub.status.idle":"2023-05-10T07:44:09.694096Z","shell.execute_reply.started":"2023-05-10T07:44:09.682171Z","shell.execute_reply":"2023-05-10T07:44:09.693324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_proc['HasFence'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:09.695583Z","iopub.execute_input":"2023-05-10T07:44:09.695927Z","iopub.status.idle":"2023-05-10T07:44:09.707193Z","shell.execute_reply.started":"2023-05-10T07:44:09.695896Z","shell.execute_reply":"2023-05-10T07:44:09.705869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"14. **Estaciones**: Queremos valorar si la estación influye en el precio de venta\n\n1: Invierno\n\n2: Primavera\n\n3: Verano\n\n4: Otoño\n\nPero observamos que no hay una gran diferencia de precio entre las viviendas que se venden en según que estación ya que el precio medio de una vivienda es de aproximadamente 181,234.09 euros y si las comparamos individualmente:\n\nInvierno: 182741.69 dólares, es ligeramente mayor que la media en un 0.77%\n\nPrimavera 174628.73 dólares es un 3.72% menor que la media\n\nVerano 182282.67 dólares es ligeramente menor que la media en un 0.24%\n\nOtoño 188285.27 dólares es un 3.90% mayor que la media","metadata":{}},{"cell_type":"code","source":"# Creamos un diccionario para mapear los meses a estaciones\nestaciones = {1: '1', 2: '1', 3: '2', 4: '2', 5: '2', 6: '3', 7: '3', 8: '3', 9: '4', 10: '4', 11: '4', 12: '1'}\n\n# Creamos una nueva columna 'Estacion' en train_proc y test_proc\ntrain_proc['Season'] = train_proc['MoSold'].map(estaciones).astype(int)\ntest_proc['Season'] = test_proc['MoSold'].map(estaciones).astype(int)\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"Season\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n#Calculamos la media\nseason_means = train_proc.groupby('Season')['SalePrice'].mean()\n\n# Imprimir los resultados\nprint(season_means)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:09.708568Z","iopub.execute_input":"2023-05-10T07:44:09.708893Z","iopub.status.idle":"2023-05-10T07:44:09.726175Z","shell.execute_reply.started":"2023-05-10T07:44:09.708865Z","shell.execute_reply":"2023-05-10T07:44:09.725189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utiliza value_counts para contar el número de ocurrencias en la columna \"Season\"\ncount = train_proc['Season'].value_counts()\n\n# Calcula los porcentajes de cada valor\npercentages = count / len(train_proc['Season']) * 100\n\n# Imprime el resultado con un mensaje adicional\nfor season, count, percentage in zip(count.index, count, percentages):\n    print(f\"El valor {season} aparece en {percentage:.2f}% ({count}) del total de casas vendidas.\")","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:09.727543Z","iopub.execute_input":"2023-05-10T07:44:09.728369Z","iopub.status.idle":"2023-05-10T07:44:09.736881Z","shell.execute_reply.started":"2023-05-10T07:44:09.728336Z","shell.execute_reply":"2023-05-10T07:44:09.735925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos afirmar categóricamente que las viviendas se venden de una manera mucho mayor en primavera o verano, siendo otoño y sobre todo invierno las estaciones con un índice mucho más bajo de ventas.","metadata":{}},{"cell_type":"markdown","source":"15. **Calidad general de la vivienda**: la calidad de la casa y su condición son dos factores importantes que influyen en su precio de venta, y multiplicarlos puede resultar en una medida más completa de su calidad general.\n\nAl calcular su correlación con SalePrice y su relación gráficamente podemos observar que tienen una correlación positiva moderada, conforme en mejor estado esté la vivienda, más alto será el precio de la misma.\n\nEn este caso no borraremos las variables a raiz de las cuales hemos calculado la compuesta ya que tienen una correlación mayor que la que acabamos de generar","metadata":{}},{"cell_type":"code","source":"# Creamos la nueva categoría\ntrain_proc[\"OverallGrade\"] = train_proc[\"OverallQual\"] * train_proc[\"OverallCond\"]\ntest_proc[\"OverallGrade\"] = test_proc[\"OverallQual\"] * test_proc[\"OverallCond\"]\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"OverallGrade\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Imprimimos un gráfico de dispersión que nos relacione las dos\nsns.scatterplot(x=train_proc[\"OverallGrade\"], y=train_proc[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:09.738374Z","iopub.execute_input":"2023-05-10T07:44:09.738732Z","iopub.status.idle":"2023-05-10T07:44:10.115622Z","shell.execute_reply.started":"2023-05-10T07:44:09.738701Z","shell.execute_reply":"2023-05-10T07:44:10.114599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"16. **Grado del exterior**: grado de calidad del exterior de la vivienda. Conseguimos esta variable multiplicando la calidad general del exterior (\"ExterQual\") por la condición de este exterior (\"ExterCond\"). De esta manera creamos una variable compuesta que nos relaciona estas dos\nHemos convertido previamente las dos variables a tipo numérico para poder hacer esta operación:\n\n    - Ex: 5\n\n    - Gd: 4\n\n    - TA: 3\n\n    - Fa: 2\n\n    - Po: 1\n\nObservamos que tenemos una correlación moderada-alta (0.57) con SalePrice, podemos asegurar que un valor mayor de la calidad general del exterior influirá en un precio mayor de nuestro piso.","metadata":{}},{"cell_type":"code","source":"# Generamos la nueva variable\ntrain_proc[\"ExterGrade\"] = train_proc[\"ExterQual\"] * train_proc[\"ExterCond\"]\ntest_proc[\"ExterGrade\"] = test_proc[\"ExterQual\"] * test_proc[\"ExterCond\"]\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"ExterGrade\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Imprimimos un gráfico de dispersión que nos relacione las dos\nsns.scatterplot(x=train_proc[\"ExterGrade\"], y=train_proc[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:10.117204Z","iopub.execute_input":"2023-05-10T07:44:10.117513Z","iopub.status.idle":"2023-05-10T07:44:10.491550Z","shell.execute_reply.started":"2023-05-10T07:44:10.117486Z","shell.execute_reply":"2023-05-10T07:44:10.490387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"17. **Grado del garaje**: grado general del garaje de la vivienda. Igual que en el caso anterior, las variables que utilizaremos para conseguir esta nueva eran variables categóricas, entonces las hemos convertido en numéricas para poder conseguir esta nueva categoría.\n\n- Ex: 5\n\n- Gd: 4\n\n- TA: 3\n\n- Fa: 2\n\n- Po: 1\n\n- NA: 0\n\nComprobamos que la correlación positiva que conseguimos (0,27), no es excesivamente grande, luego podremos afirmar, que el grado del garaje influye solamente de una manera no muy significativa en el SalePrice","metadata":{}},{"cell_type":"code","source":"# Generamos la nueva variable\ntrain_proc[\"GarageGrade\"] = train_proc[\"GarageQual\"] * train_proc[\"GarageCond\"]\ntest_proc[\"GarageGrade\"] = test_proc[\"GarageQual\"] * test_proc[\"GarageCond\"]\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"GarageGrade\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Imprimimos un gráfico de dispersión que nos relacione las dos\nsns.scatterplot(x=train_proc[\"GarageGrade\"], y=train_proc[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:10.493199Z","iopub.execute_input":"2023-05-10T07:44:10.493712Z","iopub.status.idle":"2023-05-10T07:44:10.878221Z","shell.execute_reply.started":"2023-05-10T07:44:10.493670Z","shell.execute_reply":"2023-05-10T07:44:10.877080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"18. **Casa comprada antes de la venta**: saber si la casa fue comprada sobre plano o ya estaba construída puede ser un dato importante para saber.\n\nCreamos esta variable sustituyendo por 0 todos los valores de SaleCondition excepto los de si la casa no fue terminada antes de su venta.\n\n  - 1: Comprada sobre plano\n  - 0: Comprada ya construída\n\nObservamos que tiene una correlación moderada, pero sin embargo, su precio de venta si la casa fue comprada sobre plano es un 58.93% mayor que si ya estaba construída.","metadata":{}},{"cell_type":"code","source":"# Creamos la nueva variable\ntrain_proc[\"BoughtOffPlan\"] = train_proc.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \n                                                      \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\n\ntest_proc[\"BoughtOffPlan\"] = test_proc.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \n                                                      \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\n\n# Calculamos su correlación con la variable target\ncorr = train_proc[\"BoughtOffPlan\"].corr(train_proc[\"SalePrice\"])\nprint(\"Correlación:\", corr, end='\\n\\n')\n\n# Calculamos la media\nBoughtOffPlan_means = train_proc.groupby('BoughtOffPlan')['SalePrice'].mean()\nprint(BoughtOffPlan_means)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:10.879560Z","iopub.execute_input":"2023-05-10T07:44:10.879961Z","iopub.status.idle":"2023-05-10T07:44:10.896752Z","shell.execute_reply.started":"2023-05-10T07:44:10.879931Z","shell.execute_reply":"2023-05-10T07:44:10.895938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Una vez creadas todas las nuevas categorías vamos a eliminar algunas de las que nos han servido para crear estas últimas ya que muchas presentan problemas de colinealidad y no nos aportan más valor que las compuestas","metadata":{}},{"cell_type":"code","source":"train_proc = train_proc.drop(['GarageArea', 'GarageCars', '1stFlrSF', '2ndFlrSF', 'TotalBsmtSF','FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath','ExterQual', 'ExterCond','GarageQual', 'GarageCond'], axis=1)\ntest_proc = test_proc.drop(['GarageArea', 'GarageCars', '1stFlrSF', '2ndFlrSF', 'TotalBsmtSF','FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath','ExterQual', 'ExterCond','GarageQual', 'GarageCond'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:10.897920Z","iopub.execute_input":"2023-05-10T07:44:10.898971Z","iopub.status.idle":"2023-05-10T07:44:10.910313Z","shell.execute_reply.started":"2023-05-10T07:44:10.898912Z","shell.execute_reply":"2023-05-10T07:44:10.909283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-four-subsection-three\"></a>\n### **4.3 Transformación logarítmica de la variable target ('SalePrice')**\n\nCómo hemos podido observar previamente nuestra variable target tiene una ligera asimetría y una distribución que se aleja de la normal. Vamos a estudiarlo con más detalle\n","metadata":{}},{"cell_type":"code","source":"# Asimetría y kurtosis\nprint(\"Skewness: %f\" % train_proc['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_proc['SalePrice'].kurt())","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:10.911977Z","iopub.execute_input":"2023-05-10T07:44:10.912319Z","iopub.status.idle":"2023-05-10T07:44:10.923138Z","shell.execute_reply.started":"2023-05-10T07:44:10.912292Z","shell.execute_reply":"2023-05-10T07:44:10.922101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Asimetría**: El valor de 1.882876 indica que la distribución es asimétrica y que la cola de la distribución se extiende hacia la derecha. Esto significa que hay una mayor concentración de valores más bajos de 'SalePrice' y una menor concentración de valores más altos.\n\n**Kurtosis**: El valor de 6.536282 indica que la distribución es leptocúrtica, lo que significa que tiene una concentración relativamente alta de valores alrededor de la media y una cola más pesada que la distribución normal. En otras palabras, los valores de 'SalePrice' están más concentrados alrededor de la media y hay una mayor frecuencia de valores extremos, en comparación con una distribución normal.\n\nPor tanto vamos a imprimir la distribución de la variable para observar estas anomalías gráficamente","metadata":{}},{"cell_type":"code","source":"# Crear una figura con tres subplots\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# Plot de la distribución Johnson SU\nsns.distplot(train_proc['SalePrice'], kde=False, fit=stats.johnsonsu, ax=axes[0])\naxes[0].set_title(\"Johnson SU\")\n\n# Plot de la distribución normal\nsns.distplot(train_proc['SalePrice'], kde=False, fit=stats.norm, ax=axes[1])\naxes[1].set_title(\"Normal\")\n\n# Plot de la distribución log-normal\nsns.distplot(train_proc['SalePrice'], kde=False, fit=stats.lognorm, ax=axes[2])\naxes[2].set_title(\"Log Normal\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:10.924652Z","iopub.execute_input":"2023-05-10T07:44:10.924966Z","iopub.status.idle":"2023-05-10T07:44:12.499338Z","shell.execute_reply.started":"2023-05-10T07:44:10.924940Z","shell.execute_reply":"2023-05-10T07:44:12.498168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En el análisis de distribución de la variable objetivo **'SalePrice'** observamos que la dispersión de los datos no sigue una distribución normal, sino que se ajusta mejor a una distribución log-normal o una distribución Johnson SU.\n\nA su vez, la distribución de **'SalePrice'** es casi idéntica a la distribución Johnson SU, lo que sugiere que los datos pueden ser modelados por esta distribución. Esto será importante para tener en cuenta al realizar análisis estadísticos, ya que algunas técnicas y pruebas estadísticas asumen una distribución normal de los datos. Por lo tanto, puede ser necesario ajustar el modelo para tener en cuenta la distribución no normal de 'SalePrice'.","metadata":{}},{"cell_type":"code","source":"# Prueba de normalidad de la variable SalePrice\nstat, p = shapiro(train_proc['SalePrice'])\n\n# Prueba de hipótesis\nalpha = 0.05\nif p > alpha:\n    print(\"La distribución de SalePrice es normal (no se rechaza la hipótesis nula)\")\nelse:\n    print(\"La distribución de SalePrice no es normal (se rechaza la hipótesis nula)\")","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:12.500776Z","iopub.execute_input":"2023-05-10T07:44:12.501083Z","iopub.status.idle":"2023-05-10T07:44:12.506974Z","shell.execute_reply.started":"2023-05-10T07:44:12.501055Z","shell.execute_reply":"2023-05-10T07:44:12.505874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se realiza una prueba de normalidad en **'SalePrice'** utilizando la prueba de **Shapiro-Wilk**. Si el valor de p es mayor que el nivel de significancia (0.05), se concluye que la distribución de **'SalePrice'** es normal y no se rechaza la hipótesis nula. En caso contrario, se concluye que la distribución no es normal y se rechaza la hipótesis nula.\n\nNuestro resultado nos confirma que la distribución **no es normal**, entonces aplicaremos una transformación logarítmica para poder adaptar su distribucion a la normal.","metadata":{}},{"cell_type":"code","source":"# Imprimir el histograma y las probabilidades antes de la transformación\nfig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(train_proc['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(train_proc['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frecuencia')\nplt.title('distribución SalePrice')\nplt.subplot(1,2,2)\nres = stats.probplot(train_proc['SalePrice'], plot=plt)\nplt.suptitle('Antes de la transformación')\n\n# Aplicar transformación logarítmica \ntrain_proc.SalePrice = np.log1p(train_proc.SalePrice )\n# Almacenar los valores transformados en una nueva variable\ny_train = train_proc.SalePrice.values\ny_train_orig = train_proc.SalePrice\n\n# Imprimir el histograma y las probabilidades después de la transformación\nfig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(train_proc['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(train_proc['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frecuencia')\nplt.title('distribución SalePrice')\nplt.subplot(1,2,2)\nres = stats.probplot(train_proc['SalePrice'], plot=plt)\nplt.suptitle('Después de la transformación')","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:12.508423Z","iopub.execute_input":"2023-05-10T07:44:12.508864Z","iopub.status.idle":"2023-05-10T07:44:14.316795Z","shell.execute_reply.started":"2023-05-10T07:44:12.508833Z","shell.execute_reply":"2023-05-10T07:44:14.315814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Los valores de mu y sigma son los parámetros de la distribución normal que mejor se ajusta a los datos de la variable SalePrice antes y después de la transformación logarítmica, respectivamente.\n\nEn la primera figura, antes de la transformación, se puede observar que la distribución de SalePrice tiene una asimetría positiva y una cola larga hacia la derecha. La media (mu) de la distribución es 180921.20 y su desviación estándar (sigma) es 79415.29.\n\nEn la segunda figura, después de aplicar la transformación logarítmica, se puede ver que la distribución se aproxima a una distribución normal, es decir, con una simetría mucho más cercana. La media (mu) de la distribución es 12.02 y su desviación estándar (sigma) es 0.4.\n\nHemos conseguido reducir la heterocedasticidad en SalePrice","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-four-subsection-four\"></a>\n### **4.4 Transformación Box-Cox de las variables excesivamente asimétricas**\n\nVamos a comprobar la asimetría de las variables con las que contamos, para ajustarlas a la normal, tal como hemos hecho con la variable target","metadata":{}},{"cell_type":"code","source":"# Visualizamos la asimetría de todas\nfor col in train_proc.columns:\n    if train_proc[col].dtype != 'object':\n        print(f\"{col}: {train_proc[col].skew()}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:14.318380Z","iopub.execute_input":"2023-05-10T07:44:14.318674Z","iopub.status.idle":"2023-05-10T07:44:14.343046Z","shell.execute_reply.started":"2023-05-10T07:44:14.318648Z","shell.execute_reply":"2023-05-10T07:44:14.341687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mostramos las 20 con mayor asimetría\nasymmetry = train_proc.skew()\ntop_20_asymmetry = asymmetry.abs().nlargest(20)\nprint(top_20_asymmetry)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:44:14.344414Z","iopub.execute_input":"2023-05-10T07:44:14.344844Z","iopub.status.idle":"2023-05-10T07:44:14.374905Z","shell.execute_reply.started":"2023-05-10T07:44:14.344789Z","shell.execute_reply":"2023-05-10T07:44:14.373807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Considerando que tener una asimetría mayor de 0.50 o menor de 0.50 es malo para el entrenamiento de modelos, podemos observar que tenemos un problema, ya que tenemos un montón de categorías fuera de este rango.\n\nVamos a eliminar las 5 primeras ya que tienen una asimetría enorme y no nos van a ayudar en nuestro modelo","metadata":{}},{"cell_type":"code","source":"train_proc = train_proc.drop(['Utilities', 'Street', 'PoolArea', 'PoolQC','MiscVal'], axis=1)\ntest_proc = test_proc.drop(['Utilities', 'Street', 'PoolArea', 'PoolQC','MiscVal'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:47:09.040971Z","iopub.execute_input":"2023-05-10T07:47:09.041422Z","iopub.status.idle":"2023-05-10T07:47:09.178281Z","shell.execute_reply.started":"2023-05-10T07:47:09.041378Z","shell.execute_reply":"2023-05-10T07:47:09.176488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Datasets duplicados para efectuar pruebas\n#train_pruebas = train_proc.copy()\n#test_pruebas = test_proc.copy()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:23:37.818017Z","iopub.execute_input":"2023-05-10T07:23:37.818739Z","iopub.status.idle":"2023-05-10T07:23:37.829687Z","shell.execute_reply.started":"2023-05-10T07:23:37.818682Z","shell.execute_reply":"2023-05-10T07:23:37.828573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aplicamos la transformación **Box-Cox** a nuestras variables numéricas, de esta manera podemos corregir o atenuar su asimetría. Aplicar esta transformación puede mejorar la precisión de nuestros modelos al hacer que los datos sean más cercanos a una distribución normal. Hemos realizado también pruebas utilizando la transformación **Yeo-Johnson**, que es similar a la transformación **Box-Cox**, pero es capaz de manejar datos con valores negativos y cero, pero no ha dado lugar a mejores resultados. También hemos probado a utilizar una transformación logarítmica y tampoco conseguimos un resultado mejor.\n\nEste proceso lo vamos a hacer tanto para el dataset train como para el dataset test también. Para el caso del dataset train, excluiremos la variable SalePrice porque ya la hemos tratado previamente.\n\nInformación extra [transformación Box-Cox](https://www.statisticshowto.com/probability-and-statistics/normal-distributions/box-cox-transformation/)","metadata":{}},{"cell_type":"code","source":"# Seleccionar las variables numéricas excluyendo SalePrice\nnumeric_features = train_proc.select_dtypes(include=['int64', 'int32', 'float64']).drop(['SalePrice'], axis=1).columns.tolist()\n\n# Calcular la asimetría de las variables numéricas\nskew_features_before = train_proc[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n\n# Seleccionar las variables con alta asimetría\nhigh_skew = skew_features_before[skew_features_before > 0.5]\nskew_index = high_skew.index\n\n# Aplicar transformación de Box-Cox a las variables con alta asimetría\nskew_features_after = []\nfor i in skew_index:\n    train_proc[i], lam = boxcox(train_proc[i]+1)\n    skew_features_after.append(skew(train_proc[i]))\n\n# Mostrar tabla con variables, asimetría antes y después de transformación\ntable = pd.DataFrame({'Variable': skew_index,\n                      'Asimetría antes': skew_features_before[skew_index].values,\n                      'Asimetría después': skew_features_after})\nprint(table)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:47:17.899636Z","iopub.execute_input":"2023-05-10T07:47:17.900633Z","iopub.status.idle":"2023-05-10T07:47:18.068594Z","shell.execute_reply.started":"2023-05-10T07:47:17.900584Z","shell.execute_reply":"2023-05-10T07:47:18.065665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La tabla muestra el cambio en la asimetría de cada variable después de aplicar una transformación para corregir la asimetría. Cómo hemos explicado previamente, un valor de cero indica una distribución perfectamente simétrica y valores positivos o negativos indican diferentes grados de asimetría a la derecha o a la izquierda, respectivamente.\n\nPor ejemplo, la variable **LotArea** tenía una asimetría de 12.62 antes de la transformación y después de la transformación, la asimetría se redujo significativamente a 0.04. Esto indica que la distribución original era muy asimétrica a la derecha, y después de la transformación, la distribución se acercó a la simetría.\n\nPor otro lado, la variable **haspool** tenía una asimetría de 15.48 antes y después de la transformación, lo que indica que esta variable ya era bastante simétrica desde el principio y la transformación no tuvo un gran impacto en su asimetría. He intentado reducirla con una transformación de raíz cuadrada y con una transformación de potencia fraccionaria pero no ha resultado.\n\nEn resumen, esta tabla muestra cómo la aplicación de una transformación puede ayudar a corregir la asimetría en las variables del dataset y mejorar la calidad de los análisis y modelos posteriores.\n\nHemos aplicado el mismo proceso al dataset de test.","metadata":{}},{"cell_type":"code","source":"# Seleccionar las variables numéricas\nnumeric_features_test = test_proc.select_dtypes(include=['int64', 'int32', 'float64']).columns.tolist()\n\n# Calcular la asimetría de las variables numéricas\nskew_features_before = test_proc[numeric_features_test].apply(lambda x: skew(x)).sort_values(ascending=False)\n\n# Seleccionar las variables con alta asimetría\nhigh_skew = skew_features_before[skew_features_before > 0.5]\nskew_index = high_skew.index\n\n# Aplicar transformación de Box-Cox a las variables con alta asimetría\nskew_features_after = []\nfor i in skew_index:\n    test_proc[i], lam = boxcox(test_proc[i]+1)\n    skew_features_after.append(skew(test_proc[i]))\n\n# Mostrar tabla con variables, asimetría antes y después de transformación\ntable = pd.DataFrame({'Variable': skew_index,\n                      'Asimetría antes': skew_features_before[skew_index].values,\n                      'Asimetría después': skew_features_after})\nprint(table)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:48:23.794239Z","iopub.execute_input":"2023-05-10T07:48:23.794623Z","iopub.status.idle":"2023-05-10T07:48:23.942051Z","shell.execute_reply.started":"2023-05-10T07:48:23.794593Z","shell.execute_reply":"2023-05-10T07:48:23.940918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vamos a eliminar más variables que tienen una asimetría excesivamente alta y nos van a perjudicar en la predicción de nuestro modelo>","metadata":{}},{"cell_type":"code","source":"train_proc = train_proc.drop(['3SsnPorch', 'LowQualFinSF', 'Alley', 'EnclosedPorch','BsmtFinSF2'], axis=1)\ntest_proc = test_proc.drop(['3SsnPorch', 'LowQualFinSF', 'Alley', 'EnclosedPorch','BsmtFinSF2'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:48:28.288884Z","iopub.execute_input":"2023-05-10T07:48:28.289344Z","iopub.status.idle":"2023-05-10T07:48:28.301602Z","shell.execute_reply.started":"2023-05-10T07:48:28.289306Z","shell.execute_reply":"2023-05-10T07:48:28.300444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-four-subsection-five\"></a>\n### **4.5 Escalado de variables numéricas**\n\nLa idea es que al escalar las variables numéricas, facilitemos el entrenamiento de modelos de aprendizaje automático que pueden tener dificultades para trabajar con variables que tienen diferentes escalas.\n\nExisten diferentes técnicas de escalado, como el escalado estándar (StandardScaler) que se utiliza en este código, el escalado min-max y el escalado por robustez, cada uno con sus propias ventajas y desventajas dependiendo del conjunto de datos y el modelo de aprendizaje automático que se está utilizando.\n\nEl escalado **StandardScaler** es una técnica de preprocesamiento de datos que se utiliza para estandarizar las características (o variables) de un conjunto de datos. Consiste en ajustar y transformar las características para que tengan una media de cero y una desviación estándar de uno. Esto significa que los datos se distribuyen en torno a la media de cero y tienen una variación similar. El resultado es que las características están en la misma escala y pueden ser comparables entre sí. Esto es especialmente útil para algoritmos de aprendizaje automático que requieren que las variables tengan un rango similar para un rendimiento óptimo. Normalmente se aplica generalmente a variables numéricas continuas y tiene la ventaja de que es fácil de interpretar y aplicar.\n\nMás información en este [Enlace](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n","metadata":{}},{"cell_type":"code","source":"# Seleccionar las variables numéricas excluyendo SalePrice\nnumeric_features = train_proc.select_dtypes(include=['int64', 'int32', 'float64']).columns.tolist()\nnumeric_features.remove('SalePrice')\n\n# Escalar las variables numéricas con StandardScaler\nscaler = StandardScaler()\ntrain_scaled = scaler.fit_transform(train_proc[numeric_features])\n\n# Crear un nuevo DataFrame con las variables escaladas y las categóricas\ntrain_scaled = pd.DataFrame(train_scaled, columns=numeric_features, index=train_proc.index)\ntrain_scaled = pd.concat([train_scaled, train_proc.select_dtypes(include=['object', 'category']), train_proc['SalePrice']], axis=1)\n\n# Calcular la asimetría de las variables escaladas\nskew_features_after = train_scaled[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n\n# Plotear la asimetría antes y después de escalar\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].set_title('Antes de escalar')\nskew_features_before.plot(kind='bar', ax=ax[0])\nax[1].set_title('Después de escalar')\nskew_features_after.plot(kind='bar', ax=ax[1])\nplt.tight_layout()\n\nplt.xticks(fontsize=8)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:49:46.291456Z","iopub.execute_input":"2023-05-10T07:49:46.291836Z","iopub.status.idle":"2023-05-10T07:49:48.416308Z","shell.execute_reply.started":"2023-05-10T07:49:46.291806Z","shell.execute_reply":"2023-05-10T07:49:48.415486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos que realizando este escalado, conseguimos una redución de la asimetría muy importante en un gran número de variables, igualandola a cero y en otros casos reduciendo de gran manera su valor.\n\nPor otra parte hay algunas categorías que siguen teniendo una asimetría muy alta en términos positivos (**'haspool', 'ScreenPorch', 'BoughtOffPlan'**) o negativos (**'Heating', 'HasBsmt', 'Functional'**)\nConsidero que tanto **'haspool'** como **'Heating'** son demasiado importantes para nuestra predicción como para eliminarlas\n\nAplicaremos este escalado también a nuestro dataset test","metadata":{}},{"cell_type":"code","source":"# Escalar las variables numéricas con StandardScaler\nscaler = StandardScaler()\ntest_scaled = scaler.fit_transform(test_proc[numeric_features])\n\n# Crear un nuevo DataFrame con las variables escaladas y las categóricas\ntest_scaled = pd.DataFrame(test_scaled, columns=numeric_features, index=test_proc.index)\ntest_scaled = pd.concat([test_scaled, test_proc.select_dtypes(include=['object', 'category'])], axis=1)\n\n# Calcular la asimetría de las variables escaladas\nskew_features_after = test_scaled[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n\n# Plotear la asimetría antes y después de escalar\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].set_title('Antes de escalar')\nskew_features_before.plot(kind='bar', ax=ax[0])\nax[1].set_title('Después de escalar')\nskew_features_after.plot(kind='bar', ax=ax[1])\nplt.tight_layout()\n\nplt.xticks(fontsize=8)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T07:51:35.172414Z","iopub.execute_input":"2023-05-10T07:51:35.172921Z","iopub.status.idle":"2023-05-10T07:51:37.281876Z","shell.execute_reply.started":"2023-05-10T07:51:35.172876Z","shell.execute_reply":"2023-05-10T07:51:37.280849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-four-subsection-six\"></a>\n### **4.6 Variables Dummies**\n\nLas variables dummies son variables ficticias que se utilizan para representar una variable categórica o nominal en un modelo estadístico. Al crear variables dummies, se asigna un valor numérico a cada categoría de la variable original, lo que permite su inclusión en un modelo de regresión lineal.\n\nAl crear variables dummies para cada categoría de la variable categórica, se pueden incluir en un modelo de regresión lineal y analizar cómo cada categoría afecta a la variable de interés.\n\nAdemás, las variables dummies pueden ayudar a controlar el efecto de variables confusas o variables de control en el modelo, lo que puede mejorar la precisión de las estimaciones.\n\nMás información en este [enlace](https://timeseriesreasoning.com/contents/dummy-variables-in-a-regression-model/)","metadata":{}},{"cell_type":"code","source":"# Comprobamos el tamaño de train\ntrain_scaled.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:01:24.897862Z","iopub.execute_input":"2023-05-10T08:01:24.898288Z","iopub.status.idle":"2023-05-10T08:01:24.905263Z","shell.execute_reply.started":"2023-05-10T08:01:24.898252Z","shell.execute_reply":"2023-05-10T08:01:24.904204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comprobamos el tamaño de test\ntest_scaled.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:01:29.158136Z","iopub.execute_input":"2023-05-10T08:01:29.161476Z","iopub.status.idle":"2023-05-10T08:01:29.168541Z","shell.execute_reply.started":"2023-05-10T08:01:29.161419Z","shell.execute_reply":"2023-05-10T08:01:29.167359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Volvemos a juntar los datasets almacenando aparte 'SalePrice'\ncommon_cols = train_scaled.columns.intersection(test_scaled.columns)\ndf = train_scaled[['SalePrice']]\nall_data = pd.concat([train_scaled[common_cols], test_scaled[common_cols]])","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:01:33.626301Z","iopub.execute_input":"2023-05-10T08:01:33.626682Z","iopub.status.idle":"2023-05-10T08:01:33.638993Z","shell.execute_reply.started":"2023-05-10T08:01:33.626652Z","shell.execute_reply":"2023-05-10T08:01:33.637836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creamos nuestras variables dummies eliminando las categoricas a partir de las cuales creamos las dummies\nall_data = pd.get_dummies(all_data, drop_first=True)\nprint(all_data.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:01:34.503444Z","iopub.execute_input":"2023-05-10T08:01:34.504331Z","iopub.status.idle":"2023-05-10T08:01:34.543492Z","shell.execute_reply.started":"2023-05-10T08:01:34.504297Z","shell.execute_reply":"2023-05-10T08:01:34.542429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos como ha aumentado el tamaño de nuestro dataset hasta 317 categorías, gracias a las variables dummies","metadata":{}},{"cell_type":"code","source":"# Separamos total_data en dos DataFrames separados, train y test, según la longitud de train_scaled.\ntrain_end = all_data.iloc[:len(train_scaled), :]\ntest_end = all_data.iloc[len(train_scaled):, :]","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:01:36.478206Z","iopub.execute_input":"2023-05-10T08:01:36.478590Z","iopub.status.idle":"2023-05-10T08:01:36.484035Z","shell.execute_reply.started":"2023-05-10T08:01:36.478557Z","shell.execute_reply":"2023-05-10T08:01:36.483217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comprobamos que tienen las mismas columnas","metadata":{}},{"cell_type":"code","source":"train_end.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:01:45.062708Z","iopub.execute_input":"2023-05-10T08:01:45.063106Z","iopub.status.idle":"2023-05-10T08:01:45.070032Z","shell.execute_reply.started":"2023-05-10T08:01:45.063076Z","shell.execute_reply":"2023-05-10T08:01:45.068868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_end.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:01:45.302246Z","iopub.execute_input":"2023-05-10T08:01:45.302670Z","iopub.status.idle":"2023-05-10T08:01:45.310688Z","shell.execute_reply.started":"2023-05-10T08:01:45.302638Z","shell.execute_reply":"2023-05-10T08:01:45.309407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n## **5. Modelos**\n\nUna vez con nuestros datasets preprocesados y con todas sus variables numéricas procedemos a buscar cual es el modelo más adecuado para entrenar nuestro dataset y sacar el mejor rendimiento\n\nDividiremos nuestro modelo de entrenamiento en un 80/20 y elegiremos un valor de semilla aleatoria random_state=42 para asegurar que la división entre conjuntos de entrenamiento y prueba sea reproducible en futuras ejecuciones del código.","metadata":{}},{"cell_type":"code","source":"# Separar las características y la variable objetivo\nX = train_end\ny = train_scaled['SalePrice']\n\n# Dividir los datos en conjuntos de entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:01:48.539388Z","iopub.execute_input":"2023-05-10T08:01:48.539765Z","iopub.status.idle":"2023-05-10T08:01:48.552383Z","shell.execute_reply.started":"2023-05-10T08:01:48.539737Z","shell.execute_reply":"2023-05-10T08:01:48.551113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:01:48.747379Z","iopub.execute_input":"2023-05-10T08:01:48.747799Z","iopub.status.idle":"2023-05-10T08:01:48.753194Z","shell.execute_reply.started":"2023-05-10T08:01:48.747765Z","shell.execute_reply":"2023-05-10T08:01:48.752217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-five-subsection-one\"></a>\n### **5.1 Estrategias**","metadata":{}},{"cell_type":"markdown","source":"#### **Cross-validation**\n\n- La validación cruzada es una estrategia fundamental para evaluar la capacidad predictiva de un modelo de regresión logística.\n- Permite evitar el sobreajuste del modelo a los datos de entrenamiento, lo que mejora su capacidad de generalización.\n- La elección adecuada del número de folds en la validación cruzada puede tener un impacto significativo en la precisión de la evaluación del modelo.\n- Es importante elegir un esquema de validación cruzada apropiado, por ejemplo, stratified k-fold cross-validation, para evitar la introducción de sesgos en la evaluación del modelo.","metadata":{}},{"cell_type":"code","source":"# Cálculo de la puntuación de la validación cruzada con la puntuación fijada en el error medio absoluto negativo\ndef cross_validation(model):\n    \n    scores = np.sqrt(-cross_val_score(model, X_train, y_train, cv = 12, scoring = \"neg_mean_squared_error\"))\n    mean = np.mean(scores)\n    print(\"Mean CV score: \",mean)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:01:50.340936Z","iopub.execute_input":"2023-05-10T08:01:50.341357Z","iopub.status.idle":"2023-05-10T08:01:50.347780Z","shell.execute_reply.started":"2023-05-10T08:01:50.341326Z","shell.execute_reply":"2023-05-10T08:01:50.346527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **RMSE**\n\n- Es una métrica comúnmente utilizada para medir la precisión de un modelo de regresión.\n- Representa la raíz cuadrada de la diferencia entre el valor real y el valor predicho al cuadrado, promediado sobre todas las observaciones\n- Cuanto menor sea el valor del RMSE, mayor será la precisión del modelo.","metadata":{}},{"cell_type":"code","source":"# Función para calcular el Error Cuadrático Medio (RMSE)\ndef rmse(y_pred, y_train): \n    \n    rmse_ = np.sqrt(metrics.mean_squared_error(y_pred,y_train))\n    print(\"rmse: \", rmse_)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:01:50.819428Z","iopub.execute_input":"2023-05-10T08:01:50.820334Z","iopub.status.idle":"2023-05-10T08:01:50.826709Z","shell.execute_reply.started":"2023-05-10T08:01:50.820297Z","shell.execute_reply":"2023-05-10T08:01:50.825778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Plotear relación Real-Predicción**\n\nGenero una función para graficar (diagrama de dispersión) una comparación entre los valores reales y los valores predichos por un modelo de regresión.","metadata":{}},{"cell_type":"code","source":"# Función para plotear actual-predicción\ndef actual_vs_pred_plot(y_train,y_pred):\n    \n    fig = plt.figure(figsize=(12,12))\n    fig, ax = plt.subplots()\n    \n    ax.scatter(y_train, y_pred,color = \"red\",edgecolor = 'black')\n    ax.plot([y_train.min(),y_train.max()], [y_train.min(), y_train.max()], 'k--',lw=0.2)\n    ax.set_xlabel('Real')\n    ax.set_ylabel('Predicción')\n    plt.suptitle(\"Gráfico de dispersión real frente a predicción\",size=14)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:01:52.473802Z","iopub.execute_input":"2023-05-10T08:01:52.474230Z","iopub.status.idle":"2023-05-10T08:01:52.482093Z","shell.execute_reply.started":"2023-05-10T08:01:52.474192Z","shell.execute_reply":"2023-05-10T08:01:52.480932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-five-subsection-two\"></a>\n### **5.2 Regresión Lineal**","metadata":{}},{"cell_type":"markdown","source":"El primer modelo que utilizamos es uno de regresión lineal. En este modelo, se espera que el valor objetivo sea una combinación lineal de las características. Los coeficientes se fijan para minimizar la suma residual de cuadrados entre el objetivo previsto y las características observadas.\n\n**Parámetros utilizados**:\n\n- **fit_intercept**: un valor booleano que indica si se debe incluir o no el intercepto (constante) en el modelo. El valor por defecto es True.\n\n- **normalize**: un valor booleano que indica si se deben normalizar o no las variables explicativas antes de ajustar el modelo. Si se establece en True, todas las variables explicativas se escalan para tener media cero y varianza unitaria antes de ajustar el modelo. El valor por defecto es False.\n\nMás información en el siguiente [enlace](https://ebac.mx/blog/regreson-lineal)\n\n\n**Pros**:\n- Fácil de entender e interpretar.\n- Rápido de implementar y computacionalmente eficiente.\n- Puede ser utilizado como punto de partida para modelos más complejos.\n\n**Contras**:\n- Asunción de linealidad entre las variables independientes y dependientes.\n- Sensible a los valores atípicos y la presencia de puntos influyentes.\n- No es adecuado para relaciones no lineales entre variables.","metadata":{}},{"cell_type":"code","source":"# Crear una instancia de la clase Regresión Lineal\nreg = linear_model.LinearRegression(fit_intercept=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:02:01.166345Z","iopub.execute_input":"2023-05-10T08:02:01.166729Z","iopub.status.idle":"2023-05-10T08:02:01.172286Z","shell.execute_reply.started":"2023-05-10T08:02:01.166698Z","shell.execute_reply":"2023-05-10T08:02:01.170953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calcular su validación cruzada\ncross_validation(reg)\n\n#Ajustamos el modelo\nmodel_reg = reg.fit(X_train, y_train)\n\n#Predecimos el valor de SalePrice en el df de entrenamiento\ny1_pred = reg.predict(X_train)\n\n#Calculamos el error cuadrático medio\nrmse(y1_pred,y_train)\n\n#Predecimos el valor de SalePrice en el df de test\ny1_pred_v = reg.predict(X_test)\n\n#Calculamos el error cuadrático medio\nrmse(y1_pred_v, y_test)\n\n# Calculamos coeficiente de determinación (R-cuadrado) \ny1_pred_v = reg.predict(X_test)\nr2 = r2_score(y_test, y1_pred_v)\nprint(\"R-cuadrado: {:.2f}\".format(r2))\n\n#Ploteamos el resultado\nactual_vs_pred_plot(y_test,y1_pred_v)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:02:03.249519Z","iopub.execute_input":"2023-05-10T08:02:03.250161Z","iopub.status.idle":"2023-05-10T08:02:05.317114Z","shell.execute_reply.started":"2023-05-10T08:02:03.250102Z","shell.execute_reply":"2023-05-10T08:02:05.316070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El valor del **RMSE** (error cuadrático medio) es de 0.0850, lo cual indica que el modelo tiene un error promedio de predicción de aproximadamente 0.084 unidades de la variable de respuesta en la escala en la que está medida. Esto nos indica que tiene una precisión razonable para predecir los valores de la variable de respuesta\n\nEl otro valor que mostramos es el puntaje de validación cruzada (**CV**), que es una medida de rendimiento del modelo. Este puntaje se calcula utilizando una técnica de validación cruzada, que divide los datos en conjuntos de entrenamiento y prueba, y luego ajusta el modelo y lo evalúa en cada conjunto de prueba. \n\nEl valor del puntaje de **CV** promedio se da como 593910323.935, lo cual indica que el modelo tiene un alto error promedio de predicción en el conjunto de datos de prueba utilizado en la validación cruzada. Probablemente el modelo podría no estar generalizando bien a datos nuevos o desconocidos. \n\nEl resultado del coeficiente de determinación (**R-cuadrado**) también es desastroso.\n\nPor lo tanto, tenemos que o bien ajustar el modelo o explorar otras técnicas de modelado para mejorar su rendimiento ya que este no nos sirve.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-five-subsection-three\"></a>\n### **5.3 Regresión Ridge**","metadata":{}},{"cell_type":"markdown","source":"El segundo modelo que hemos utilizado es la regresión Ridge. \n\nLa regresión Ridge es un modelo de regresión lineal regularizado que utiliza una penalización L2 para evitar el sobreajuste del modelo a los datos de entrenamiento. La función de coste que se optimiza incluye tanto la suma de los errores al cuadrado como una penalización proporcional a la suma de los cuadrados de los coeficientes del modelo. Esto da lugar a una solución que restringe la magnitud de los coeficientes, lo que reduce la complejidad del modelo y mejora su capacidad de generalización. El parámetro de regularización se ajusta mediante validación cruzada para encontrar el mejor equilibrio entre ajuste y generalización del modelo.\n\nMás información en este [enlace](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n\nEn nuestro caso utilizaremos los siguientes parámetros:\n\n  - **Alpha**: medida de la fuerza de regularización aplicada al modelo. Buscaremos el mejor valor mediante un pipeline.\n  - **cv**: número de folds o pliegues en la validación cruzada que se utilizará para evaluar el rendimiento del modelo Ridge con diferentes valores de alfa. Elegiremos 10 para nuestro modelo\n  \n**Pros**:\n- Puede reducir la varianza del modelo y evitar el sobreajuste.\n- Capaz de lidiar con la multicolinealidad.\n- Puede ser utilizado como punto de partida para modelos más complejos.\n\n**Contras**:\n- No es útil para seleccionar variables importantes.\n- No es muy interpretable.\n- Puede requerir ajuste del parámetro de regularización.","metadata":{}},{"cell_type":"code","source":"#Para buscar el mejor valor de alphas de esta lista usaré RidgeCV\nalphas_ = [ 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 100, 200, 300, 400, 500, 800, 1000, 5000, 10000]\n\n# Usar Robust Scaler ya que a diferencia de otros escaladores, el centrado y escalado de robust scaler\n#se basa en percentiles y, por tanto, no se ve influido por un número reducido de valores marginales atípicos muy grandes.\n\nridge = make_pipeline(StandardScaler(with_mean=True, with_std=False), linear_model.RidgeCV(alphas = alphas_, cv = 10))","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:03:19.275511Z","iopub.execute_input":"2023-05-10T08:03:19.275912Z","iopub.status.idle":"2023-05-10T08:03:19.282964Z","shell.execute_reply.started":"2023-05-10T08:03:19.275879Z","shell.execute_reply":"2023-05-10T08:03:19.281703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calcular su validación cruzada\ncross_validation(ridge)\n\n#fit\nmodel_ridge = ridge.fit(X_train, y_train)\n\n#Predecimos el valor de SalePrice en el df de entrenamiento\ny2_pred = ridge.predict(X_train)\n\n#Calculamos el error cuadrático medio\nrmse(y2_pred,y_train)\n\n#Predecimos el valor de SalePrice en el df de test\ny2_pred_v = ridge.predict(X_test)\n\n#Calculamos el error cuadrático medio\nrmse(y2_pred_v, y_test)\n\n# Calculamos coeficiente de determinación (R-cuadrado) \ny2_pred_v = ridge.predict(X_test)\nr2 = r2_score(y_test, y2_pred_v)\nprint(\"R-cuadrado: {:.2f}\".format(r2))\n\n#Ploteamos el resultado\nactual_vs_pred_plot(y_test,y2_pred_v)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:03:23.279692Z","iopub.execute_input":"2023-05-10T08:03:23.280081Z","iopub.status.idle":"2023-05-10T08:04:54.645140Z","shell.execute_reply.started":"2023-05-10T08:03:23.280051Z","shell.execute_reply":"2023-05-10T08:04:54.644068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conseguimos con la **regresión Ridge** un **RMSE** para el conjunto de entrenamiento de 0.095, que es muchísimo mejor que el modelo de regresión lineal, para el conjunto de test el **RMSE** es de 0.115. Además, el score **CV** promedio es de 0.111, lo que sugiere que el modelo de Ridge tiene una buena capacidad de generalización. Por último obtenemos un valor de **R-cuadrado** de 0.91 que indica que el modelo de regresión ajustado es capaz de explicar el 91% de la variación total en la variable de respuesta. Es decir, este modelo tiene una capacidad de ajuste muy buena y explica la mayoría de la variabilidad presente en la variable objetivo.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-five-subsection-four\"></a>\n### **5.4 Regresión Lasso**","metadata":{}},{"cell_type":"markdown","source":"La regresión Lasso es también una versión regularizada de la regresión lineal. La regresión Lasso realiza automáticamente la selección de características y puede estimar coeficientes dispersos.\n\nHemos utilizado el modelo LassoCV para aplicar la regresión Lasso, ya que incorpora una validación cruzada del parámetro alfa.\n\nMás información en el siguiente [enlace](https://sparkbyexamples.com/machine-learning/lasso-regression/)\n\nEstos son los principales hiperparámetros que se utilizan:\n\n- **Alpha**: Controla la fuerza de la regularización. Un valor más alto de alpha aumenta la regularización y reduce la complejidad del modelo. Buscaremos el mejor valor\n- **max_iter**: El número máximo de iteraciones permitidas para la convergencia del modelo. Utilizaremos 2000.\n- **cv**: número de folds o pliegues en la validación cruzada que se utilizará para evaluar el rendimiento del modelo Ridge con diferentes valores de alfa. Elegiremos 12 para nuestro modelo\n- **random_state**: Parámetro que establece una semilla para el generador de números aleatorios, utilizaremos 42\n\n**Pros**:\n- Puede reducir la varianza del modelo y evitar el sobreajuste.\n- Capaz de seleccionar variables importantes y eliminar variables irrelevantes.\n- Puede ser utilizado como punto de partida para modelos más complejos.\n\n**Contras**:\n\n- No es adecuado para la selección de variables altamente correlacionadas.\n- No es muy interpretable.\n- Puede requerir ajuste del parámetro de regularización.","metadata":{}},{"cell_type":"code","source":"# Para buscar el mejor valor de alphas de esta lista usaré LassoCV\nalpha2 = [0.0001, 0.0002, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.001, 0.002, 0.004, 0.01, 0.1, 1]\n\n# Utilizar un Robust Scaler para que las predicciones no se vean influidas por un número reducido de valores marginales atípicos muy grandes\nlasso = make_pipeline(RobustScaler(), linear_model.LassoCV(alphas = alpha2, random_state=42, cv=12, max_iter=2000))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:08:23.767481Z","iopub.execute_input":"2023-05-10T08:08:23.767878Z","iopub.status.idle":"2023-05-10T08:08:23.774681Z","shell.execute_reply.started":"2023-05-10T08:08:23.767847Z","shell.execute_reply":"2023-05-10T08:08:23.773408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculamos su Validación Cruzada\ncross_validation(lasso)\n\n#Ajustamos el modelo\nmodel_lasso = lasso.fit(X_train, y_train)\n\n#Predecimos el valor de SalePrice en el df de train\ny3_pred = lasso.predict(X_train)\n\n#Calculamos el error cuadrático medio\nrmse(y3_pred,y_train)\n\n#Predecimos el valor de SalePrice en el df de test\ny3_pred_v = lasso.predict(X_test)\n\n#Calculamos el error cuadrático medio\nrmse(y3_pred_v, y_test)\n\n# Calculamos coeficiente de determinación (R-cuadrado) \ny3_pred_v = lasso.predict(X_test)\nr2 = r2_score(y_test, y3_pred_v)\nprint(\"R-cuadrado: {:.2f}\".format(r2))\n\n#Ploteamos el resultado\nactual_vs_pred_plot(y_test,y3_pred_v)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:08:28.514856Z","iopub.execute_input":"2023-05-10T08:08:28.515272Z","iopub.status.idle":"2023-05-10T08:08:58.035349Z","shell.execute_reply.started":"2023-05-10T08:08:28.515241Z","shell.execute_reply":"2023-05-10T08:08:58.034151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conseguimos con la **regresión Lasso** un **RMSE** para el conjunto de entrenamiento de 0.099, que es similar al modelo de regresión Ridge, para el conjunto de test el **RMSE** es de 0.116. Además, el score **CV** promedio es de 0.110, lo que sugiere que el modelo de Lasso tiene una buena capacidad de generalización. Por último obtenemos un valor de R-cuadrado de 0.91 que indica que el modelo de regresión ajustado es capaz de explicar el 91% de la variación total en la variable de respuesta. Es decir, este modelo tiene una capacidad de ajuste muy buena y explica la mayoría de la variabilidad presente en la variable objetivo.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-five-subsection-five\"></a>\n### **5.5 Modelo de los K vecinos más cercanos**","metadata":{}},{"cell_type":"markdown","source":"El modelo de los K vecinos más cercanos muy utilizado para tareas de regresión. Se trata de un modelo sencillo de aprendizaje automático supervisado. Además, KNN es un método no paramétrico, lo que significa que no se asume una distribución particular para los datos, lo que lo hace más flexible en comparación con modelos paramétricos como la regresión lineal. Sin embargo, tenemos que tener en cuenta que KNN no funciona bien cuando hay muchas características.\n\nMás información en el siguiente [enlace](https://www.ibm.com/topics/knn)\n\nUtilizamos los siguientes parámetros:\n\n   - **n_neighbors**: En nuestro caso hemos probado con diferentes vecinos para probar el rendimiento del modelo y quedarnos con el mejor. El número de vecinos se fijó en tres valores diferentes y se observó el rendimiento de este modelo. Finalmente nos quedamos con 5.\n   - **weights**: Función de pesos para la predicción, en este caso se usa 'uniforme', es decir, todos los vecinos tienen el mismo peso en la predicción.\n   - **algorithm**: Algoritmo usado para calcular los vecinos más cercanos, en este caso se usa 'auto', que determina automáticamente el algoritmo a usar en función de los datos.\n   - **leaf_size**: Tamaño de los nodos hoja en el árbol de búsqueda de vecinos. El tamaño de hoja se fijó en 25.\n   \n**Pros**:\n- Fácil de entender e implementar.\n- Capaz de manejar datos no lineales.\n- No hace suposiciones sobre la distribución de los datos.\n\n**Contras**:\n\n- Sensible a los valores atípicos y la presencia de ruido en los datos.\n- Computacionalmente costoso para grandes conjuntos de datos.\n- No es muy interpretable.","metadata":{}},{"cell_type":"code","source":"# N = 5\nneigh = KNeighborsRegressor(n_neighbors = 5,\n                            weights = 'uniform',\n                            algorithm = 'auto',\n                            leaf_size=25)\nneigh.fit(X_train,y_train)\n\n#Predecimos el valor de SalePrice en el df de train\ny4_pred = neigh.predict(X_train)\n\n#Calculamos el error cuadrático medio\nrmse(y4_pred,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:09:53.640098Z","iopub.execute_input":"2023-05-10T08:09:53.640487Z","iopub.status.idle":"2023-05-10T08:09:53.746053Z","shell.execute_reply.started":"2023-05-10T08:09:53.640460Z","shell.execute_reply":"2023-05-10T08:09:53.745189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# N = 7\nneigh1 = KNeighborsRegressor(n_neighbors = 7,\n                             weights = 'uniform',\n                             leaf_size=25)\nneigh1.fit(X_train,y_train)\n\n#Predecimos el valor de SalePrice en el df de train\ny_pred = neigh1.predict(X_train)\n\n#Calculamos el error cuadrático medio\nrmse(y_pred,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:09:54.205097Z","iopub.execute_input":"2023-05-10T08:09:54.206065Z","iopub.status.idle":"2023-05-10T08:09:54.260646Z","shell.execute_reply.started":"2023-05-10T08:09:54.206027Z","shell.execute_reply":"2023-05-10T08:09:54.259670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# N = 9 #\nneigh2 = KNeighborsRegressor(n_neighbors = 9,\n                             weights = 'uniform',\n                             leaf_size=25)\nneigh2.fit(X_train,y_train)\n\n#Predecimos el valor de SalePrice en el df de train\ny_pred = neigh2.predict(X_train)\n\n#Calculamos el error cuadrático medio\nrmse(y_pred,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:09:54.635442Z","iopub.execute_input":"2023-05-10T08:09:54.635833Z","iopub.status.idle":"2023-05-10T08:09:54.690786Z","shell.execute_reply.started":"2023-05-10T08:09:54.635803Z","shell.execute_reply":"2023-05-10T08:09:54.689907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos que **N5** es el que mejor rendimiento nos aporta de los 3, luego escogeremos ese para generar nuestro modelo","metadata":{}},{"cell_type":"code","source":"# Calculamos la validación cruzada\ncross_validation(neigh)\n\n#Predecimos el valor de SalePrice en el df de test\ny4_pred_v = neigh.predict(X_test)\n\n#Calculamos el error cuadrático medio\nrmse(y4_pred_v, y_test)\n\n# Calculamos coeficiente de determinación (R-cuadrado) \ny4_pred_v = neigh.predict(X_test)\nr2 = r2_score(y_test, y4_pred_v)\nprint(\"R-cuadrado: {:.2f}\".format(r2))\n\n#Ploteamos el resultado\nactual_vs_pred_plot(y_test,y4_pred_v)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:09:57.780660Z","iopub.execute_input":"2023-05-10T08:09:57.782438Z","iopub.status.idle":"2023-05-10T08:09:58.346197Z","shell.execute_reply.started":"2023-05-10T08:09:57.782399Z","shell.execute_reply":"2023-05-10T08:09:58.345102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conseguimos con los **K vecinos más cercanos** un **RMSE** para el conjunto de entrenamiento de 0.1264, una cantidad superior que los anteriores, para el conjunto de test el **RMSE** es de 0.1689. Además, el score **CV** promedio es de 0.155, lo que sugiere que el modelo de K vecinos más cercanos tiene una aceptable capacidad de generalización. Por último obtenemos un valor de **R-cuadrado** de 0.82 que indica que el modelo de regresión ajustado es capaz de explicar el 82% de la variación total en la variable de respuesta. Es decir, este modelo tiene una capacidad de ajuste buena y explica una gran parte de la variabilidad presente en la variable objetivo, sin embargo es sensiblemente peor que los anteriores.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-five-subsection-six\"></a>\n### **5.6 Árboles de decisión**","metadata":{}},{"cell_type":"markdown","source":"El modelo de árbol de decisión también se utiliza para ajustar estos datos, ya que no requiere mucha limpieza de datos y no se ve influido por los valores atípicos. \n\nLos árboles de decisión pueden, a diferencia de los modelos lineales, ajustarse a conjuntos de datos linealmente inseparables. \n\nMás información en el siguiente [enlace](https://scikit-learn.org/stable/modules/tree.html)\n\nNuestro modelo tiene los siguientes hiperparámetros:\n  \n  - **max_depth**: es la profundidad máxima que puede tener el árbol de decisión. En nuestro caso probaremos con 5 y 7 niveles.\n  - **min_samples_leaf**: es el número mínimo de muestras que debe tener cada hoja del árbol. Si el número de muestras es menor al valor de este parámetro, el algoritmo de construcción del árbol dejará de hacer divisiones en esa rama. En nuestro caso probaremos con 5 y 9 muestras por hoja.\n  - **random_state**: es la semilla que se utiliza para la generación de números aleatorios. En este caso se está fijando en 42 para poder reproducir los mismos resultados en cada ejecución del modelo.\n  \n\n**Pros**:\n- Fácil de entender e interpretar.\n- Capaz de manejar datos no lineales y no paramétricos.\n- No es sensible a los valores atípicos.\n\n**Contras**:\n- Propenso al sobreajuste si se construye un árbol profundo.\n- No es muy estable, una pequeña variación en los datos de entrada puede generar un árbol diferente.\n- No es adecuado para datos con relaciones complejas entre las variables.","metadata":{}},{"cell_type":"code","source":"# Aplicamos max_depth=7 y min_samples_leaf=5\ntree_regr1 = tree.DecisionTreeRegressor(max_depth = 7, min_samples_leaf=5,random_state=42)\n\n# Aplicamos max_depth=9 y min_samples_leaf=9\ntree_regr2 = tree.DecisionTreeRegressor(max_depth = 9,min_samples_leaf=9,random_state=42)\n\n# Ajustamos los datos de training a un modelo de árbol de decisión.\ntree_regr11 = tree_regr1.fit(X_train,y_train)\ntree_regr12 = tree_regr2.fit(X_train,y_train)\n\n#Predecimos el valor de SalePrice en el df de train\ny1 = tree_regr1.predict(X_train)\ny2 = tree_regr2.predict(X_train)\n\ncross_validation(tree_regr1)\n\n#Calculamos el error cuadrático medio\nrmse(y1,y_train)\n\ncross_validation(tree_regr2)\n\n#Calculamos el error cuadrático medio\nrmse(y2,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:11:48.526350Z","iopub.execute_input":"2023-05-10T08:11:48.527004Z","iopub.status.idle":"2023-05-10T08:11:49.529252Z","shell.execute_reply.started":"2023-05-10T08:11:48.526956Z","shell.execute_reply":"2023-05-10T08:11:49.528098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nos quedamos con **tree_regr1** ya que tiene un mejor **rmse**","metadata":{}},{"cell_type":"code","source":"#Predecimos el valor de SalePrice en el df de test\ny5_pred_v = tree_regr1.predict(X_test)\n\n#Calculamos el error cuadrático medio\nrmse(y5_pred_v, y_test)\n\n# Calculamos coeficiente de determinación (R-cuadrado) \ny5_pred_v = tree_regr1.predict(X_test)\nr2 = r2_score(y_test, y5_pred_v)\nprint(\"R-cuadrado: {:.2f}\".format(r2))\n\n#Ploteamos el resultado\nactual_vs_pred_plot(y_test,y5_pred_v)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:12:01.937630Z","iopub.execute_input":"2023-05-10T08:12:01.938082Z","iopub.status.idle":"2023-05-10T08:12:02.230698Z","shell.execute_reply.started":"2023-05-10T08:12:01.938046Z","shell.execute_reply":"2023-05-10T08:12:02.229443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conseguimos con los **Árboles de decisión** un **RMSE** para el conjunto de entrenamiento de 0.106, una cantidad sensiblemente mejor a los anteriores, para el conjunto de test el **RMSE** es de 0.179, peor que los anteriores. El score **CV** promedio es de 0.170, lo que sugiere que el modelo de Árboles de decisión tiene una aceptable capacidad de generalización. Por último obtenemos un valor de **R-cuadrado** de 0.80 que indica que el modelo de regresión ajustado es capaz de explicar el 80% de la variación total en la variable de respuesta. Es decir, este modelo aun teniendo una capacidad de ajuste buena y explicar una gran parte de la variabilidad presente en la variable objetivo, es peor que los anteriores y no lo utilizaremos.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-five-subsection-seven\"></a>\n### **5.7 Modelo Random Forest**","metadata":{}},{"cell_type":"markdown","source":"El modelo de Random Forests es un conjunto de árboles de decisión que trabajan en paralelo para hacer una predicción. Cada árbol en el modelo se ajusta a una muestra aleatoria del conjunto de datos de entrenamiento y utiliza una selección aleatoria de características para construir una estructura de árbol.\nHemos realizado una búsqueda de cual es la mejor configuración para conseguir el modelo con mejor rendimiento, y conseguimos los siguientes hiperparámetros:\n    \n  - **n_estimators**: Número de árboles en el random forestx, en nuestro caso elegimos 400.\n  - **max_depth**: La profundidad máxima de cada árbol de decisión en el bosque aleatorio. En este caso, se establece en 17.\n  - **min_samples_leaf**: es el número mínimo de muestras que debe tener cada hoja del árbol. En nuestro caso elegimos 3.\n  - **random_state**: es la semilla que se utiliza para la generación de números aleatorios. En este caso se está fijando en 42 para poder reproducir los mismos resultados en cada ejecución del modelo.\n  \nMás información en el siguiente [enlace](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)\n\n**Pros**:\n- Reducción de la varianza y el sobreajuste en comparación con un solo árbol.\n- Capaz de manejar datos no lineales y no paramétricos.\n- No es sensible a los valores atípicos.\n\n**Contras**:\n- Menos interpretable que un solo árbol.\n- Computacionalmente costoso para grandes conjuntos de datos.\n- No es adecuado para datos con relaciones complejas entre las variables.","metadata":{}},{"cell_type":"code","source":"# Generamos un modelo\nrforest = RandomForestRegressor(n_estimators=200,max_depth=13,random_state=42)\n\n# Búsqueda en cuadrícula para encontrar el mejor valor de C, gamma y épsilon\nparam_grid  = {'n_estimators': [100,200,300,400],\n               'max_depth': [5,9,13,17], \n               'min_samples_leaf': [3,7,11,15]}\n\n# Establecemos la validación cruzada en 5 y el verbose en 1\nclf = GridSearchCV(rforest, param_grid, cv = 5, n_jobs = -2, verbose=1)\n\n# Ajustamos el modelo\nclf.fit(X_train,y_train)\n\n# Mostramos los mejores parámetros\nclf.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:12:46.357058Z","iopub.execute_input":"2023-05-10T08:12:46.357472Z","iopub.status.idle":"2023-05-10T08:20:10.066642Z","shell.execute_reply.started":"2023-05-10T08:12:46.357439Z","shell.execute_reply":"2023-05-10T08:20:10.065258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Estos son los mejores parámetros que hemos conseguido para tener el mejor modelo:\n\n- 'max_depth': 13\n- 'min_samples_leaf': 3\n- 'n_estimators': 400","metadata":{}},{"cell_type":"code","source":"# Generamos el modelo con estos parámetros\nrforest = RandomForestRegressor(n_estimators=400, max_depth=13, min_samples_leaf=3, random_state=42)\n\n# Calculamos la validación cruzada\ncross_validation(rforest)\n\n# Ajustamos el modelo\nmodel_rforest = rforest.fit(X_train, y_train)\n\n# Predecimos el valor de SalePrice en el df de train\ny6_pred = rforest.predict(X_train)\n\n#Calculamos el error cuadrático medio\nrmse(y6_pred,y_train)\n\n# Predecimos el valor de SalePrice en el df de test\ny6_pred_v = rforest.predict(X_test)\n\n#Calculamos el error cuadrático medio\nrmse(y6_pred_v, y_test)\n\n# Calculamos coeficiente de determinación (R-cuadrado) \ny6_pred_v = rforest.predict(X_test)\nr2 = r2_score(y_test, y6_pred_v)\nprint(\"R-cuadrado: {:.2f}\".format(r2))\n\n#Ploteamos el resultado\nactual_vs_pred_plot(y_test,y6_pred_v)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:25:06.140405Z","iopub.execute_input":"2023-05-10T08:25:06.140795Z","iopub.status.idle":"2023-05-10T08:26:52.058250Z","shell.execute_reply.started":"2023-05-10T08:25:06.140766Z","shell.execute_reply":"2023-05-10T08:26:52.057149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conseguimos con los **RandomForest** un **RMSE** para el conjunto de entrenamiento de 0.067, una cantidad sensiblemente mejor a los anteriores, para el conjunto de test el **RMSE** es de 0.141, ligeramente peor que los anteriores. El score CV promedio es de 0.130. Por último obtenemos un valor de **R-cuadrado** de 0.87 que indica que el modelo de regresión ajustado es capaz de explicar el 87% de la variación total en la variable de respuesta. Es decir, este modelo tiene una capacidad de ajuste buena y explica una gran parte de la variabilidad presente en la variable objetivo.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-five-subsection-eight\"></a>\n### **5.8 Modelo Regresor de vectores de apoyo (SVR)**","metadata":{}},{"cell_type":"markdown","source":"El modelo Regresor de Vectores de Apoyo (SVR) es una técnica de aprendizaje supervisado utilizada para la regresión. Utiliza una función kernel para transformar los datos de entrada en un espacio de mayor dimensión y luego encuentra un hiperplano óptimo para separar los datos de salida. El objetivo es encontrar la función que mejor se ajuste a los datos con el menor error posible.\n\nUtilizaremos estos hiperparámetros:\n  \n  - **C**: parámetro de regularización que controla el equilibrio entre la precisión del modelo y su capacidad de generalización. Usaremos 7.\n\n  - **Gamma**: coeficiente de ajuste del kernel RBF. Este hiperparámetro controla la forma de la función de decisión y afecta el grado de ajuste a los datos de entrenamiento. Usaremos 0.001\n\n  - **Epsilon**: el margen de error permitido para la función de regresión. Se utiliza para definir la región de insensibilidad alrededor de la función de regresión. Usaremos 0.01\n\nMás información en el siguiente [enlace](https://www.jacobsoft.com.mx/es_mx/support-vector-regression/)\n\n**Pros**:\n- Capaz de manejar datos no lineales y no paramétricos.\n- Capaz de manejar datos con alta dimensionalidad.\n- Reducción de la sensibilidad a los valores atípicos.\n\n**Contras**:\n- Requiere preprocesamiento de los datos.\n- Puede ser computacionalmente costoso.\n- Requiere ajuste de hiperparámetros para un rendimiento óptimo.","metadata":{}},{"cell_type":"code","source":"#Generamos un modelo básico\nsvr_basic = SVR(C = 10, gamma = 0.001)\n\n# Búsqueda en cuadrícula para encontrar el mejor valor de C, gamma y epsilon y el kernel por defecto 'rbf'\nparam_grid  = {'C': [5,7,10,15,20,30],'gamma': [0.001, 0.0001, 0.0011, 0.00011], 'epsilon': [0.1, 0.01, 0.001, 0.005, 0.007, 0.008, 0.009] }\n\n# Establecemos la validación cruzada en 5\nclf = GridSearchCV(svr_basic, param_grid, cv = 10, n_jobs = -2)\nclf.fit(X_train,y_train)\n\n# Mostramos los mejores parámetros\nclf.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:28:03.823573Z","iopub.execute_input":"2023-05-10T08:28:03.824012Z","iopub.status.idle":"2023-05-10T08:32:11.047092Z","shell.execute_reply.started":"2023-05-10T08:28:03.823978Z","shell.execute_reply":"2023-05-10T08:32:11.045627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hacemos el modelo final SVR con los mejores parámetros buscados en la cuadrícula.\nsvr = make_pipeline(MinMaxScaler(), SVR(C= 7, epsilon= 0.1, gamma=0.001, kernel = \"sigmoid\"))\n\n# Calculamos la validación cruzada\ncross_validation(svr)\n\n#Ajustamos\nmodel_svr = svr.fit(X_train, y_train)\n\n# Predecimos el valor de SalePrice en el df de train\ny7_pred = svr.predict(X_train)\n\n#Calculamos el error cuadrático medio\nrmse(y7_pred,y_train)\n\n# Predecimos el valor de SalePrice en el df de test\ny7_pred_v = svr.predict(X_test)\n\n#Calculamos el error cuadrático medio\nrmse(y7_pred_v, y_test)\n\n# Calculamos coeficiente de determinación (R-cuadrado) \ny7_pred_v = rforest.predict(X_test)\nr2 = r2_score(y_test, y7_pred_v)\nprint(\"R-cuadrado: {:.2f}\".format(r2))\n\n#Ploteamos el resultado\nactual_vs_pred_plot(y_test,y7_pred_v)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:32:57.403950Z","iopub.execute_input":"2023-05-10T08:32:57.404388Z","iopub.status.idle":"2023-05-10T08:32:59.265105Z","shell.execute_reply.started":"2023-05-10T08:32:57.404352Z","shell.execute_reply":"2023-05-10T08:32:59.264030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conseguimos con los **SVR** un **RMSE** para el conjunto de entrenamiento de 0.118, una cantidad bastante peor que los anteriores, para el conjunto de test el **RMSE** es de 0.139, ligeramente peor que los anteriores. El score **CV** promedio es de 0.128. Por último obtenemos un valor de **R-cuadrado** de 0.87 que indica que el modelo de regresión ajustado es capaz de explicar el 87% de la variación total en la variable de respuesta. Es decir, este modelo tiene una capacidad de ajuste buena y explica una gran parte de la variabilidad presente en la variable objetivo, sin embargo tampoco es de los mejores.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-five-subsection-nine\"></a>\n### **5.9 Modelo Gradient Boosting Regressor**","metadata":{}},{"cell_type":"markdown","source":"Gradient Boosting Regressor es un modelo de regresión que utiliza múltiples árboles de decisión para predecir una variable continua. Este modelo se basa en la idea de que, en lugar de ajustar un solo árbol de decisión complejo para ajustar los datos, se pueden ajustar muchos árboles más simples y combinados para producir una predicción más precisa\n\nHe probado a ajustar con diferentes valores algunos parámetros para ver cual era el modelo que tenía un mejor rendimiento.\n\n- **n_estimators**: número de árboles de decisión a utilizar en el modelo. Escogemos 200\n- **max_depth**: profundidad máxima de cada árbol de decisión en el modelo. Utilizamos 9\n- **learning_rate**: tasa de aprendizaje utilizada para actualizar los pesos en cada iteración. Utilizamos 0.05\n- **loss**: función de pérdida utilizada para evaluar el rendimiento del modelo. Elegimos 'huber'\n- **min_samples_leaf**: número mínimo de muestras requeridas para estar en una hoja en cada árbol de decisión. Usamos 10\n\nMás información en el siguiente [enlace](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html)\n\n**Pros**:\n- Tiene una alta precisión en la predicción de datos.\n- Puede manejar diferentes tipos de variables y no requiere que se normalicen los datos.\n- Puede manejar datos faltantes.\n\n**Contras**:\n- Es propenso a sobreajuste, especialmente si se utilizan parámetros incorrectos.\n- Requiere un tiempo de entrenamiento más largo en comparación con otros modelos.\n- Es más difícil de interpretar que otros modelos, ya que es un conjunto de árboles de decisión.","metadata":{}},{"cell_type":"code","source":"# Probamos con max_depth=7 y con min_samples_leaf=7\ngbr1 = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth = 7,\n                                min_samples_leaf=7, loss='huber', random_state =42)\n\n# Probamos con max_depth=9 y con min_samples_leaf=10\ngbr2 = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth = 9,\n                                min_samples_leaf=10, loss='huber', random_state =42) \n\n# Calculamos las validaciones cruzadas\ncross_validation(gbr1)\ncross_validation(gbr2)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:33:40.186189Z","iopub.execute_input":"2023-05-10T08:33:40.186565Z","iopub.status.idle":"2023-05-10T08:36:30.872574Z","shell.execute_reply.started":"2023-05-10T08:33:40.186537Z","shell.execute_reply":"2023-05-10T08:36:30.871326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Volvemos a probar ampliando el n_estimators y reduciendo el learning_rate\ngbr3 = GradientBoostingRegressor(n_estimators=500, learning_rate=0.03, max_depth=9,\n                                 min_samples_leaf=10, loss='huber', random_state=42)\n\n# Calculamos la validacin cruzada\ncross_validation(gbr3)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:36:30.874940Z","iopub.execute_input":"2023-05-10T08:36:30.876028Z","iopub.status.idle":"2023-05-10T08:40:10.982090Z","shell.execute_reply.started":"2023-05-10T08:36:30.875986Z","shell.execute_reply":"2023-05-10T08:40:10.980889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ajustamos\nmodel_gbr1 = gbr1.fit(X_train, y_train)\nmodel_gbr2 = gbr2.fit(X_train, y_train)\nmodel_gbr3 = gbr3.fit(X_train, y_train)\n\n# Predecimos el valor de SalePrice en el df de train\ny_g1_pred = gbr1.predict(X_train)\ny_g2_pred = gbr2.predict(X_train)\ny_g3_pred = gbr3.predict(X_train)\n\n\n# Calculamos el error cuadrático medio\nrmse(y_g1_pred,y_train)\nrmse(y_g2_pred,y_train)\nrmse(y_g3_pred,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:40:10.983688Z","iopub.execute_input":"2023-05-10T08:40:10.984007Z","iopub.status.idle":"2023-05-10T08:40:45.863477Z","shell.execute_reply.started":"2023-05-10T08:40:10.983979Z","shell.execute_reply":"2023-05-10T08:40:45.862289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nos quedamos con el modelo **gbr1** ya que es el que tiene tanto un mejor CV como un mejor rmse","metadata":{}},{"cell_type":"code","source":"# Predecimos el valor de SalePrice en el df de test\ny8_pred_v = gbr1.predict(X_test)\n\n# Calculamos el error cuadrático medio\nrmse(y8_pred_v, y_test)\n\n# Calculamos coeficiente de determinación (R-cuadrado) \nr2 = r2_score(y_test, y8_pred_v)\nprint(\"R-cuadrado: {:.2f}\".format(r2))\n\n# #Ploteamos el mejor resultado (gbr2)\nactual_vs_pred_plot(y_test, y8_pred_v)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:42:10.152913Z","iopub.execute_input":"2023-05-10T08:42:10.153442Z","iopub.status.idle":"2023-05-10T08:42:10.497141Z","shell.execute_reply.started":"2023-05-10T08:42:10.153396Z","shell.execute_reply":"2023-05-10T08:42:10.496080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conseguimos con el **Modelo Gradient Boosting Regressor** un **RMSE** para el conjunto de entrenamiento de 0.044, una cantidad bastante peor que los anteriores, para el conjunto de test el **RMSE** es de 0.134, ligeramente peor que los anteriores. Esta diferencia tan grande entre train y test seguramente se deba a que se esté sobreajustando los datos de entrenamiento. El score **CV** promedio es de 0.124. Por último obtenemos un valor de **R-cuadrado** de 0.88 que indica que el modelo de es capaz de explicar el 88% de la variación total en la variable de respuesta. Es decir, este modelo tiene una capacidad de ajuste buena y explica una gran parte de la variabilidad presente en la variable objetivo, sin embargo otra vez no es de los mejores.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-five-subsection-ten\"></a>\n### **5.10 Modelo Stacked Regressor**","metadata":{}},{"cell_type":"markdown","source":"El modelo Stacked Regressor es un modelo de ensamblaje que combina las predicciones de varios modelos de regresión en una sola predicción final. Se ajusta a los datos utilizando validación cruzada y puede incluir cualquier número de modelos de regresión como entradas. En nuestro caso hemos incluido los 4 modelos con mejor rmse de todos los que habíamos conseguido (Lasso, Ridge, Random Forest y Gradient Boosting Regressor)\n\nMás información en el siguiente [enlace](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html)\n\n\n**Pros**:\n- Puede combinar las fortalezas de varios modelos en uno solo, lo que puede aumentar la precisión.\n- Puede ser utilizado con diferentes tipos de modelos.\n- Es más flexible que otros modelos y se puede adaptar fácilmente a diferentes tipos de datos.\n        \n**Contras**:\n- Requiere un conjunto de datos de entrenamiento grande y diverso para obtener resultados precisos.\n- Puede ser propenso a sobreajuste, especialmente si se utilizan demasiados modelos.\n- Es más difícil de interpretar que otros modelos, ya que combina varios modelos en uno solo.","metadata":{}},{"cell_type":"code","source":"# Usamos Random Forest, Support Vector Regressor y Gradient Boosting para construir un modelo de Stacked Regresor porque tienen los RMSE más bajos. \nestimators = [('Random Forest', rforest),\n              (\"Gradient Boosting Regressor\",gbr1),\n              (\"Lasso\",lasso),\n              (\"Ridge\",ridge)\n              ]\n\n# Creamos el modelo\nstacked = StackingRegressor(estimators = estimators, final_estimator = rforest, cv=5)\n\n# Calculamos la validación cruzada\n# He comentado esta línea ya que retrasa mucho el código. Pero es recomendable descomentarla\n#cross_validation(stacked)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:44:26.582660Z","iopub.execute_input":"2023-05-10T08:44:26.583090Z","iopub.status.idle":"2023-05-10T08:44:26.589938Z","shell.execute_reply.started":"2023-05-10T08:44:26.583054Z","shell.execute_reply":"2023-05-10T08:44:26.588811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Realizamos las primeras pruebas para elegir el que mejor nos funcione. Empezaremos con el elegido en primer lugar, los modelos con mejor rmse.","metadata":{}},{"cell_type":"code","source":"#PRUEBA 1\n\n#Ajustamos\nmodel_stack = stacked.fit(X_train, y_train)\n\n#Predecimos el valor de SalePrice en el df de train\ny9_pred = stacked.predict(X_train)\n\n#Calculamos el error cuadrático medio\nrmse(y9_pred,y_train)\n\n#Predecimos el valor de SalePrice en el df de test\ny9_pred_v = stacked.predict(X_test)\n\n#Calculamos el error cuadrático medio\nrmse(y9_pred_v, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:44:31.111553Z","iopub.execute_input":"2023-05-10T08:44:31.111949Z","iopub.status.idle":"2023-05-10T08:46:46.412205Z","shell.execute_reply.started":"2023-05-10T08:44:31.111918Z","shell.execute_reply":"2023-05-10T08:46:46.410690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obtenemos un **rmse** en el conjunto de test aceptable, pero inferior al conseguido en Ridge y Lasso, entonces vamos a seguir probando para intentar mejorarlos.\n\nLe vamos a agregar un modelo de **XGBoost**, al agregarlo a un modelo de stacking, se puede aprovechar la potencia y flexibilidad de XGBoost para mejorar el rendimiento general del modelo de stacking. Además, XGBoost puede capturar patrones y relaciones en los datos que otros modelos de stacking no pueden, lo que puede resultar en una mayor precisión de las predicciones.","metadata":{}},{"cell_type":"code","source":"#PRUEBA XGB\n# Crear modelo de nivel superior\nxgb_model = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 100)\n\n# Crear modelo de stacking\nestimators = [('Random Forest', rforest),\n              (\"Gradient Boosting Regressor\",gbr2),\n              (\"Lasso\",lasso),\n              (\"Ridge\",ridge),\n              (\"XGBoost\", xgb_model)\n              ]\n\nstacked1 = StackingRegressor(estimators=estimators, final_estimator=reg)\n\n#Ajustamos\nmodel_stack = stacked1.fit(X_train, y_train)\n\n#Predecimos el valor de SalePrice en el df de trai\ny9_pred = stacked1.predict(X_train)\n\n#Calculamos el error cuadrático medio de train\nrmse(y9_pred,y_train)\n\n#Predecimos el valor de SalePrice en el df de test\ny9_pred_v = stacked1.predict(X_test)\n\n#Calculamos el error cuadrático medio de test\nrmse(y9_pred_v, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:51:13.854671Z","iopub.execute_input":"2023-05-10T08:51:13.856823Z","iopub.status.idle":"2023-05-10T08:53:34.525655Z","shell.execute_reply.started":"2023-05-10T08:51:13.856787Z","shell.execute_reply":"2023-05-10T08:53:34.524693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observamos que añadiendo el modelo de XGB mejoramos ya los datos de Ridge y Lasso. Vamos a seguir probando. Ahora cambiando los modelos que usaremos en estimators y añadiendo K Vecinos en vez de Lasso.","metadata":{}},{"cell_type":"code","source":"#PRUEBA XGB 2\n# Crear modelo de nivel superior\nxgb_model = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 100)\n\n# Crear modelo de stacking\nestimators = [('Random Forest', rforest),\n              (\"Gradient Boosting Regressor\",gbr2),\n              (\"K\",neigh),\n              (\"Ridge\",ridge),\n              (\"XGBoost\", xgb_model)\n              ]\n\nstacked2 = StackingRegressor(estimators=estimators, final_estimator=gbr2)\n\n#Ajustamos\nmodel_stack = stacked2.fit(X_train, y_train)\n\n#predict value of sale price on the training set\ny9_pred = stacked2.predict(X_train)\n\n#caculate root mean square error\nrmse(y9_pred,y_train)\n\n#predict value of sale price on the validation set\ny9_pred_v = stacked2.predict(X_test)\n\n#caculate root mean square error\nrmse(y9_pred_v, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:55:51.612860Z","iopub.execute_input":"2023-05-10T08:55:51.613968Z","iopub.status.idle":"2023-05-10T08:57:59.507768Z","shell.execute_reply.started":"2023-05-10T08:55:51.613932Z","shell.execute_reply":"2023-05-10T08:57:59.506614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Empeoramos el resultado, vamos a realizar una última prueba poniendo como final_estimator una regresión Lineal","metadata":{}},{"cell_type":"code","source":"#PRUEBA XGB 3\n\n# Crear modelo de nivel superior\nxgb_model = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 100)\n\n# Crear modelo de stacking\nestimators = [('Random Forest', rforest),\n              (\"Gradient Boosting Regressor\",gbr2),\n              (\"K\",neigh),\n              (\"Ridge\",ridge),\n              (\"XGBoost\", xgb_model)\n              ]\n\nstacked3 = StackingRegressor(estimators=estimators, final_estimator=reg)\n\n#Ajustamos\nmodel_stack = stacked3.fit(X_train, y_train)\n\n#predict value of sale price on the training set\ny9_pred = stacked3.predict(X_train)\n\n#caculate root mean square error\nrmse(y9_pred,y_train)\n\n#predict value of sale price on the validation set\ny9_pred_v = stacked3.predict(X_test)\n\n#caculate root mean square error\nrmse(y9_pred_v, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:49:08.275667Z","iopub.execute_input":"2023-05-10T08:49:08.275988Z","iopub.status.idle":"2023-05-10T08:51:13.849937Z","shell.execute_reply.started":"2023-05-10T08:49:08.275960Z","shell.execute_reply":"2023-05-10T08:51:13.849051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hemos conseguido con esta nuestro mejor resultado de **rmse** (0,114) para el conjunto de test, luego este será el modelo que utilizaremos. Ahora vamos a calcular su CV","metadata":{}},{"cell_type":"code","source":"# Calculamos su validación cruzada\ncross_validation(stacked3)\n\n# Calculamos coeficiente de determinación (R-cuadrado) \nr2 = r2_score(y_test, y9_pred_v)\nprint(\"R-cuadrado: {:.2f}\".format(r2))\n\n# Ploteamos el mejor resultado\nactual_vs_pred_plot(y_test, y9_pred_v)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T08:58:50.415505Z","iopub.execute_input":"2023-05-10T08:58:50.415894Z","iopub.status.idle":"2023-05-10T09:22:34.342153Z","shell.execute_reply.started":"2023-05-10T08:58:50.415865Z","shell.execute_reply":"2023-05-10T09:22:34.340917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Después de nuestras pruebas hemos conseguido un modelo de **Stacked Regressor** bastante bueno. Después de generar tres modelos y elegir el modelo de que nos combinaba Random Forest, Gradient Boosting Regressor,K-vecinos, Ridge y XGBoost que era el que tenía una mejor puntuación observamos que el valor de **R-cuadrado** explica el 91% de la variabilidad en los datos. Con el valor de **RMSE** observamos que el modelo tiene un error promedio de predicción de 0.1147 unidades en el conjunto de test y 0.0786 en el de entrenamiento, lo que es bajo, y la precisión promedio en la validación cruzada **CV** es de 0.1099, lo que indica que el modelo es capaz de generalizar bien a datos nuevos y no vistos. En resumen, el modelo es muy preciso y creo que es el que mejor funciona.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-six\"></a>\n## **6. Predicción**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-six-subsection-one\"></a>\n### **6.1 Valores**","metadata":{}},{"cell_type":"markdown","source":"#### **RMSE - Error cuadrático medio**","metadata":{}},{"cell_type":"markdown","source":"Ordenados de mejor a peor puntuación\n\n- Stacked Regressor: 0.1147\n- Ridge: 0.1156\n- Lasso: 0.1161\n- Gradient Boosting Regressor: 0.1350\n- Random Forest: 0.1414\n- SVR : 0.1398\n- K-vecinos: 0.1689\n- Árboles de decisión: 0.1791\n- Regresión lineal: 4911624666.0435\n\n\nSegún el error de raíz cuadrado medio (rmse), el modelo que mejor ajusta los datos es el Stacked Regressor con un valor de 0.1147, seguido de cerca por Lasso y Ridge con valores de 0.1161 y 0.1156 respectivamente. El peor modelo es la regresión lineal con un valor de 4911624666.0435. El coeficiente de determinación R-cuadrado también indica que el Stacked Regressor es el mejor modelo, seguido por Ridge y Lasso, y que los peores modelos son Árboles de decisión y K-vecinos.","metadata":{}},{"cell_type":"markdown","source":"#### **CV - Validación cruzada**","metadata":{}},{"cell_type":"markdown","source":"Ordenados de mejor a peor puntuación:\n\n- Stacked Regressor: 0.1095\n- Lasso: 0.1109\n- Ridge: 0.1116\n- Gradient Boosting Regressor: 0.1248\n- SVR: 0.1286\n- Random Forest: 0.1308\n- K-vecinos: 0.1550\n- Árboles de decisión: 0.1708\n- Regresion lineal: 593910323.9356\n\nLos modelos con los mejores resultados en la lista son el Stacked Regressor con una puntuación de 0.1095, seguido por Lasso con 0.1109 y Ridge con 0.1116. El modelo con el peor rendimiento en la lista es la Regresión lineal, con una puntuación CV muy alta de 593910323.9356.","metadata":{}},{"cell_type":"markdown","source":"#### **R2 - Coeficiente de Determinación**","metadata":{}},{"cell_type":"markdown","source":"Ordenados de mejor a peor puntuación:\n\n- Lasso: 0.91\n- Ridge: 0.91\n- Stacked Regressor: 0.89\n- Gradient Boosting Regressor: 0.88\n- Random Forest: 0.87\n- SVR: 0.87\n- K-vecinos: 0.82\n- Árboles de decisión: 0.80\n- Regresion lineal: -1.5349096479779338e+20\n\nPodemos observar que los modelos de Ridge, Lasso y Stacked Regressor tienen un rendimiento muy similar y son los que presentan los coeficientes de determinación más altos, seguidos de cerca por el modelo de Gradient Boosting. Por otro lado, los modelos de Random Forest, SVR, K-Vecinos y Tree Decision también presentan un rendimiento aceptable, pero ligeramente inferior en comparación con los primeros modelos mencionados.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-six-subsection-two\"></a>\n### **6.2 Características más importantes del modelo**","metadata":{}},{"cell_type":"markdown","source":"Vamos a crear un gráfico para visualizar las 20 caraterísticas más importantes del modelo elegido **Stacked Regression**","metadata":{}},{"cell_type":"code","source":"# Obtener las características más importantes del modelo\nimportances = []\nfor estimator in stacked3.estimators_:\n    if hasattr(estimator, 'feature_importances_'):\n        importances.append(estimator.feature_importances_)\n    else:\n        importances.append([0] * len(X_train.columns))\n\n# Promediar las importancias de las características a través de los estimadores base\nimportances = np.mean(importances, axis=0)\n\n# Obtener el nombre de las características\nfeature_names = X_train.columns\n\n# Obtener las 20 características más importantes y sus valores\nimportances_sorted_idx = np.argsort(importances)[::-1][:20]  # Índices de las 20 características más importantes\nsorted_importances = importances[importances_sorted_idx]\nsorted_feature_names = feature_names[importances_sorted_idx]\n\n# Crear gráfico de barras de las características más importantes\nplt.bar(sorted_feature_names, sorted_importances)\nplt.xticks(rotation=90)\nplt.ylabel('Importancia')\nplt.show()\n\n# Imprimir lista independiente con las características más importantes y sus valores\nprint('Características más importantes:\\n')\nfor feature, importance in zip(sorted_feature_names, sorted_importances):\n    print(f'{feature}: {importance:.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-05-10T09:39:56.594851Z","iopub.execute_input":"2023-05-10T09:39:56.595267Z","iopub.status.idle":"2023-05-10T09:39:57.089689Z","shell.execute_reply.started":"2023-05-10T09:39:56.595235Z","shell.execute_reply":"2023-05-10T09:39:57.088557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusiones importancia de las categorías**\n\n- La variable **ConstructArea** es la característica más importante en la predicción del precio de la vivienda, con una importancia del 23.52%. Esto significa que un aumento en el tamaño de la vivienda tiene el mayor impacto en el aumento del precio de la vivienda.\n- La segunda característica más importante es **OverallQual** (9.74%), que se refiere a la calidad general de los materiales y acabados de la casa. Una calificación alta en esta característica indica una casa de alta calidad, lo que tiene un efecto positivo en el precio.\n- **TotalSF** (3.33%) se refiere al tamaño total de la vivienda, incluyendo la superficie construida y la superficie habitable. Esto indica que el tamaño de la casa en general es un factor importante en la determinación del precio.\n- **KitchenQual** (2.49%) se refiere a la calidad de los acabados en la cocina. Esto sugiere que una cocina bien equipada y de alta calidad puede tener un impacto significativo en el precio de la vivienda, ya que puede indicar una casa de alto standing.\n- **YearBuilt** (1.31%) es la característica más importante relacionada con la edad de la casa. Esto sugiere que los compradores valoran más las casas más nuevas.\n- **Total_Bathrooms** (0.92%) se refiere al número total de baños en la casa. Esto indica que las casas con más baños tienden a tener precios más altos. Esto está muy relacionado con el tamaño de la casa, a mayor tamaño, normalmente más baños.\n- **GarageGrade** (0.74%) se refiere a la calidad de la construcción del garaje. Un garaje bien construido y de alta calidad puede aumentar el valor de la casa.\n- **LotArea** (0.54%) se refiere al tamaño del terreno en pies cuadrados. Esto sugiere que las casas con más terreno tienden a tener precios más altos.\n- **FireplaceQu_0** (0.56%) indica que la ausencia de chimeneas en la casa puede tener un efecto negativo en el precio.\n- **MSZoning_RM** (0.43%) se refiere a la ubicación de la casa en una zona residencial de densidad media. Esto sugiere que las casas en zonas residenciales más densas pueden tener precios más bajos. Como hemos observado previamente la localización de la casa afecta mucho a su precio.\n- **Exterior1st_BrkComm** (0.40%) se refiere al material de revestimiento exterior de la casa. Un revestimiento de ladrillo comercial puede indicar una casa más antigua y tener un impacto negativo en el precio.\n\nResumiendo, observando la lista de características más importantes vemos que los compradores de vivienda valoran especialmente el tamaño y la calidad de la construcción de la casa, así como la calidad de los acabados y la edad de la casa. Además, la presencia de características como chimeneas y garajes también pueden tener un impacto significativo en el precio, ya que son características que pueden ir asociadas a una casa de mayor calidad.\n\nPodemos observar además que muchas de estas características son características compuestas que hemos creado previamente.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-six-subsection-three\"></a>\n### **6.3 Exportación modelo final (Stacked Regressor)**","metadata":{}},{"cell_type":"markdown","source":"Finalmente vamos a predecir con nuestro modelo de **Stacked Regressor** y vamos a generar un csv de salida","metadata":{}},{"cell_type":"code","source":"#predecir el valor del precio de venta en el conjunto de test\ny_final_pred = stacked.predict(test_end)\ny_final_pred","metadata":{"execution":{"iopub.status.busy":"2023-05-10T10:05:45.514389Z","iopub.execute_input":"2023-05-10T10:05:45.515508Z","iopub.status.idle":"2023-05-10T10:05:45.845369Z","shell.execute_reply.started":"2023-05-10T10:05:45.515469Z","shell.execute_reply":"2023-05-10T10:05:45.844199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformar los valores de salida del modelo de regresión a su escala original\npredictions = np.expm1(y_final_pred)\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T10:05:49.352594Z","iopub.execute_input":"2023-05-10T10:05:49.353112Z","iopub.status.idle":"2023-05-10T10:05:49.361198Z","shell.execute_reply.started":"2023-05-10T10:05:49.353070Z","shell.execute_reply":"2023-05-10T10:05:49.359751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creamos el df de **submit**","metadata":{}},{"cell_type":"code","source":"# Crear df\nsubmit = pd.DataFrame()\n# Añadimos el ID\nsubmit['Id'] = test_ID\n# Añadimos las predicciones\nsubmit['SalePrice'] = predictions","metadata":{"execution":{"iopub.status.busy":"2023-05-10T10:06:39.927018Z","iopub.execute_input":"2023-05-10T10:06:39.927905Z","iopub.status.idle":"2023-05-10T10:06:39.936280Z","shell.execute_reply.started":"2023-05-10T10:06:39.927863Z","shell.execute_reply":"2023-05-10T10:06:39.935167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exportamos finalmente el modelo en .csv","metadata":{}},{"cell_type":"code","source":"submit.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T10:08:25.939945Z","iopub.execute_input":"2023-05-10T10:08:25.940375Z","iopub.status.idle":"2023-05-10T10:08:25.959806Z","shell.execute_reply.started":"2023-05-10T10:08:25.940339Z","shell.execute_reply":"2023-05-10T10:08:25.958354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finalmente hemos conseguido un **score de 0.12774**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-seven\"></a>\n## **CONCLUSIONES**","metadata":{}},{"cell_type":"markdown","source":"Después de realizar el análisis detallado de los datos, y utilizar diferentes técnicas avanzadas de regresión en nuestro modelo, podemos sacar algunas conclusiones acerca del mismo.\n\n- La eliminación de outliers y la imputación de nulos utilizando técnicas adecuadas ha llevado a un mejor funcionamiento de nuestro modelo, partíamos con un número muy alto de nulos y hemos conseguido gestionarlo.\n\n- En cuanto al análisis de los datos, se encontró que había una gran cantidad de valores faltantes en algunas características, como **PoolQC**, **MiscFeature** y **Alley**. Estas características fueron eliminadas posteriormente de nuestro modelo debido a la gran asimetría que tenían, lo que sugiere que estas características no tienen un impacto significativo en el precio de una casa. Podríamos considerarlas características de relleno.\n\n- Observamos que tenemos características que nos influyen mucho más que otras en el precio como todas relacionadas con el tamaño, edad, calidad y localización de la vivienda.\n\n- Para el modelo final, se generaron varias nuevas características a partir de las existentes, como la superficie total de la vivienda, la superficie habitable, la calidad general, la cantidad total de baños, la superficie del sótano terminado, si tiene o no piscina o chimenea...\nDe estas características generadas, hubieron algunas que tuvieron un gran impacto en la predicción de los precios de las casas como ConstructArea, OverallQual, TotalSF, KitchenQual, OverallGrade, YearBuilt, Total_Bathrooms...\n\n- En general, las características creadas que funcionan bien son aquellas que se relacionan directamente con la calidad y la cantidad de los diferentes elementos de la casa, como el área de construcción, la calidad de la cocina y el baño, y la cantidad total de baños y habitaciones. También es importante considerar el año de construcción y remodelación de la casa, ya que esto puede indicar su estado general y su potencial de reventa.\n\n- Aunque normalmente, no es necesario normalizar o escalar la variable target en problemas de regresión, en nuestro caso, tenía una amplia gama de valores y no estaba distribuida normalmente, la tuvimos que normalizar y escalar para evitar que los modelos se vean afectados por la heterocedasticidad y los valores atípicos.\n\n- Entre los diferentes modelos de regresión utilizados, el **Stacked Regressor** presentó el mejor desempeño, seguido de **Lasso** y **Ridge**, mientras que la **regresión lineal** dio un resultado muy pobre. Esto nos puede sugerir que la relación entre las características y la variable objetivo no es lineal, y que un modelo más complejo puede ser más adecuado para estos datos.\n\n- Al utilizar el **Stacked Regressor**, se encontró que las características más importantes para determinar el precio de una casa fueron **ConstructArea**, **OverallQua**l, **TotalSF** y **KitchenQual**, lo que concuerda con las observaciones anteriores. \n\n- También se ha observado que las características relacionadas con la antigüedad de la casa, como **YearBuilt** y **Age**, tienen una importancia importante en la predicción del precio, luego podemos afirmar que la edad de la vivienda en la fecha de la compra influye mucho en su precio y es una medida a tener en cuenta por el comprador.\n\n\n\nEn conclusión, mediante el análisis de los datos y la utilización de técnicas avanzadas de regresión conseguimos un modelo preciso para la predicción del precio de las casas","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-eight\"></a>\n## **BIBLIOGRAFÍA**","metadata":{}},{"cell_type":"markdown","source":"Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer Science & Business Media.\n\nHarrison, David & Rubinfeld, Daniel. (1978). *Hedonic housing prices and the demand for clean air*. Journal of Environmental Economics and Management\n\nBelsley, David A. & Kuh, Edwin. & Welsch, Roy E. (1980). *Regression diagnostics: identifying influential data and sources of collinearity*. New York: Wiley\n\nQu, Y., & Huang, Y. (2019). *A Comparative Study of Feature Selection and Feature Extraction Methods for Machine Learning Tasks in Real Estate*. Journal of Real Estate Research, 41(1), 107-145.\n\nWu, X., Kumar, V., Quinlan, J. R., Ghosh, J., Yang, Q., Motoda, H., McLachlan, G. J., Ng, A., Liu, B., Yu, P. S., Zhou, Z. H., Steinbach, M., & Hand, D. J. (2008). *Top 10 Algorithms in Data Mining*. Knowledge and Information Systems, 14(1), 1-37\n\n\nhttps://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/coefficient-of-determination-r-squared.html","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}